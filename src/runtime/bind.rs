/* automatically generated by rust-bindgen */

pub const _LIBC_LIMITS_H_: u32 = 1;
pub const _FEATURES_H: u32 = 1;
pub const _DEFAULT_SOURCE: u32 = 1;
pub const __USE_ISOC11: u32 = 1;
pub const __USE_ISOC99: u32 = 1;
pub const __USE_ISOC95: u32 = 1;
pub const __USE_POSIX_IMPLICITLY: u32 = 1;
pub const _POSIX_SOURCE: u32 = 1;
pub const _POSIX_C_SOURCE: u32 = 200809;
pub const __USE_POSIX: u32 = 1;
pub const __USE_POSIX2: u32 = 1;
pub const __USE_POSIX199309: u32 = 1;
pub const __USE_POSIX199506: u32 = 1;
pub const __USE_XOPEN2K: u32 = 1;
pub const __USE_XOPEN2K8: u32 = 1;
pub const _ATFILE_SOURCE: u32 = 1;
pub const __USE_MISC: u32 = 1;
pub const __USE_ATFILE: u32 = 1;
pub const __USE_FORTIFY_LEVEL: u32 = 0;
pub const __GLIBC_USE_DEPRECATED_GETS: u32 = 0;
pub const _STDC_PREDEF_H: u32 = 1;
pub const __STDC_IEC_559__: u32 = 1;
pub const __STDC_IEC_559_COMPLEX__: u32 = 1;
pub const __STDC_ISO_10646__: u32 = 201706;
pub const __GNU_LIBRARY__: u32 = 6;
pub const __GLIBC__: u32 = 2;
pub const __GLIBC_MINOR__: u32 = 28;
pub const _SYS_CDEFS_H: u32 = 1;
pub const __glibc_c99_flexarr_available: u32 = 1;
pub const __WORDSIZE: u32 = 64;
pub const __WORDSIZE_TIME64_COMPAT32: u32 = 1;
pub const __SYSCALL_WORDSIZE: u32 = 64;
pub const __HAVE_GENERIC_SELECTION: u32 = 1;
pub const __GLIBC_USE_LIB_EXT2: u32 = 0;
pub const __GLIBC_USE_IEC_60559_BFP_EXT: u32 = 0;
pub const __GLIBC_USE_IEC_60559_FUNCS_EXT: u32 = 0;
pub const __GLIBC_USE_IEC_60559_TYPES_EXT: u32 = 0;
pub const MB_LEN_MAX: u32 = 16;
pub const _BITS_POSIX1_LIM_H: u32 = 1;
pub const _POSIX_AIO_LISTIO_MAX: u32 = 2;
pub const _POSIX_AIO_MAX: u32 = 1;
pub const _POSIX_ARG_MAX: u32 = 4096;
pub const _POSIX_CHILD_MAX: u32 = 25;
pub const _POSIX_DELAYTIMER_MAX: u32 = 32;
pub const _POSIX_HOST_NAME_MAX: u32 = 255;
pub const _POSIX_LINK_MAX: u32 = 8;
pub const _POSIX_LOGIN_NAME_MAX: u32 = 9;
pub const _POSIX_MAX_CANON: u32 = 255;
pub const _POSIX_MAX_INPUT: u32 = 255;
pub const _POSIX_MQ_OPEN_MAX: u32 = 8;
pub const _POSIX_MQ_PRIO_MAX: u32 = 32;
pub const _POSIX_NAME_MAX: u32 = 14;
pub const _POSIX_NGROUPS_MAX: u32 = 8;
pub const _POSIX_OPEN_MAX: u32 = 20;
pub const _POSIX_PATH_MAX: u32 = 256;
pub const _POSIX_PIPE_BUF: u32 = 512;
pub const _POSIX_RE_DUP_MAX: u32 = 255;
pub const _POSIX_RTSIG_MAX: u32 = 8;
pub const _POSIX_SEM_NSEMS_MAX: u32 = 256;
pub const _POSIX_SEM_VALUE_MAX: u32 = 32767;
pub const _POSIX_SIGQUEUE_MAX: u32 = 32;
pub const _POSIX_SSIZE_MAX: u32 = 32767;
pub const _POSIX_STREAM_MAX: u32 = 8;
pub const _POSIX_SYMLINK_MAX: u32 = 255;
pub const _POSIX_SYMLOOP_MAX: u32 = 8;
pub const _POSIX_TIMER_MAX: u32 = 32;
pub const _POSIX_TTY_NAME_MAX: u32 = 9;
pub const _POSIX_TZNAME_MAX: u32 = 6;
pub const _POSIX_CLOCKRES_MIN: u32 = 20000000;
pub const NR_OPEN: u32 = 1024;
pub const NGROUPS_MAX: u32 = 65536;
pub const ARG_MAX: u32 = 131072;
pub const LINK_MAX: u32 = 127;
pub const MAX_CANON: u32 = 255;
pub const MAX_INPUT: u32 = 255;
pub const NAME_MAX: u32 = 255;
pub const PATH_MAX: u32 = 4096;
pub const PIPE_BUF: u32 = 4096;
pub const XATTR_NAME_MAX: u32 = 255;
pub const XATTR_SIZE_MAX: u32 = 65536;
pub const XATTR_LIST_MAX: u32 = 65536;
pub const RTSIG_MAX: u32 = 32;
pub const _POSIX_THREAD_KEYS_MAX: u32 = 128;
pub const PTHREAD_KEYS_MAX: u32 = 1024;
pub const _POSIX_THREAD_DESTRUCTOR_ITERATIONS: u32 = 4;
pub const PTHREAD_DESTRUCTOR_ITERATIONS: u32 = 4;
pub const _POSIX_THREAD_THREADS_MAX: u32 = 64;
pub const AIO_PRIO_DELTA_MAX: u32 = 20;
pub const PTHREAD_STACK_MIN: u32 = 16384;
pub const DELAYTIMER_MAX: u32 = 2147483647;
pub const TTY_NAME_MAX: u32 = 32;
pub const LOGIN_NAME_MAX: u32 = 256;
pub const HOST_NAME_MAX: u32 = 64;
pub const MQ_PRIO_MAX: u32 = 32768;
pub const SEM_VALUE_MAX: u32 = 2147483647;
pub const _BITS_POSIX2_LIM_H: u32 = 1;
pub const _POSIX2_BC_BASE_MAX: u32 = 99;
pub const _POSIX2_BC_DIM_MAX: u32 = 2048;
pub const _POSIX2_BC_SCALE_MAX: u32 = 99;
pub const _POSIX2_BC_STRING_MAX: u32 = 1000;
pub const _POSIX2_COLL_WEIGHTS_MAX: u32 = 2;
pub const _POSIX2_EXPR_NEST_MAX: u32 = 32;
pub const _POSIX2_LINE_MAX: u32 = 2048;
pub const _POSIX2_RE_DUP_MAX: u32 = 255;
pub const _POSIX2_CHARCLASS_NAME_MAX: u32 = 14;
pub const BC_BASE_MAX: u32 = 99;
pub const BC_DIM_MAX: u32 = 2048;
pub const BC_SCALE_MAX: u32 = 99;
pub const BC_STRING_MAX: u32 = 1000;
pub const COLL_WEIGHTS_MAX: u32 = 255;
pub const EXPR_NEST_MAX: u32 = 32;
pub const LINE_MAX: u32 = 2048;
pub const CHARCLASS_NAME_MAX: u32 = 2048;
pub const RE_DUP_MAX: u32 = 32767;
pub const cudaHostAllocDefault: u32 = 0;
pub const cudaHostAllocPortable: u32 = 1;
pub const cudaHostAllocMapped: u32 = 2;
pub const cudaHostAllocWriteCombined: u32 = 4;
pub const cudaHostRegisterDefault: u32 = 0;
pub const cudaHostRegisterPortable: u32 = 1;
pub const cudaHostRegisterMapped: u32 = 2;
pub const cudaHostRegisterIoMemory: u32 = 4;
pub const cudaPeerAccessDefault: u32 = 0;
pub const cudaStreamDefault: u32 = 0;
pub const cudaStreamNonBlocking: u32 = 1;
pub const cudaEventDefault: u32 = 0;
pub const cudaEventBlockingSync: u32 = 1;
pub const cudaEventDisableTiming: u32 = 2;
pub const cudaEventInterprocess: u32 = 4;
pub const cudaDeviceScheduleAuto: u32 = 0;
pub const cudaDeviceScheduleSpin: u32 = 1;
pub const cudaDeviceScheduleYield: u32 = 2;
pub const cudaDeviceScheduleBlockingSync: u32 = 4;
pub const cudaDeviceBlockingSync: u32 = 4;
pub const cudaDeviceScheduleMask: u32 = 7;
pub const cudaDeviceMapHost: u32 = 8;
pub const cudaDeviceLmemResizeToMax: u32 = 16;
pub const cudaDeviceMask: u32 = 31;
pub const cudaArrayDefault: u32 = 0;
pub const cudaArrayLayered: u32 = 1;
pub const cudaArraySurfaceLoadStore: u32 = 2;
pub const cudaArrayCubemap: u32 = 4;
pub const cudaArrayTextureGather: u32 = 8;
pub const cudaArrayColorAttachment: u32 = 32;
pub const cudaIpcMemLazyEnablePeerAccess: u32 = 1;
pub const cudaMemAttachGlobal: u32 = 1;
pub const cudaMemAttachHost: u32 = 2;
pub const cudaMemAttachSingle: u32 = 4;
pub const cudaOccupancyDefault: u32 = 0;
pub const cudaOccupancyDisableCachingOverride: u32 = 1;
pub const cudaCooperativeLaunchMultiDeviceNoPreSync: u32 = 1;
pub const cudaCooperativeLaunchMultiDeviceNoPostSync: u32 = 2;
pub const CUDA_IPC_HANDLE_SIZE: u32 = 64;
pub const cudaExternalMemoryDedicated: u32 = 1;
pub const cudaSurfaceType1D: u32 = 1;
pub const cudaSurfaceType2D: u32 = 2;
pub const cudaSurfaceType3D: u32 = 3;
pub const cudaSurfaceTypeCubemap: u32 = 12;
pub const cudaSurfaceType1DLayered: u32 = 241;
pub const cudaSurfaceType2DLayered: u32 = 242;
pub const cudaSurfaceTypeCubemapLayered: u32 = 252;
pub const cudaTextureType1D: u32 = 1;
pub const cudaTextureType2D: u32 = 2;
pub const cudaTextureType3D: u32 = 3;
pub const cudaTextureTypeCubemap: u32 = 12;
pub const cudaTextureType1DLayered: u32 = 241;
pub const cudaTextureType2DLayered: u32 = 242;
pub const cudaTextureTypeCubemapLayered: u32 = 252;
pub const CUDART_VERSION: u32 = 10000;
pub const cudaRoundMode_cudaRoundNearest: cudaRoundMode = 0;
pub const cudaRoundMode_cudaRoundZero: cudaRoundMode = 1;
pub const cudaRoundMode_cudaRoundPosInf: cudaRoundMode = 2;
pub const cudaRoundMode_cudaRoundMinInf: cudaRoundMode = 3;
///                                                                              *
///                                                                              *
///                                                                              *
pub type cudaRoundMode = u32;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct char1 {
    pub x: ::std::os::raw::c_schar,
}
#[test]
fn bindgen_test_layout_char1() {
    assert_eq!(
        ::std::mem::size_of::<char1>(),
        1usize,
        concat!("Size of: ", stringify!(char1))
    );
    assert_eq!(
        ::std::mem::align_of::<char1>(),
        1usize,
        concat!("Alignment of ", stringify!(char1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<char1>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(char1), "::", stringify!(x))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct uchar1 {
    pub x: ::std::os::raw::c_uchar,
}
#[test]
fn bindgen_test_layout_uchar1() {
    assert_eq!(
        ::std::mem::size_of::<uchar1>(),
        1usize,
        concat!("Size of: ", stringify!(uchar1))
    );
    assert_eq!(
        ::std::mem::align_of::<uchar1>(),
        1usize,
        concat!("Alignment of ", stringify!(uchar1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uchar1>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(uchar1), "::", stringify!(x))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct char2 {
    pub x: ::std::os::raw::c_schar,
    pub y: ::std::os::raw::c_schar,
    pub __bindgen_align: [u16; 0usize],
}
#[test]
fn bindgen_test_layout_char2() {
    assert_eq!(
        ::std::mem::size_of::<char2>(),
        2usize,
        concat!("Size of: ", stringify!(char2))
    );
    assert_eq!(
        ::std::mem::align_of::<char2>(),
        2usize,
        concat!("Alignment of ", stringify!(char2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<char2>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(char2), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<char2>())).y as *const _ as usize },
        1usize,
        concat!("Offset of field: ", stringify!(char2), "::", stringify!(y))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct uchar2 {
    pub x: ::std::os::raw::c_uchar,
    pub y: ::std::os::raw::c_uchar,
    pub __bindgen_align: [u16; 0usize],
}
#[test]
fn bindgen_test_layout_uchar2() {
    assert_eq!(
        ::std::mem::size_of::<uchar2>(),
        2usize,
        concat!("Size of: ", stringify!(uchar2))
    );
    assert_eq!(
        ::std::mem::align_of::<uchar2>(),
        2usize,
        concat!("Alignment of ", stringify!(uchar2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uchar2>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(uchar2), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uchar2>())).y as *const _ as usize },
        1usize,
        concat!("Offset of field: ", stringify!(uchar2), "::", stringify!(y))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct char3 {
    pub x: ::std::os::raw::c_schar,
    pub y: ::std::os::raw::c_schar,
    pub z: ::std::os::raw::c_schar,
}
#[test]
fn bindgen_test_layout_char3() {
    assert_eq!(
        ::std::mem::size_of::<char3>(),
        3usize,
        concat!("Size of: ", stringify!(char3))
    );
    assert_eq!(
        ::std::mem::align_of::<char3>(),
        1usize,
        concat!("Alignment of ", stringify!(char3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<char3>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(char3), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<char3>())).y as *const _ as usize },
        1usize,
        concat!("Offset of field: ", stringify!(char3), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<char3>())).z as *const _ as usize },
        2usize,
        concat!("Offset of field: ", stringify!(char3), "::", stringify!(z))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct uchar3 {
    pub x: ::std::os::raw::c_uchar,
    pub y: ::std::os::raw::c_uchar,
    pub z: ::std::os::raw::c_uchar,
}
#[test]
fn bindgen_test_layout_uchar3() {
    assert_eq!(
        ::std::mem::size_of::<uchar3>(),
        3usize,
        concat!("Size of: ", stringify!(uchar3))
    );
    assert_eq!(
        ::std::mem::align_of::<uchar3>(),
        1usize,
        concat!("Alignment of ", stringify!(uchar3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uchar3>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(uchar3), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uchar3>())).y as *const _ as usize },
        1usize,
        concat!("Offset of field: ", stringify!(uchar3), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uchar3>())).z as *const _ as usize },
        2usize,
        concat!("Offset of field: ", stringify!(uchar3), "::", stringify!(z))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct char4 {
    pub x: ::std::os::raw::c_schar,
    pub y: ::std::os::raw::c_schar,
    pub z: ::std::os::raw::c_schar,
    pub w: ::std::os::raw::c_schar,
    pub __bindgen_align: [u32; 0usize],
}
#[test]
fn bindgen_test_layout_char4() {
    assert_eq!(
        ::std::mem::size_of::<char4>(),
        4usize,
        concat!("Size of: ", stringify!(char4))
    );
    assert_eq!(
        ::std::mem::align_of::<char4>(),
        4usize,
        concat!("Alignment of ", stringify!(char4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<char4>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(char4), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<char4>())).y as *const _ as usize },
        1usize,
        concat!("Offset of field: ", stringify!(char4), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<char4>())).z as *const _ as usize },
        2usize,
        concat!("Offset of field: ", stringify!(char4), "::", stringify!(z))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<char4>())).w as *const _ as usize },
        3usize,
        concat!("Offset of field: ", stringify!(char4), "::", stringify!(w))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct uchar4 {
    pub x: ::std::os::raw::c_uchar,
    pub y: ::std::os::raw::c_uchar,
    pub z: ::std::os::raw::c_uchar,
    pub w: ::std::os::raw::c_uchar,
    pub __bindgen_align: [u32; 0usize],
}
#[test]
fn bindgen_test_layout_uchar4() {
    assert_eq!(
        ::std::mem::size_of::<uchar4>(),
        4usize,
        concat!("Size of: ", stringify!(uchar4))
    );
    assert_eq!(
        ::std::mem::align_of::<uchar4>(),
        4usize,
        concat!("Alignment of ", stringify!(uchar4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uchar4>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(uchar4), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uchar4>())).y as *const _ as usize },
        1usize,
        concat!("Offset of field: ", stringify!(uchar4), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uchar4>())).z as *const _ as usize },
        2usize,
        concat!("Offset of field: ", stringify!(uchar4), "::", stringify!(z))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uchar4>())).w as *const _ as usize },
        3usize,
        concat!("Offset of field: ", stringify!(uchar4), "::", stringify!(w))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct short1 {
    pub x: ::std::os::raw::c_short,
}
#[test]
fn bindgen_test_layout_short1() {
    assert_eq!(
        ::std::mem::size_of::<short1>(),
        2usize,
        concat!("Size of: ", stringify!(short1))
    );
    assert_eq!(
        ::std::mem::align_of::<short1>(),
        2usize,
        concat!("Alignment of ", stringify!(short1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<short1>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(short1), "::", stringify!(x))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ushort1 {
    pub x: ::std::os::raw::c_ushort,
}
#[test]
fn bindgen_test_layout_ushort1() {
    assert_eq!(
        ::std::mem::size_of::<ushort1>(),
        2usize,
        concat!("Size of: ", stringify!(ushort1))
    );
    assert_eq!(
        ::std::mem::align_of::<ushort1>(),
        2usize,
        concat!("Alignment of ", stringify!(ushort1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ushort1>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ushort1),
            "::",
            stringify!(x)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct short2 {
    pub x: ::std::os::raw::c_short,
    pub y: ::std::os::raw::c_short,
    pub __bindgen_align: [u32; 0usize],
}
#[test]
fn bindgen_test_layout_short2() {
    assert_eq!(
        ::std::mem::size_of::<short2>(),
        4usize,
        concat!("Size of: ", stringify!(short2))
    );
    assert_eq!(
        ::std::mem::align_of::<short2>(),
        4usize,
        concat!("Alignment of ", stringify!(short2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<short2>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(short2), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<short2>())).y as *const _ as usize },
        2usize,
        concat!("Offset of field: ", stringify!(short2), "::", stringify!(y))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ushort2 {
    pub x: ::std::os::raw::c_ushort,
    pub y: ::std::os::raw::c_ushort,
    pub __bindgen_align: [u32; 0usize],
}
#[test]
fn bindgen_test_layout_ushort2() {
    assert_eq!(
        ::std::mem::size_of::<ushort2>(),
        4usize,
        concat!("Size of: ", stringify!(ushort2))
    );
    assert_eq!(
        ::std::mem::align_of::<ushort2>(),
        4usize,
        concat!("Alignment of ", stringify!(ushort2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ushort2>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ushort2),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ushort2>())).y as *const _ as usize },
        2usize,
        concat!(
            "Offset of field: ",
            stringify!(ushort2),
            "::",
            stringify!(y)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct short3 {
    pub x: ::std::os::raw::c_short,
    pub y: ::std::os::raw::c_short,
    pub z: ::std::os::raw::c_short,
}
#[test]
fn bindgen_test_layout_short3() {
    assert_eq!(
        ::std::mem::size_of::<short3>(),
        6usize,
        concat!("Size of: ", stringify!(short3))
    );
    assert_eq!(
        ::std::mem::align_of::<short3>(),
        2usize,
        concat!("Alignment of ", stringify!(short3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<short3>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(short3), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<short3>())).y as *const _ as usize },
        2usize,
        concat!("Offset of field: ", stringify!(short3), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<short3>())).z as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(short3), "::", stringify!(z))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ushort3 {
    pub x: ::std::os::raw::c_ushort,
    pub y: ::std::os::raw::c_ushort,
    pub z: ::std::os::raw::c_ushort,
}
#[test]
fn bindgen_test_layout_ushort3() {
    assert_eq!(
        ::std::mem::size_of::<ushort3>(),
        6usize,
        concat!("Size of: ", stringify!(ushort3))
    );
    assert_eq!(
        ::std::mem::align_of::<ushort3>(),
        2usize,
        concat!("Alignment of ", stringify!(ushort3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ushort3>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ushort3),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ushort3>())).y as *const _ as usize },
        2usize,
        concat!(
            "Offset of field: ",
            stringify!(ushort3),
            "::",
            stringify!(y)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ushort3>())).z as *const _ as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(ushort3),
            "::",
            stringify!(z)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct short4 {
    pub x: ::std::os::raw::c_short,
    pub y: ::std::os::raw::c_short,
    pub z: ::std::os::raw::c_short,
    pub w: ::std::os::raw::c_short,
    pub __bindgen_align: [u64; 0usize],
}
#[test]
fn bindgen_test_layout_short4() {
    assert_eq!(
        ::std::mem::size_of::<short4>(),
        8usize,
        concat!("Size of: ", stringify!(short4))
    );
    assert_eq!(
        ::std::mem::align_of::<short4>(),
        8usize,
        concat!("Alignment of ", stringify!(short4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<short4>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(short4), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<short4>())).y as *const _ as usize },
        2usize,
        concat!("Offset of field: ", stringify!(short4), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<short4>())).z as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(short4), "::", stringify!(z))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<short4>())).w as *const _ as usize },
        6usize,
        concat!("Offset of field: ", stringify!(short4), "::", stringify!(w))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ushort4 {
    pub x: ::std::os::raw::c_ushort,
    pub y: ::std::os::raw::c_ushort,
    pub z: ::std::os::raw::c_ushort,
    pub w: ::std::os::raw::c_ushort,
    pub __bindgen_align: [u64; 0usize],
}
#[test]
fn bindgen_test_layout_ushort4() {
    assert_eq!(
        ::std::mem::size_of::<ushort4>(),
        8usize,
        concat!("Size of: ", stringify!(ushort4))
    );
    assert_eq!(
        ::std::mem::align_of::<ushort4>(),
        8usize,
        concat!("Alignment of ", stringify!(ushort4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ushort4>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ushort4),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ushort4>())).y as *const _ as usize },
        2usize,
        concat!(
            "Offset of field: ",
            stringify!(ushort4),
            "::",
            stringify!(y)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ushort4>())).z as *const _ as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(ushort4),
            "::",
            stringify!(z)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ushort4>())).w as *const _ as usize },
        6usize,
        concat!(
            "Offset of field: ",
            stringify!(ushort4),
            "::",
            stringify!(w)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct int1 {
    pub x: ::std::os::raw::c_int,
}
#[test]
fn bindgen_test_layout_int1() {
    assert_eq!(
        ::std::mem::size_of::<int1>(),
        4usize,
        concat!("Size of: ", stringify!(int1))
    );
    assert_eq!(
        ::std::mem::align_of::<int1>(),
        4usize,
        concat!("Alignment of ", stringify!(int1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<int1>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(int1), "::", stringify!(x))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct uint1 {
    pub x: ::std::os::raw::c_uint,
}
#[test]
fn bindgen_test_layout_uint1() {
    assert_eq!(
        ::std::mem::size_of::<uint1>(),
        4usize,
        concat!("Size of: ", stringify!(uint1))
    );
    assert_eq!(
        ::std::mem::align_of::<uint1>(),
        4usize,
        concat!("Alignment of ", stringify!(uint1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uint1>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(uint1), "::", stringify!(x))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct int2 {
    pub x: ::std::os::raw::c_int,
    pub y: ::std::os::raw::c_int,
    pub __bindgen_align: [u64; 0usize],
}
#[test]
fn bindgen_test_layout_int2() {
    assert_eq!(
        ::std::mem::size_of::<int2>(),
        8usize,
        concat!("Size of: ", stringify!(int2))
    );
    assert_eq!(
        ::std::mem::align_of::<int2>(),
        8usize,
        concat!("Alignment of ", stringify!(int2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<int2>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(int2), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<int2>())).y as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(int2), "::", stringify!(y))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct uint2 {
    pub x: ::std::os::raw::c_uint,
    pub y: ::std::os::raw::c_uint,
    pub __bindgen_align: [u64; 0usize],
}
#[test]
fn bindgen_test_layout_uint2() {
    assert_eq!(
        ::std::mem::size_of::<uint2>(),
        8usize,
        concat!("Size of: ", stringify!(uint2))
    );
    assert_eq!(
        ::std::mem::align_of::<uint2>(),
        8usize,
        concat!("Alignment of ", stringify!(uint2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uint2>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(uint2), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uint2>())).y as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(uint2), "::", stringify!(y))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct int3 {
    pub x: ::std::os::raw::c_int,
    pub y: ::std::os::raw::c_int,
    pub z: ::std::os::raw::c_int,
}
#[test]
fn bindgen_test_layout_int3() {
    assert_eq!(
        ::std::mem::size_of::<int3>(),
        12usize,
        concat!("Size of: ", stringify!(int3))
    );
    assert_eq!(
        ::std::mem::align_of::<int3>(),
        4usize,
        concat!("Alignment of ", stringify!(int3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<int3>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(int3), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<int3>())).y as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(int3), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<int3>())).z as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(int3), "::", stringify!(z))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct uint3 {
    pub x: ::std::os::raw::c_uint,
    pub y: ::std::os::raw::c_uint,
    pub z: ::std::os::raw::c_uint,
}
#[test]
fn bindgen_test_layout_uint3() {
    assert_eq!(
        ::std::mem::size_of::<uint3>(),
        12usize,
        concat!("Size of: ", stringify!(uint3))
    );
    assert_eq!(
        ::std::mem::align_of::<uint3>(),
        4usize,
        concat!("Alignment of ", stringify!(uint3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uint3>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(uint3), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uint3>())).y as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(uint3), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uint3>())).z as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(uint3), "::", stringify!(z))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct int4 {
    pub x: ::std::os::raw::c_int,
    pub y: ::std::os::raw::c_int,
    pub z: ::std::os::raw::c_int,
    pub w: ::std::os::raw::c_int,
}
#[test]
fn bindgen_test_layout_int4() {
    assert_eq!(
        ::std::mem::size_of::<int4>(),
        16usize,
        concat!("Size of: ", stringify!(int4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<int4>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(int4), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<int4>())).y as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(int4), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<int4>())).z as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(int4), "::", stringify!(z))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<int4>())).w as *const _ as usize },
        12usize,
        concat!("Offset of field: ", stringify!(int4), "::", stringify!(w))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct uint4 {
    pub x: ::std::os::raw::c_uint,
    pub y: ::std::os::raw::c_uint,
    pub z: ::std::os::raw::c_uint,
    pub w: ::std::os::raw::c_uint,
}
#[test]
fn bindgen_test_layout_uint4() {
    assert_eq!(
        ::std::mem::size_of::<uint4>(),
        16usize,
        concat!("Size of: ", stringify!(uint4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uint4>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(uint4), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uint4>())).y as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(uint4), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uint4>())).z as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(uint4), "::", stringify!(z))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<uint4>())).w as *const _ as usize },
        12usize,
        concat!("Offset of field: ", stringify!(uint4), "::", stringify!(w))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct long1 {
    pub x: ::std::os::raw::c_long,
}
#[test]
fn bindgen_test_layout_long1() {
    assert_eq!(
        ::std::mem::size_of::<long1>(),
        8usize,
        concat!("Size of: ", stringify!(long1))
    );
    assert_eq!(
        ::std::mem::align_of::<long1>(),
        8usize,
        concat!("Alignment of ", stringify!(long1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<long1>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(long1), "::", stringify!(x))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ulong1 {
    pub x: ::std::os::raw::c_ulong,
}
#[test]
fn bindgen_test_layout_ulong1() {
    assert_eq!(
        ::std::mem::size_of::<ulong1>(),
        8usize,
        concat!("Size of: ", stringify!(ulong1))
    );
    assert_eq!(
        ::std::mem::align_of::<ulong1>(),
        8usize,
        concat!("Alignment of ", stringify!(ulong1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulong1>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(ulong1), "::", stringify!(x))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct long2 {
    pub x: ::std::os::raw::c_long,
    pub y: ::std::os::raw::c_long,
}
#[test]
fn bindgen_test_layout_long2() {
    assert_eq!(
        ::std::mem::size_of::<long2>(),
        16usize,
        concat!("Size of: ", stringify!(long2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<long2>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(long2), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<long2>())).y as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(long2), "::", stringify!(y))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ulong2 {
    pub x: ::std::os::raw::c_ulong,
    pub y: ::std::os::raw::c_ulong,
}
#[test]
fn bindgen_test_layout_ulong2() {
    assert_eq!(
        ::std::mem::size_of::<ulong2>(),
        16usize,
        concat!("Size of: ", stringify!(ulong2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulong2>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(ulong2), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulong2>())).y as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(ulong2), "::", stringify!(y))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct long3 {
    pub x: ::std::os::raw::c_long,
    pub y: ::std::os::raw::c_long,
    pub z: ::std::os::raw::c_long,
}
#[test]
fn bindgen_test_layout_long3() {
    assert_eq!(
        ::std::mem::size_of::<long3>(),
        24usize,
        concat!("Size of: ", stringify!(long3))
    );
    assert_eq!(
        ::std::mem::align_of::<long3>(),
        8usize,
        concat!("Alignment of ", stringify!(long3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<long3>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(long3), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<long3>())).y as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(long3), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<long3>())).z as *const _ as usize },
        16usize,
        concat!("Offset of field: ", stringify!(long3), "::", stringify!(z))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ulong3 {
    pub x: ::std::os::raw::c_ulong,
    pub y: ::std::os::raw::c_ulong,
    pub z: ::std::os::raw::c_ulong,
}
#[test]
fn bindgen_test_layout_ulong3() {
    assert_eq!(
        ::std::mem::size_of::<ulong3>(),
        24usize,
        concat!("Size of: ", stringify!(ulong3))
    );
    assert_eq!(
        ::std::mem::align_of::<ulong3>(),
        8usize,
        concat!("Alignment of ", stringify!(ulong3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulong3>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(ulong3), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulong3>())).y as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(ulong3), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulong3>())).z as *const _ as usize },
        16usize,
        concat!("Offset of field: ", stringify!(ulong3), "::", stringify!(z))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct long4 {
    pub x: ::std::os::raw::c_long,
    pub y: ::std::os::raw::c_long,
    pub z: ::std::os::raw::c_long,
    pub w: ::std::os::raw::c_long,
}
#[test]
fn bindgen_test_layout_long4() {
    assert_eq!(
        ::std::mem::size_of::<long4>(),
        32usize,
        concat!("Size of: ", stringify!(long4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<long4>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(long4), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<long4>())).y as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(long4), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<long4>())).z as *const _ as usize },
        16usize,
        concat!("Offset of field: ", stringify!(long4), "::", stringify!(z))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<long4>())).w as *const _ as usize },
        24usize,
        concat!("Offset of field: ", stringify!(long4), "::", stringify!(w))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ulong4 {
    pub x: ::std::os::raw::c_ulong,
    pub y: ::std::os::raw::c_ulong,
    pub z: ::std::os::raw::c_ulong,
    pub w: ::std::os::raw::c_ulong,
}
#[test]
fn bindgen_test_layout_ulong4() {
    assert_eq!(
        ::std::mem::size_of::<ulong4>(),
        32usize,
        concat!("Size of: ", stringify!(ulong4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulong4>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(ulong4), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulong4>())).y as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(ulong4), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulong4>())).z as *const _ as usize },
        16usize,
        concat!("Offset of field: ", stringify!(ulong4), "::", stringify!(z))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulong4>())).w as *const _ as usize },
        24usize,
        concat!("Offset of field: ", stringify!(ulong4), "::", stringify!(w))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct float1 {
    pub x: f32,
}
#[test]
fn bindgen_test_layout_float1() {
    assert_eq!(
        ::std::mem::size_of::<float1>(),
        4usize,
        concat!("Size of: ", stringify!(float1))
    );
    assert_eq!(
        ::std::mem::align_of::<float1>(),
        4usize,
        concat!("Alignment of ", stringify!(float1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<float1>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(float1), "::", stringify!(x))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct float2 {
    pub x: f32,
    pub y: f32,
    pub __bindgen_align: [u64; 0usize],
}
#[test]
fn bindgen_test_layout_float2() {
    assert_eq!(
        ::std::mem::size_of::<float2>(),
        8usize,
        concat!("Size of: ", stringify!(float2))
    );
    assert_eq!(
        ::std::mem::align_of::<float2>(),
        8usize,
        concat!("Alignment of ", stringify!(float2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<float2>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(float2), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<float2>())).y as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(float2), "::", stringify!(y))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct float3 {
    pub x: f32,
    pub y: f32,
    pub z: f32,
}
#[test]
fn bindgen_test_layout_float3() {
    assert_eq!(
        ::std::mem::size_of::<float3>(),
        12usize,
        concat!("Size of: ", stringify!(float3))
    );
    assert_eq!(
        ::std::mem::align_of::<float3>(),
        4usize,
        concat!("Alignment of ", stringify!(float3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<float3>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(float3), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<float3>())).y as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(float3), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<float3>())).z as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(float3), "::", stringify!(z))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct float4 {
    pub x: f32,
    pub y: f32,
    pub z: f32,
    pub w: f32,
}
#[test]
fn bindgen_test_layout_float4() {
    assert_eq!(
        ::std::mem::size_of::<float4>(),
        16usize,
        concat!("Size of: ", stringify!(float4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<float4>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(float4), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<float4>())).y as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(float4), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<float4>())).z as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(float4), "::", stringify!(z))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<float4>())).w as *const _ as usize },
        12usize,
        concat!("Offset of field: ", stringify!(float4), "::", stringify!(w))
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct longlong1 {
    pub x: ::std::os::raw::c_longlong,
}
#[test]
fn bindgen_test_layout_longlong1() {
    assert_eq!(
        ::std::mem::size_of::<longlong1>(),
        8usize,
        concat!("Size of: ", stringify!(longlong1))
    );
    assert_eq!(
        ::std::mem::align_of::<longlong1>(),
        8usize,
        concat!("Alignment of ", stringify!(longlong1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<longlong1>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(longlong1),
            "::",
            stringify!(x)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ulonglong1 {
    pub x: ::std::os::raw::c_ulonglong,
}
#[test]
fn bindgen_test_layout_ulonglong1() {
    assert_eq!(
        ::std::mem::size_of::<ulonglong1>(),
        8usize,
        concat!("Size of: ", stringify!(ulonglong1))
    );
    assert_eq!(
        ::std::mem::align_of::<ulonglong1>(),
        8usize,
        concat!("Alignment of ", stringify!(ulonglong1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulonglong1>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ulonglong1),
            "::",
            stringify!(x)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct longlong2 {
    pub x: ::std::os::raw::c_longlong,
    pub y: ::std::os::raw::c_longlong,
}
#[test]
fn bindgen_test_layout_longlong2() {
    assert_eq!(
        ::std::mem::size_of::<longlong2>(),
        16usize,
        concat!("Size of: ", stringify!(longlong2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<longlong2>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(longlong2),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<longlong2>())).y as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(longlong2),
            "::",
            stringify!(y)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ulonglong2 {
    pub x: ::std::os::raw::c_ulonglong,
    pub y: ::std::os::raw::c_ulonglong,
}
#[test]
fn bindgen_test_layout_ulonglong2() {
    assert_eq!(
        ::std::mem::size_of::<ulonglong2>(),
        16usize,
        concat!("Size of: ", stringify!(ulonglong2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulonglong2>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ulonglong2),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulonglong2>())).y as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ulonglong2),
            "::",
            stringify!(y)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct longlong3 {
    pub x: ::std::os::raw::c_longlong,
    pub y: ::std::os::raw::c_longlong,
    pub z: ::std::os::raw::c_longlong,
}
#[test]
fn bindgen_test_layout_longlong3() {
    assert_eq!(
        ::std::mem::size_of::<longlong3>(),
        24usize,
        concat!("Size of: ", stringify!(longlong3))
    );
    assert_eq!(
        ::std::mem::align_of::<longlong3>(),
        8usize,
        concat!("Alignment of ", stringify!(longlong3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<longlong3>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(longlong3),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<longlong3>())).y as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(longlong3),
            "::",
            stringify!(y)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<longlong3>())).z as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(longlong3),
            "::",
            stringify!(z)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ulonglong3 {
    pub x: ::std::os::raw::c_ulonglong,
    pub y: ::std::os::raw::c_ulonglong,
    pub z: ::std::os::raw::c_ulonglong,
}
#[test]
fn bindgen_test_layout_ulonglong3() {
    assert_eq!(
        ::std::mem::size_of::<ulonglong3>(),
        24usize,
        concat!("Size of: ", stringify!(ulonglong3))
    );
    assert_eq!(
        ::std::mem::align_of::<ulonglong3>(),
        8usize,
        concat!("Alignment of ", stringify!(ulonglong3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulonglong3>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ulonglong3),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulonglong3>())).y as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ulonglong3),
            "::",
            stringify!(y)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulonglong3>())).z as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ulonglong3),
            "::",
            stringify!(z)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct longlong4 {
    pub x: ::std::os::raw::c_longlong,
    pub y: ::std::os::raw::c_longlong,
    pub z: ::std::os::raw::c_longlong,
    pub w: ::std::os::raw::c_longlong,
}
#[test]
fn bindgen_test_layout_longlong4() {
    assert_eq!(
        ::std::mem::size_of::<longlong4>(),
        32usize,
        concat!("Size of: ", stringify!(longlong4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<longlong4>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(longlong4),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<longlong4>())).y as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(longlong4),
            "::",
            stringify!(y)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<longlong4>())).z as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(longlong4),
            "::",
            stringify!(z)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<longlong4>())).w as *const _ as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(longlong4),
            "::",
            stringify!(w)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct ulonglong4 {
    pub x: ::std::os::raw::c_ulonglong,
    pub y: ::std::os::raw::c_ulonglong,
    pub z: ::std::os::raw::c_ulonglong,
    pub w: ::std::os::raw::c_ulonglong,
}
#[test]
fn bindgen_test_layout_ulonglong4() {
    assert_eq!(
        ::std::mem::size_of::<ulonglong4>(),
        32usize,
        concat!("Size of: ", stringify!(ulonglong4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulonglong4>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(ulonglong4),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulonglong4>())).y as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(ulonglong4),
            "::",
            stringify!(y)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulonglong4>())).z as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(ulonglong4),
            "::",
            stringify!(z)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<ulonglong4>())).w as *const _ as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(ulonglong4),
            "::",
            stringify!(w)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct double1 {
    pub x: f64,
}
#[test]
fn bindgen_test_layout_double1() {
    assert_eq!(
        ::std::mem::size_of::<double1>(),
        8usize,
        concat!("Size of: ", stringify!(double1))
    );
    assert_eq!(
        ::std::mem::align_of::<double1>(),
        8usize,
        concat!("Alignment of ", stringify!(double1))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<double1>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(double1),
            "::",
            stringify!(x)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct double2 {
    pub x: f64,
    pub y: f64,
}
#[test]
fn bindgen_test_layout_double2() {
    assert_eq!(
        ::std::mem::size_of::<double2>(),
        16usize,
        concat!("Size of: ", stringify!(double2))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<double2>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(double2),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<double2>())).y as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(double2),
            "::",
            stringify!(y)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct double3 {
    pub x: f64,
    pub y: f64,
    pub z: f64,
}
#[test]
fn bindgen_test_layout_double3() {
    assert_eq!(
        ::std::mem::size_of::<double3>(),
        24usize,
        concat!("Size of: ", stringify!(double3))
    );
    assert_eq!(
        ::std::mem::align_of::<double3>(),
        8usize,
        concat!("Alignment of ", stringify!(double3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<double3>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(double3),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<double3>())).y as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(double3),
            "::",
            stringify!(y)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<double3>())).z as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(double3),
            "::",
            stringify!(z)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct double4 {
    pub x: f64,
    pub y: f64,
    pub z: f64,
    pub w: f64,
}
#[test]
fn bindgen_test_layout_double4() {
    assert_eq!(
        ::std::mem::size_of::<double4>(),
        32usize,
        concat!("Size of: ", stringify!(double4))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<double4>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(double4),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<double4>())).y as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(double4),
            "::",
            stringify!(y)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<double4>())).z as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(double4),
            "::",
            stringify!(z)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<double4>())).w as *const _ as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(double4),
            "::",
            stringify!(w)
        )
    );
}
///                                                                              *
///                                                                              *
///                                                                              *
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct dim3 {
    pub x: ::std::os::raw::c_uint,
    pub y: ::std::os::raw::c_uint,
    pub z: ::std::os::raw::c_uint,
}
#[test]
fn bindgen_test_layout_dim3() {
    assert_eq!(
        ::std::mem::size_of::<dim3>(),
        12usize,
        concat!("Size of: ", stringify!(dim3))
    );
    assert_eq!(
        ::std::mem::align_of::<dim3>(),
        4usize,
        concat!("Alignment of ", stringify!(dim3))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<dim3>())).x as *const _ as usize },
        0usize,
        concat!("Offset of field: ", stringify!(dim3), "::", stringify!(x))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<dim3>())).y as *const _ as usize },
        4usize,
        concat!("Offset of field: ", stringify!(dim3), "::", stringify!(y))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<dim3>())).z as *const _ as usize },
        8usize,
        concat!("Offset of field: ", stringify!(dim3), "::", stringify!(z))
    );
}
pub type wchar_t = ::std::os::raw::c_int;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct max_align_t {
    pub __clang_max_align_nonce1: ::std::os::raw::c_longlong,
    pub __bindgen_padding_0: u64,
    pub __clang_max_align_nonce2: f64,
}
#[test]
fn bindgen_test_layout_max_align_t() {
    assert_eq!(
        ::std::mem::size_of::<max_align_t>(),
        24usize, //Manual
        concat!("Size of: ", stringify!(max_align_t))
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<max_align_t>())).__clang_max_align_nonce1 as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(max_align_t),
            "::",
            stringify!(__clang_max_align_nonce1)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<max_align_t>())).__clang_max_align_nonce2 as *const _ as usize
        },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(max_align_t),
            "::",
            stringify!(__clang_max_align_nonce2)
        )
    );
}
/// The API call returned with no errors. In the case of query calls, this
/// also means that the operation being queried is complete (see
/// ::cudaEventQuery() and ::cudaStreamQuery()).
pub const cudaError_cudaSuccess: cudaError = 0;
/// The device function being invoked (usually via ::cudaLaunchKernel()) was not
/// previously configured via the ::cudaConfigureCall() function.
pub const cudaError_cudaErrorMissingConfiguration: cudaError = 1;
/// The API call failed because it was unable to allocate enough memory to
/// perform the requested operation.
pub const cudaError_cudaErrorMemoryAllocation: cudaError = 2;
/// The API call failed because the CUDA driver and runtime could not be
/// initialized.
pub const cudaError_cudaErrorInitializationError: cudaError = 3;
/// An exception occurred on the device while executing a kernel. Common
/// causes include dereferencing an invalid device pointer and accessing
/// out of bounds shared memory. All existing device memory allocations
/// are invalid. To continue using CUDA, the process must be terminated
/// and relaunched.
pub const cudaError_cudaErrorLaunchFailure: cudaError = 4;
/// This indicated that a previous kernel launch failed. This was previously
/// used for device emulation of kernel launches.
/// \deprecated
/// This error return is deprecated as of CUDA 3.1. Device emulation mode was
/// removed with the CUDA 3.1 release.
pub const cudaError_cudaErrorPriorLaunchFailure: cudaError = 5;
/// This indicates that the device kernel took too long to execute. This can
/// only occur if timeouts are enabled - see the device property
/// \ref ::cudaDeviceProp::kernelExecTimeoutEnabled "kernelExecTimeoutEnabled"
/// for more information.
/// This leaves the process in an inconsistent state and any further CUDA work
/// will return the same error. To continue using CUDA, the process must be terminated
/// and relaunched.
pub const cudaError_cudaErrorLaunchTimeout: cudaError = 6;
/// This indicates that a launch did not occur because it did not have
/// appropriate resources. Although this error is similar to
/// ::cudaErrorInvalidConfiguration, this error usually indicates that the
/// user has attempted to pass too many arguments to the device kernel, or the
/// kernel launch specifies too many threads for the kernel's register count.
pub const cudaError_cudaErrorLaunchOutOfResources: cudaError = 7;
/// The requested device function does not exist or is not compiled for the
/// proper device architecture.
pub const cudaError_cudaErrorInvalidDeviceFunction: cudaError = 8;
/// This indicates that a kernel launch is requesting resources that can
/// never be satisfied by the current device. Requesting more shared memory
/// per block than the device supports will trigger this error, as will
/// requesting too many threads or blocks. See ::cudaDeviceProp for more
/// device limitations.
pub const cudaError_cudaErrorInvalidConfiguration: cudaError = 9;
/// This indicates that the device ordinal supplied by the user does not
/// correspond to a valid CUDA device.
pub const cudaError_cudaErrorInvalidDevice: cudaError = 10;
/// This indicates that one or more of the parameters passed to the API call
/// is not within an acceptable range of values.
pub const cudaError_cudaErrorInvalidValue: cudaError = 11;
/// This indicates that one or more of the pitch-related parameters passed
/// to the API call is not within the acceptable range for pitch.
pub const cudaError_cudaErrorInvalidPitchValue: cudaError = 12;
/// This indicates that the symbol name/identifier passed to the API call
/// is not a valid name or identifier.
pub const cudaError_cudaErrorInvalidSymbol: cudaError = 13;
/// This indicates that the buffer object could not be mapped.
pub const cudaError_cudaErrorMapBufferObjectFailed: cudaError = 14;
/// This indicates that the buffer object could not be unmapped.
pub const cudaError_cudaErrorUnmapBufferObjectFailed: cudaError = 15;
/// This indicates that at least one host pointer passed to the API call is
/// not a valid host pointer.
pub const cudaError_cudaErrorInvalidHostPointer: cudaError = 16;
/// This indicates that at least one device pointer passed to the API call is
/// not a valid device pointer.
pub const cudaError_cudaErrorInvalidDevicePointer: cudaError = 17;
/// This indicates that the texture passed to the API call is not a valid
/// texture.
pub const cudaError_cudaErrorInvalidTexture: cudaError = 18;
/// This indicates that the texture binding is not valid. This occurs if you
/// call ::cudaGetTextureAlignmentOffset() with an unbound texture.
pub const cudaError_cudaErrorInvalidTextureBinding: cudaError = 19;
/// This indicates that the channel descriptor passed to the API call is not
/// valid. This occurs if the format is not one of the formats specified by
/// ::cudaChannelFormatKind, or if one of the dimensions is invalid.
pub const cudaError_cudaErrorInvalidChannelDescriptor: cudaError = 20;
/// This indicates that the direction of the memcpy passed to the API call is
/// not one of the types specified by ::cudaMemcpyKind.
pub const cudaError_cudaErrorInvalidMemcpyDirection: cudaError = 21;
/// This indicated that the user has taken the address of a constant variable,
/// which was forbidden up until the CUDA 3.1 release.
/// \deprecated
/// This error return is deprecated as of CUDA 3.1. Variables in constant
/// memory may now have their address taken by the runtime via
/// ::cudaGetSymbolAddress().
pub const cudaError_cudaErrorAddressOfConstant: cudaError = 22;
/// This indicated that a texture fetch was not able to be performed.
/// This was previously used for device emulation of texture operations.
/// \deprecated
/// This error return is deprecated as of CUDA 3.1. Device emulation mode was
/// removed with the CUDA 3.1 release.
pub const cudaError_cudaErrorTextureFetchFailed: cudaError = 23;
/// This indicated that a texture was not bound for access.
/// This was previously used for device emulation of texture operations.
/// \deprecated
/// This error return is deprecated as of CUDA 3.1. Device emulation mode was
/// removed with the CUDA 3.1 release.
pub const cudaError_cudaErrorTextureNotBound: cudaError = 24;
/// This indicated that a synchronization operation had failed.
/// This was previously used for some device emulation functions.
/// \deprecated
/// This error return is deprecated as of CUDA 3.1. Device emulation mode was
/// removed with the CUDA 3.1 release.
pub const cudaError_cudaErrorSynchronizationError: cudaError = 25;
/// This indicates that a non-float texture was being accessed with linear
/// filtering. This is not supported by CUDA.
pub const cudaError_cudaErrorInvalidFilterSetting: cudaError = 26;
/// This indicates that an attempt was made to read a non-float texture as a
/// normalized float. This is not supported by CUDA.
pub const cudaError_cudaErrorInvalidNormSetting: cudaError = 27;
/// Mixing of device and device emulation code was not allowed.
/// \deprecated
/// This error return is deprecated as of CUDA 3.1. Device emulation mode was
/// removed with the CUDA 3.1 release.
pub const cudaError_cudaErrorMixedDeviceExecution: cudaError = 28;
/// This indicates that a CUDA Runtime API call cannot be executed because
/// it is being called during process shut down, at a point in time after
/// CUDA driver has been unloaded.
pub const cudaError_cudaErrorCudartUnloading: cudaError = 29;
/// This indicates that an unknown internal error has occurred.
pub const cudaError_cudaErrorUnknown: cudaError = 30;
/// This indicates that the API call is not yet implemented. Production
/// releases of CUDA will never return this error.
/// \deprecated
/// This error return is deprecated as of CUDA 4.1.
pub const cudaError_cudaErrorNotYetImplemented: cudaError = 31;
/// This indicated that an emulated device pointer exceeded the 32-bit address
/// range.
/// \deprecated
/// This error return is deprecated as of CUDA 3.1. Device emulation mode was
/// removed with the CUDA 3.1 release.
pub const cudaError_cudaErrorMemoryValueTooLarge: cudaError = 32;
/// This indicates that a resource handle passed to the API call was not
/// valid. Resource handles are opaque types like ::cudaStream_t and
/// ::cudaEvent_t.
pub const cudaError_cudaErrorInvalidResourceHandle: cudaError = 33;
/// This indicates that asynchronous operations issued previously have not
/// completed yet. This result is not actually an error, but must be indicated
/// differently than ::cudaSuccess (which indicates completion). Calls that
/// may return this value include ::cudaEventQuery() and ::cudaStreamQuery().
pub const cudaError_cudaErrorNotReady: cudaError = 34;
/// This indicates that the installed NVIDIA CUDA driver is older than the
/// CUDA runtime library. This is not a supported configuration. Users should
/// install an updated NVIDIA display driver to allow the application to run.
pub const cudaError_cudaErrorInsufficientDriver: cudaError = 35;
/// This indicates that the user has called ::cudaSetValidDevices(),
/// ::cudaSetDeviceFlags(), ::cudaD3D9SetDirect3DDevice(),
/// ::cudaD3D10SetDirect3DDevice, ::cudaD3D11SetDirect3DDevice(), or
/// ::cudaVDPAUSetVDPAUDevice() after initializing the CUDA runtime by
/// calling non-device management operations (allocating memory and
/// launching kernels are examples of non-device management operations).
/// This error can also be returned if using runtime/driver
/// interoperability and there is an existing ::CUcontext active on the
/// host thread.
pub const cudaError_cudaErrorSetOnActiveProcess: cudaError = 36;
/// This indicates that the surface passed to the API call is not a valid
/// surface.
pub const cudaError_cudaErrorInvalidSurface: cudaError = 37;
/// This indicates that no CUDA-capable devices were detected by the installed
/// CUDA driver.
pub const cudaError_cudaErrorNoDevice: cudaError = 38;
/// This indicates that an uncorrectable ECC error was detected during
/// execution.
pub const cudaError_cudaErrorECCUncorrectable: cudaError = 39;
/// This indicates that a link to a shared object failed to resolve.
pub const cudaError_cudaErrorSharedObjectSymbolNotFound: cudaError = 40;
/// This indicates that initialization of a shared object failed.
pub const cudaError_cudaErrorSharedObjectInitFailed: cudaError = 41;
/// This indicates that the ::cudaLimit passed to the API call is not
/// supported by the active device.
pub const cudaError_cudaErrorUnsupportedLimit: cudaError = 42;
/// This indicates that multiple global or constant variables (across separate
/// CUDA source files in the application) share the same string name.
pub const cudaError_cudaErrorDuplicateVariableName: cudaError = 43;
/// This indicates that multiple textures (across separate CUDA source
/// files in the application) share the same string name.
pub const cudaError_cudaErrorDuplicateTextureName: cudaError = 44;
/// This indicates that multiple surfaces (across separate CUDA source
/// files in the application) share the same string name.
pub const cudaError_cudaErrorDuplicateSurfaceName: cudaError = 45;
/// This indicates that all CUDA devices are busy or unavailable at the current
/// time. Devices are often busy/unavailable due to use of
/// ::cudaComputeModeExclusive, ::cudaComputeModeProhibited or when long
/// running CUDA kernels have filled up the GPU and are blocking new work
/// from starting. They can also be unavailable due to memory constraints
/// on a device that already has active CUDA work being performed.
pub const cudaError_cudaErrorDevicesUnavailable: cudaError = 46;
/// This indicates that the device kernel image is invalid.
pub const cudaError_cudaErrorInvalidKernelImage: cudaError = 47;
/// This indicates that there is no kernel image available that is suitable
/// for the device. This can occur when a user specifies code generation
/// options for a particular CUDA source file that do not include the
/// corresponding device configuration.
pub const cudaError_cudaErrorNoKernelImageForDevice: cudaError = 48;
/// This indicates that the current context is not compatible with this
/// the CUDA Runtime. This can only occur if you are using CUDA
/// Runtime/Driver interoperability and have created an existing Driver
/// context using the driver API. The Driver context may be incompatible
/// either because the Driver context was created using an older version
/// of the API, because the Runtime API call expects a primary driver
/// context and the Driver context is not primary, or because the Driver
/// context has been destroyed. Please see \ref CUDART_DRIVER "Interactions
/// with the CUDA Driver API" for more information.
pub const cudaError_cudaErrorIncompatibleDriverContext: cudaError = 49;
/// This error indicates that a call to ::cudaDeviceEnablePeerAccess() is
/// trying to re-enable peer addressing on from a context which has already
/// had peer addressing enabled.
pub const cudaError_cudaErrorPeerAccessAlreadyEnabled: cudaError = 50;
/// This error indicates that ::cudaDeviceDisablePeerAccess() is trying to
/// disable peer addressing which has not been enabled yet via
/// ::cudaDeviceEnablePeerAccess().
pub const cudaError_cudaErrorPeerAccessNotEnabled: cudaError = 51;
/// This indicates that a call tried to access an exclusive-thread device that
/// is already in use by a different thread.
pub const cudaError_cudaErrorDeviceAlreadyInUse: cudaError = 54;
/// This indicates profiler is not initialized for this run. This can
/// happen when the application is running with external profiling tools
/// like visual profiler.
pub const cudaError_cudaErrorProfilerDisabled: cudaError = 55;
/// \deprecated
/// This error return is deprecated as of CUDA 5.0. It is no longer an error
/// to attempt to enable/disable the profiling via ::cudaProfilerStart or
/// ::cudaProfilerStop without initialization.
pub const cudaError_cudaErrorProfilerNotInitialized: cudaError = 56;
/// \deprecated
/// This error return is deprecated as of CUDA 5.0. It is no longer an error
/// to call cudaProfilerStart() when profiling is already enabled.
pub const cudaError_cudaErrorProfilerAlreadyStarted: cudaError = 57;
/// \deprecated
/// This error return is deprecated as of CUDA 5.0. It is no longer an error
/// to call cudaProfilerStop() when profiling is already disabled.
pub const cudaError_cudaErrorProfilerAlreadyStopped: cudaError = 58;
/// An assert triggered in device code during kernel execution. The device
/// cannot be used again. All existing allocations are invalid. To continue
/// using CUDA, the process must be terminated and relaunched.
pub const cudaError_cudaErrorAssert: cudaError = 59;
/// This error indicates that the hardware resources required to enable
/// peer access have been exhausted for one or more of the devices
/// passed to ::cudaEnablePeerAccess().
pub const cudaError_cudaErrorTooManyPeers: cudaError = 60;
/// This error indicates that the memory range passed to ::cudaHostRegister()
/// has already been registered.
pub const cudaError_cudaErrorHostMemoryAlreadyRegistered: cudaError = 61;
/// This error indicates that the pointer passed to ::cudaHostUnregister()
/// does not correspond to any currently registered memory region.
pub const cudaError_cudaErrorHostMemoryNotRegistered: cudaError = 62;
/// This error indicates that an OS call failed.
pub const cudaError_cudaErrorOperatingSystem: cudaError = 63;
/// This error indicates that P2P access is not supported across the given
/// devices.
pub const cudaError_cudaErrorPeerAccessUnsupported: cudaError = 64;
/// This error indicates that a device runtime grid launch did not occur
/// because the depth of the child grid would exceed the maximum supported
/// number of nested grid launches.
pub const cudaError_cudaErrorLaunchMaxDepthExceeded: cudaError = 65;
/// This error indicates that a grid launch did not occur because the kernel
/// uses file-scoped textures which are unsupported by the device runtime.
/// Kernels launched via the device runtime only support textures created with
/// the Texture Object API's.
pub const cudaError_cudaErrorLaunchFileScopedTex: cudaError = 66;
/// This error indicates that a grid launch did not occur because the kernel
/// uses file-scoped surfaces which are unsupported by the device runtime.
/// Kernels launched via the device runtime only support surfaces created with
/// the Surface Object API's.
pub const cudaError_cudaErrorLaunchFileScopedSurf: cudaError = 67;
/// This error indicates that a call to ::cudaDeviceSynchronize made from
/// the device runtime failed because the call was made at grid depth greater
/// than than either the default (2 levels of grids) or user specified device
/// limit ::cudaLimitDevRuntimeSyncDepth. To be able to synchronize on
/// launched grids at a greater depth successfully, the maximum nested
/// depth at which ::cudaDeviceSynchronize will be called must be specified
/// with the ::cudaLimitDevRuntimeSyncDepth limit to the ::cudaDeviceSetLimit
/// api before the host-side launch of a kernel using the device runtime.
/// Keep in mind that additional levels of sync depth require the runtime
/// to reserve large amounts of device memory that cannot be used for
/// user allocations.
pub const cudaError_cudaErrorSyncDepthExceeded: cudaError = 68;
/// This error indicates that a device runtime grid launch failed because
/// the launch would exceed the limit ::cudaLimitDevRuntimePendingLaunchCount.
/// For this launch to proceed successfully, ::cudaDeviceSetLimit must be
/// called to set the ::cudaLimitDevRuntimePendingLaunchCount to be higher
/// than the upper bound of outstanding launches that can be issued to the
/// device runtime. Keep in mind that raising the limit of pending device
/// runtime launches will require the runtime to reserve device memory that
/// cannot be used for user allocations.
pub const cudaError_cudaErrorLaunchPendingCountExceeded: cudaError = 69;
/// This error indicates the attempted operation is not permitted.
pub const cudaError_cudaErrorNotPermitted: cudaError = 70;
/// This error indicates the attempted operation is not supported
/// on the current system or device.
pub const cudaError_cudaErrorNotSupported: cudaError = 71;
/// Device encountered an error in the call stack during kernel execution,
/// possibly due to stack corruption or exceeding the stack size limit.
/// This leaves the process in an inconsistent state and any further CUDA work
/// will return the same error. To continue using CUDA, the process must be terminated
/// and relaunched.
pub const cudaError_cudaErrorHardwareStackError: cudaError = 72;
/// The device encountered an illegal instruction during kernel execution
/// This leaves the process in an inconsistent state and any further CUDA work
/// will return the same error. To continue using CUDA, the process must be terminated
/// and relaunched.
pub const cudaError_cudaErrorIllegalInstruction: cudaError = 73;
/// The device encountered a load or store instruction
/// on a memory address which is not aligned.
/// This leaves the process in an inconsistent state and any further CUDA work
/// will return the same error. To continue using CUDA, the process must be terminated
/// and relaunched.
pub const cudaError_cudaErrorMisalignedAddress: cudaError = 74;
/// While executing a kernel, the device encountered an instruction
/// which can only operate on memory locations in certain address spaces
/// (global, shared, or local), but was supplied a memory address not
/// belonging to an allowed address space.
/// This leaves the process in an inconsistent state and any further CUDA work
/// will return the same error. To continue using CUDA, the process must be terminated
/// and relaunched.
pub const cudaError_cudaErrorInvalidAddressSpace: cudaError = 75;
/// The device encountered an invalid program counter.
/// This leaves the process in an inconsistent state and any further CUDA work
/// will return the same error. To continue using CUDA, the process must be terminated
/// and relaunched.
pub const cudaError_cudaErrorInvalidPc: cudaError = 76;
/// The device encountered a load or store instruction on an invalid memory address.
/// This leaves the process in an inconsistent state and any further CUDA work
/// will return the same error. To continue using CUDA, the process must be terminated
/// and relaunched.
pub const cudaError_cudaErrorIllegalAddress: cudaError = 77;
/// A PTX compilation failed. The runtime may fall back to compiling PTX if
/// an application does not contain a suitable binary for the current device.
pub const cudaError_cudaErrorInvalidPtx: cudaError = 78;
/// This indicates an error with the OpenGL or DirectX context.
pub const cudaError_cudaErrorInvalidGraphicsContext: cudaError = 79;
/// This indicates that an uncorrectable NVLink error was detected during the
/// execution.
pub const cudaError_cudaErrorNvlinkUncorrectable: cudaError = 80;
/// This indicates that the PTX JIT compiler library was not found. The JIT Compiler
/// library is used for PTX compilation. The runtime may fall back to compiling PTX
/// if an application does not contain a suitable binary for the current device.
pub const cudaError_cudaErrorJitCompilerNotFound: cudaError = 81;
/// This error indicates that the number of blocks launched per grid for a kernel that was
/// launched via either ::cudaLaunchCooperativeKernel or ::cudaLaunchCooperativeKernelMultiDevice
/// exceeds the maximum number of blocks as allowed by ::cudaOccupancyMaxActiveBlocksPerMultiprocessor
/// or ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags times the number of multiprocessors
/// as specified by the device attribute ::cudaDevAttrMultiProcessorCount.
pub const cudaError_cudaErrorCooperativeLaunchTooLarge: cudaError = 82;
/// This error indicates that the system is not yet ready to start any CUDA
/// work.  To continue using CUDA, verify the system configuration is in a
/// valid state and all required driver daemons are actively running.
pub const cudaError_cudaErrorSystemNotReady: cudaError = 83;
/// This indicates that a resource required by the API call is not in a
/// valid state to perform the requested operation.
pub const cudaError_cudaErrorIllegalState: cudaError = 84;
/// This indicates an internal startup failure in the CUDA runtime.
pub const cudaError_cudaErrorStartupFailure: cudaError = 127;
/// The operation is not permitted when the stream is capturing.
pub const cudaError_cudaErrorStreamCaptureUnsupported: cudaError = 900;
/// The current capture sequence on the stream has been invalidated due to
/// a previous error.
pub const cudaError_cudaErrorStreamCaptureInvalidated: cudaError = 901;
/// The operation would have resulted in a merge of two independent capture
/// sequences.
pub const cudaError_cudaErrorStreamCaptureMerge: cudaError = 902;
/// The capture was not initiated in this stream.
pub const cudaError_cudaErrorStreamCaptureUnmatched: cudaError = 903;
/// The capture sequence contains a fork that was not joined to the primary
/// stream.
pub const cudaError_cudaErrorStreamCaptureUnjoined: cudaError = 904;
/// A dependency would have been created which crosses the capture sequence
/// boundary. Only implicit in-stream ordering dependencies are allowed to
/// cross the boundary.
pub const cudaError_cudaErrorStreamCaptureIsolation: cudaError = 905;
/// The operation would have resulted in a disallowed implicit dependency on
/// a current capture sequence from cudaStreamLegacy.
pub const cudaError_cudaErrorStreamCaptureImplicit: cudaError = 906;
/// The operation is not permitted on an event which was last recorded in a
/// capturing stream.
pub const cudaError_cudaErrorCapturedEvent: cudaError = 907;
/// Any unhandled CUDA driver error is added to this value and returned via
/// the runtime. Production releases of CUDA should not return such errors.
/// \deprecated
/// This error return is deprecated as of CUDA 4.1.
pub const cudaError_cudaErrorApiFailureBase: cudaError = 10000;
/// CUDA error types
pub type cudaError = u32;
///< Signed channel format
pub const cudaChannelFormatKind_cudaChannelFormatKindSigned: cudaChannelFormatKind = 0;
///< Unsigned channel format
pub const cudaChannelFormatKind_cudaChannelFormatKindUnsigned: cudaChannelFormatKind = 1;
///< Float channel format
pub const cudaChannelFormatKind_cudaChannelFormatKindFloat: cudaChannelFormatKind = 2;
///< No channel format
pub const cudaChannelFormatKind_cudaChannelFormatKindNone: cudaChannelFormatKind = 3;
/// Channel format kind
pub type cudaChannelFormatKind = u32;
/// CUDA Channel format descriptor
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaChannelFormatDesc {
    ///< x
    pub x: ::std::os::raw::c_int,
    ///< y
    pub y: ::std::os::raw::c_int,
    ///< z
    pub z: ::std::os::raw::c_int,
    ///< w
    pub w: ::std::os::raw::c_int,
    ///< Channel format kind
    pub f: cudaChannelFormatKind,
}
#[test]
fn bindgen_test_layout_cudaChannelFormatDesc() {
    assert_eq!(
        ::std::mem::size_of::<cudaChannelFormatDesc>(),
        20usize,
        concat!("Size of: ", stringify!(cudaChannelFormatDesc))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaChannelFormatDesc>(),
        4usize,
        concat!("Alignment of ", stringify!(cudaChannelFormatDesc))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaChannelFormatDesc>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaChannelFormatDesc),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaChannelFormatDesc>())).y as *const _ as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaChannelFormatDesc),
            "::",
            stringify!(y)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaChannelFormatDesc>())).z as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaChannelFormatDesc),
            "::",
            stringify!(z)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaChannelFormatDesc>())).w as *const _ as usize },
        12usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaChannelFormatDesc),
            "::",
            stringify!(w)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaChannelFormatDesc>())).f as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaChannelFormatDesc),
            "::",
            stringify!(f)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaArray {
    _unused: [u8; 0],
}
/// CUDA array
pub type cudaArray_t = *mut cudaArray;
/// CUDA array (as source copy argument)
pub type cudaArray_const_t = *const cudaArray;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaMipmappedArray {
    _unused: [u8; 0],
}
/// CUDA mipmapped array
pub type cudaMipmappedArray_t = *mut cudaMipmappedArray;
/// CUDA mipmapped array (as source argument)
pub type cudaMipmappedArray_const_t = *const cudaMipmappedArray;
///< Unregistered memory
pub const cudaMemoryType_cudaMemoryTypeUnregistered: cudaMemoryType = 0;
///< Host memory
pub const cudaMemoryType_cudaMemoryTypeHost: cudaMemoryType = 1;
///< Device memory
pub const cudaMemoryType_cudaMemoryTypeDevice: cudaMemoryType = 2;
///< Managed memory
pub const cudaMemoryType_cudaMemoryTypeManaged: cudaMemoryType = 3;
/// CUDA memory types
pub type cudaMemoryType = u32;
///< Host   -> Host
pub const cudaMemcpyKind_cudaMemcpyHostToHost: cudaMemcpyKind = 0;
///< Host   -> Device
pub const cudaMemcpyKind_cudaMemcpyHostToDevice: cudaMemcpyKind = 1;
///< Device -> Host
pub const cudaMemcpyKind_cudaMemcpyDeviceToHost: cudaMemcpyKind = 2;
///< Device -> Device
pub const cudaMemcpyKind_cudaMemcpyDeviceToDevice: cudaMemcpyKind = 3;
///< Direction of the transfer is inferred from the pointer values. Requires unified virtual addressing
pub const cudaMemcpyKind_cudaMemcpyDefault: cudaMemcpyKind = 4;
/// CUDA memory copy types
pub type cudaMemcpyKind = u32;
/// CUDA Pitched memory pointer
///
/// \sa ::make_cudaPitchedPtr
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaPitchedPtr {
    ///< Pointer to allocated memory
    pub ptr: *mut ::std::os::raw::c_void,
    ///< Pitch of allocated memory in bytes
    pub pitch: usize,
    ///< Logical width of allocation in elements
    pub xsize: usize,
    ///< Logical height of allocation in elements
    pub ysize: usize,
}
#[test]
fn bindgen_test_layout_cudaPitchedPtr() {
    assert_eq!(
        ::std::mem::size_of::<cudaPitchedPtr>(),
        32usize,
        concat!("Size of: ", stringify!(cudaPitchedPtr))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaPitchedPtr>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaPitchedPtr))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaPitchedPtr>())).ptr as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPitchedPtr),
            "::",
            stringify!(ptr)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaPitchedPtr>())).pitch as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPitchedPtr),
            "::",
            stringify!(pitch)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaPitchedPtr>())).xsize as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPitchedPtr),
            "::",
            stringify!(xsize)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaPitchedPtr>())).ysize as *const _ as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPitchedPtr),
            "::",
            stringify!(ysize)
        )
    );
}
/// CUDA extent
///
/// \sa ::make_cudaExtent
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaExtent {
    ///< Width in elements when referring to array memory, in bytes when referring to linear memory
    pub width: usize,
    ///< Height in elements
    pub height: usize,
    ///< Depth in elements
    pub depth: usize,
}
#[test]
fn bindgen_test_layout_cudaExtent() {
    assert_eq!(
        ::std::mem::size_of::<cudaExtent>(),
        24usize,
        concat!("Size of: ", stringify!(cudaExtent))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExtent>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaExtent))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaExtent>())).width as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExtent),
            "::",
            stringify!(width)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaExtent>())).height as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExtent),
            "::",
            stringify!(height)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaExtent>())).depth as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExtent),
            "::",
            stringify!(depth)
        )
    );
}
/// CUDA 3D position
///
/// \sa ::make_cudaPos
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaPos {
    ///< x
    pub x: usize,
    ///< y
    pub y: usize,
    ///< z
    pub z: usize,
}
#[test]
fn bindgen_test_layout_cudaPos() {
    assert_eq!(
        ::std::mem::size_of::<cudaPos>(),
        24usize,
        concat!("Size of: ", stringify!(cudaPos))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaPos>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaPos))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaPos>())).x as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPos),
            "::",
            stringify!(x)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaPos>())).y as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPos),
            "::",
            stringify!(y)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaPos>())).z as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPos),
            "::",
            stringify!(z)
        )
    );
}
/// CUDA 3D memory copying parameters
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaMemcpy3DParms {
    ///< Source memory address
    pub srcArray: cudaArray_t,
    ///< Source position offset
    pub srcPos: cudaPos,
    ///< Pitched source memory address
    pub srcPtr: cudaPitchedPtr,
    ///< Destination memory address
    pub dstArray: cudaArray_t,
    ///< Destination position offset
    pub dstPos: cudaPos,
    ///< Pitched destination memory address
    pub dstPtr: cudaPitchedPtr,
    ///< Requested memory copy size
    pub extent: cudaExtent,
    ///< Type of transfer
    pub kind: cudaMemcpyKind,
}
#[test]
fn bindgen_test_layout_cudaMemcpy3DParms() {
    assert_eq!(
        ::std::mem::size_of::<cudaMemcpy3DParms>(),
        160usize,
        concat!("Size of: ", stringify!(cudaMemcpy3DParms))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaMemcpy3DParms>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaMemcpy3DParms))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DParms>())).srcArray as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DParms),
            "::",
            stringify!(srcArray)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DParms>())).srcPos as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DParms),
            "::",
            stringify!(srcPos)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DParms>())).srcPtr as *const _ as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DParms),
            "::",
            stringify!(srcPtr)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DParms>())).dstArray as *const _ as usize },
        64usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DParms),
            "::",
            stringify!(dstArray)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DParms>())).dstPos as *const _ as usize },
        72usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DParms),
            "::",
            stringify!(dstPos)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DParms>())).dstPtr as *const _ as usize },
        96usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DParms),
            "::",
            stringify!(dstPtr)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DParms>())).extent as *const _ as usize },
        128usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DParms),
            "::",
            stringify!(extent)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DParms>())).kind as *const _ as usize },
        152usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DParms),
            "::",
            stringify!(kind)
        )
    );
}
/// CUDA 3D cross-device memory copying parameters
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaMemcpy3DPeerParms {
    ///< Source memory address
    pub srcArray: cudaArray_t,
    ///< Source position offset
    pub srcPos: cudaPos,
    ///< Pitched source memory address
    pub srcPtr: cudaPitchedPtr,
    ///< Source device
    pub srcDevice: ::std::os::raw::c_int,
    ///< Destination memory address
    pub dstArray: cudaArray_t,
    ///< Destination position offset
    pub dstPos: cudaPos,
    ///< Pitched destination memory address
    pub dstPtr: cudaPitchedPtr,
    ///< Destination device
    pub dstDevice: ::std::os::raw::c_int,
    ///< Requested memory copy size
    pub extent: cudaExtent,
}
#[test]
fn bindgen_test_layout_cudaMemcpy3DPeerParms() {
    assert_eq!(
        ::std::mem::size_of::<cudaMemcpy3DPeerParms>(),
        168usize,
        concat!("Size of: ", stringify!(cudaMemcpy3DPeerParms))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaMemcpy3DPeerParms>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaMemcpy3DPeerParms))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DPeerParms>())).srcArray as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DPeerParms),
            "::",
            stringify!(srcArray)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DPeerParms>())).srcPos as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DPeerParms),
            "::",
            stringify!(srcPos)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DPeerParms>())).srcPtr as *const _ as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DPeerParms),
            "::",
            stringify!(srcPtr)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DPeerParms>())).srcDevice as *const _ as usize },
        64usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DPeerParms),
            "::",
            stringify!(srcDevice)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DPeerParms>())).dstArray as *const _ as usize },
        72usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DPeerParms),
            "::",
            stringify!(dstArray)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DPeerParms>())).dstPos as *const _ as usize },
        80usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DPeerParms),
            "::",
            stringify!(dstPos)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DPeerParms>())).dstPtr as *const _ as usize },
        104usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DPeerParms),
            "::",
            stringify!(dstPtr)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DPeerParms>())).dstDevice as *const _ as usize },
        136usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DPeerParms),
            "::",
            stringify!(dstDevice)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemcpy3DPeerParms>())).extent as *const _ as usize },
        144usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemcpy3DPeerParms),
            "::",
            stringify!(extent)
        )
    );
}
/// CUDA Memset node parameters
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaMemsetParams {
    ///< Destination device pointer
    pub dst: *mut ::std::os::raw::c_void,
    ///< Pitch of destination device pointer. Unused if height is 1
    pub pitch: usize,
    ///< Value to be set
    pub value: ::std::os::raw::c_uint,
    ///< Size of each element in bytes. Must be 1, 2, or 4.
    pub elementSize: ::std::os::raw::c_uint,
    ///< Width in bytes, of the row
    pub width: usize,
    ///< Number of rows
    pub height: usize,
}
#[test]
fn bindgen_test_layout_cudaMemsetParams() {
    assert_eq!(
        ::std::mem::size_of::<cudaMemsetParams>(),
        40usize,
        concat!("Size of: ", stringify!(cudaMemsetParams))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaMemsetParams>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaMemsetParams))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemsetParams>())).dst as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemsetParams),
            "::",
            stringify!(dst)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemsetParams>())).pitch as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemsetParams),
            "::",
            stringify!(pitch)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemsetParams>())).value as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemsetParams),
            "::",
            stringify!(value)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemsetParams>())).elementSize as *const _ as usize },
        20usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemsetParams),
            "::",
            stringify!(elementSize)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemsetParams>())).width as *const _ as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemsetParams),
            "::",
            stringify!(width)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaMemsetParams>())).height as *const _ as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaMemsetParams),
            "::",
            stringify!(height)
        )
    );
}
/// CUDA host function
/// \param userData Argument value passed to the function
pub type cudaHostFn_t =
    ::std::option::Option<unsafe extern "C" fn(userData: *mut ::std::os::raw::c_void)>;
/// CUDA host node parameters
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaHostNodeParams {
    ///< The function to call when the node executes
    pub fn_: cudaHostFn_t,
    ///< Argument to pass to the function
    pub userData: *mut ::std::os::raw::c_void,
}
#[test]
fn bindgen_test_layout_cudaHostNodeParams() {
    assert_eq!(
        ::std::mem::size_of::<cudaHostNodeParams>(),
        16usize,
        concat!("Size of: ", stringify!(cudaHostNodeParams))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaHostNodeParams>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaHostNodeParams))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaHostNodeParams>())).fn_ as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaHostNodeParams),
            "::",
            stringify!(fn_)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaHostNodeParams>())).userData as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaHostNodeParams),
            "::",
            stringify!(userData)
        )
    );
}
///< Stream is not capturing
pub const cudaStreamCaptureStatus_cudaStreamCaptureStatusNone: cudaStreamCaptureStatus = 0;
///< Stream is actively capturing
pub const cudaStreamCaptureStatus_cudaStreamCaptureStatusActive: cudaStreamCaptureStatus = 1;
///< Stream is part of a capture sequence that
///has been invalidated, but not terminated
pub const cudaStreamCaptureStatus_cudaStreamCaptureStatusInvalidated: cudaStreamCaptureStatus = 2;
/// Possible stream capture statuses returned by ::cudaStreamIsCapturing
pub type cudaStreamCaptureStatus = u32;
/// CUDA graphics interop resource
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaGraphicsResource {
    _unused: [u8; 0],
}
///< Default
pub const cudaGraphicsRegisterFlags_cudaGraphicsRegisterFlagsNone: cudaGraphicsRegisterFlags = 0;
///< CUDA will not write to this resource
pub const cudaGraphicsRegisterFlags_cudaGraphicsRegisterFlagsReadOnly: cudaGraphicsRegisterFlags =
    1;
///< CUDA will only write to and will not read from this resource
pub const cudaGraphicsRegisterFlags_cudaGraphicsRegisterFlagsWriteDiscard:
    cudaGraphicsRegisterFlags = 2;
///< CUDA will bind this resource to a surface reference
pub const cudaGraphicsRegisterFlags_cudaGraphicsRegisterFlagsSurfaceLoadStore:
    cudaGraphicsRegisterFlags = 4;
///< CUDA will perform texture gather operations on this resource
pub const cudaGraphicsRegisterFlags_cudaGraphicsRegisterFlagsTextureGather:
    cudaGraphicsRegisterFlags = 8;
/// CUDA graphics interop register flags
pub type cudaGraphicsRegisterFlags = u32;
///< Default; Assume resource can be read/written
pub const cudaGraphicsMapFlags_cudaGraphicsMapFlagsNone: cudaGraphicsMapFlags = 0;
///< CUDA will not write to this resource
pub const cudaGraphicsMapFlags_cudaGraphicsMapFlagsReadOnly: cudaGraphicsMapFlags = 1;
///< CUDA will only write to and will not read from this resource
pub const cudaGraphicsMapFlags_cudaGraphicsMapFlagsWriteDiscard: cudaGraphicsMapFlags = 2;
/// CUDA graphics interop map flags
pub type cudaGraphicsMapFlags = u32;
///< Positive X face of cubemap
pub const cudaGraphicsCubeFace_cudaGraphicsCubeFacePositiveX: cudaGraphicsCubeFace = 0;
///< Negative X face of cubemap
pub const cudaGraphicsCubeFace_cudaGraphicsCubeFaceNegativeX: cudaGraphicsCubeFace = 1;
///< Positive Y face of cubemap
pub const cudaGraphicsCubeFace_cudaGraphicsCubeFacePositiveY: cudaGraphicsCubeFace = 2;
///< Negative Y face of cubemap
pub const cudaGraphicsCubeFace_cudaGraphicsCubeFaceNegativeY: cudaGraphicsCubeFace = 3;
///< Positive Z face of cubemap
pub const cudaGraphicsCubeFace_cudaGraphicsCubeFacePositiveZ: cudaGraphicsCubeFace = 4;
///< Negative Z face of cubemap
pub const cudaGraphicsCubeFace_cudaGraphicsCubeFaceNegativeZ: cudaGraphicsCubeFace = 5;
/// CUDA graphics interop array indices for cube maps
pub type cudaGraphicsCubeFace = u32;
///< Array resource
pub const cudaResourceType_cudaResourceTypeArray: cudaResourceType = 0;
///< Mipmapped array resource
pub const cudaResourceType_cudaResourceTypeMipmappedArray: cudaResourceType = 1;
///< Linear resource
pub const cudaResourceType_cudaResourceTypeLinear: cudaResourceType = 2;
///< Pitch 2D resource
pub const cudaResourceType_cudaResourceTypePitch2D: cudaResourceType = 3;
/// CUDA resource types
pub type cudaResourceType = u32;
///< No resource view format (use underlying resource format)
pub const cudaResourceViewFormat_cudaResViewFormatNone: cudaResourceViewFormat = 0;
///< 1 channel unsigned 8-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedChar1: cudaResourceViewFormat = 1;
///< 2 channel unsigned 8-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedChar2: cudaResourceViewFormat = 2;
///< 4 channel unsigned 8-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedChar4: cudaResourceViewFormat = 3;
///< 1 channel signed 8-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatSignedChar1: cudaResourceViewFormat = 4;
///< 2 channel signed 8-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatSignedChar2: cudaResourceViewFormat = 5;
///< 4 channel signed 8-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatSignedChar4: cudaResourceViewFormat = 6;
///< 1 channel unsigned 16-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedShort1: cudaResourceViewFormat = 7;
///< 2 channel unsigned 16-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedShort2: cudaResourceViewFormat = 8;
///< 4 channel unsigned 16-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedShort4: cudaResourceViewFormat = 9;
///< 1 channel signed 16-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatSignedShort1: cudaResourceViewFormat = 10;
///< 2 channel signed 16-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatSignedShort2: cudaResourceViewFormat = 11;
///< 4 channel signed 16-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatSignedShort4: cudaResourceViewFormat = 12;
///< 1 channel unsigned 32-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedInt1: cudaResourceViewFormat = 13;
///< 2 channel unsigned 32-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedInt2: cudaResourceViewFormat = 14;
///< 4 channel unsigned 32-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedInt4: cudaResourceViewFormat = 15;
///< 1 channel signed 32-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatSignedInt1: cudaResourceViewFormat = 16;
///< 2 channel signed 32-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatSignedInt2: cudaResourceViewFormat = 17;
///< 4 channel signed 32-bit integers
pub const cudaResourceViewFormat_cudaResViewFormatSignedInt4: cudaResourceViewFormat = 18;
///< 1 channel 16-bit floating point
pub const cudaResourceViewFormat_cudaResViewFormatHalf1: cudaResourceViewFormat = 19;
///< 2 channel 16-bit floating point
pub const cudaResourceViewFormat_cudaResViewFormatHalf2: cudaResourceViewFormat = 20;
///< 4 channel 16-bit floating point
pub const cudaResourceViewFormat_cudaResViewFormatHalf4: cudaResourceViewFormat = 21;
///< 1 channel 32-bit floating point
pub const cudaResourceViewFormat_cudaResViewFormatFloat1: cudaResourceViewFormat = 22;
///< 2 channel 32-bit floating point
pub const cudaResourceViewFormat_cudaResViewFormatFloat2: cudaResourceViewFormat = 23;
///< 4 channel 32-bit floating point
pub const cudaResourceViewFormat_cudaResViewFormatFloat4: cudaResourceViewFormat = 24;
///< Block compressed 1
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedBlockCompressed1: cudaResourceViewFormat =
    25;
///< Block compressed 2
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedBlockCompressed2: cudaResourceViewFormat =
    26;
///< Block compressed 3
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedBlockCompressed3: cudaResourceViewFormat =
    27;
///< Block compressed 4 unsigned
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedBlockCompressed4: cudaResourceViewFormat =
    28;
///< Block compressed 4 signed
pub const cudaResourceViewFormat_cudaResViewFormatSignedBlockCompressed4: cudaResourceViewFormat =
    29;
///< Block compressed 5 unsigned
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedBlockCompressed5: cudaResourceViewFormat =
    30;
///< Block compressed 5 signed
pub const cudaResourceViewFormat_cudaResViewFormatSignedBlockCompressed5: cudaResourceViewFormat =
    31;
///< Block compressed 6 unsigned half-float
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedBlockCompressed6H:
    cudaResourceViewFormat = 32;
///< Block compressed 6 signed half-float
pub const cudaResourceViewFormat_cudaResViewFormatSignedBlockCompressed6H: cudaResourceViewFormat =
    33;
///< Block compressed 7
pub const cudaResourceViewFormat_cudaResViewFormatUnsignedBlockCompressed7: cudaResourceViewFormat =
    34;
/// CUDA texture resource view formats
pub type cudaResourceViewFormat = u32;
/// CUDA resource descriptor
#[repr(C)]
#[derive(Copy, Clone)]
pub struct cudaResourceDesc {
    ///< Resource type
    pub resType: cudaResourceType,
    pub res: cudaResourceDesc__bindgen_ty_1,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union cudaResourceDesc__bindgen_ty_1 {
    pub array: cudaResourceDesc__bindgen_ty_1__bindgen_ty_1,
    pub mipmap: cudaResourceDesc__bindgen_ty_1__bindgen_ty_2,
    pub linear: cudaResourceDesc__bindgen_ty_1__bindgen_ty_3,
    pub pitch2D: cudaResourceDesc__bindgen_ty_1__bindgen_ty_4,
    _bindgen_union_align: [u64; 7usize],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaResourceDesc__bindgen_ty_1__bindgen_ty_1 {
    ///< CUDA array
    pub array: cudaArray_t,
}
#[test]
fn bindgen_test_layout_cudaResourceDesc__bindgen_ty_1__bindgen_ty_1() {
    assert_eq!(
        ::std::mem::size_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_1>(),
        8usize,
        concat!(
            "Size of: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_1)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_1>())).array
                as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_1),
            "::",
            stringify!(array)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaResourceDesc__bindgen_ty_1__bindgen_ty_2 {
    ///< CUDA mipmapped array
    pub mipmap: cudaMipmappedArray_t,
}
#[test]
fn bindgen_test_layout_cudaResourceDesc__bindgen_ty_1__bindgen_ty_2() {
    assert_eq!(
        ::std::mem::size_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_2>(),
        8usize,
        concat!(
            "Size of: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_2)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_2>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_2)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_2>())).mipmap
                as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_2),
            "::",
            stringify!(mipmap)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaResourceDesc__bindgen_ty_1__bindgen_ty_3 {
    ///< Device pointer
    pub devPtr: *mut ::std::os::raw::c_void,
    ///< Channel descriptor
    pub desc: cudaChannelFormatDesc,
    ///< Size in bytes
    pub sizeInBytes: usize,
}
#[test]
fn bindgen_test_layout_cudaResourceDesc__bindgen_ty_1__bindgen_ty_3() {
    assert_eq!(
        ::std::mem::size_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_3>(),
        40usize,
        concat!(
            "Size of: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_3)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_3>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_3)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_3>())).devPtr
                as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_3),
            "::",
            stringify!(devPtr)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_3>())).desc
                as *const _ as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_3),
            "::",
            stringify!(desc)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_3>())).sizeInBytes
                as *const _ as usize
        },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_3),
            "::",
            stringify!(sizeInBytes)
        )
    );
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaResourceDesc__bindgen_ty_1__bindgen_ty_4 {
    ///< Device pointer
    pub devPtr: *mut ::std::os::raw::c_void,
    ///< Channel descriptor
    pub desc: cudaChannelFormatDesc,
    ///< Width of the array in elements
    pub width: usize,
    ///< Height of the array in elements
    pub height: usize,
    ///< Pitch between two rows in bytes
    pub pitchInBytes: usize,
}
#[test]
fn bindgen_test_layout_cudaResourceDesc__bindgen_ty_1__bindgen_ty_4() {
    assert_eq!(
        ::std::mem::size_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_4>(),
        56usize,
        concat!(
            "Size of: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_4)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_4>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_4)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_4>())).devPtr
                as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_4),
            "::",
            stringify!(devPtr)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_4>())).desc
                as *const _ as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_4),
            "::",
            stringify!(desc)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_4>())).width
                as *const _ as usize
        },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_4),
            "::",
            stringify!(width)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_4>())).height
                as *const _ as usize
        },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_4),
            "::",
            stringify!(height)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1__bindgen_ty_4>())).pitchInBytes
                as *const _ as usize
        },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1__bindgen_ty_4),
            "::",
            stringify!(pitchInBytes)
        )
    );
}
#[test]
fn bindgen_test_layout_cudaResourceDesc__bindgen_ty_1() {
    assert_eq!(
        ::std::mem::size_of::<cudaResourceDesc__bindgen_ty_1>(),
        56usize,
        concat!("Size of: ", stringify!(cudaResourceDesc__bindgen_ty_1))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaResourceDesc__bindgen_ty_1>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaResourceDesc__bindgen_ty_1))
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1>())).array as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1),
            "::",
            stringify!(array)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1>())).mipmap as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1),
            "::",
            stringify!(mipmap)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1>())).linear as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1),
            "::",
            stringify!(linear)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceDesc__bindgen_ty_1>())).pitch2D as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc__bindgen_ty_1),
            "::",
            stringify!(pitch2D)
        )
    );
}
#[test]
fn bindgen_test_layout_cudaResourceDesc() {
    assert_eq!(
        ::std::mem::size_of::<cudaResourceDesc>(),
        64usize,
        concat!("Size of: ", stringify!(cudaResourceDesc))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaResourceDesc>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaResourceDesc))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaResourceDesc>())).resType as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc),
            "::",
            stringify!(resType)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaResourceDesc>())).res as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceDesc),
            "::",
            stringify!(res)
        )
    );
}
/// CUDA resource view descriptor
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaResourceViewDesc {
    ///< Resource view format
    pub format: cudaResourceViewFormat,
    ///< Width of the resource view
    pub width: usize,
    ///< Height of the resource view
    pub height: usize,
    ///< Depth of the resource view
    pub depth: usize,
    ///< First defined mipmap level
    pub firstMipmapLevel: ::std::os::raw::c_uint,
    ///< Last defined mipmap level
    pub lastMipmapLevel: ::std::os::raw::c_uint,
    ///< First layer index
    pub firstLayer: ::std::os::raw::c_uint,
    ///< Last layer index
    pub lastLayer: ::std::os::raw::c_uint,
}
#[test]
fn bindgen_test_layout_cudaResourceViewDesc() {
    assert_eq!(
        ::std::mem::size_of::<cudaResourceViewDesc>(),
        48usize,
        concat!("Size of: ", stringify!(cudaResourceViewDesc))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaResourceViewDesc>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaResourceViewDesc))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaResourceViewDesc>())).format as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceViewDesc),
            "::",
            stringify!(format)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaResourceViewDesc>())).width as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceViewDesc),
            "::",
            stringify!(width)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaResourceViewDesc>())).height as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceViewDesc),
            "::",
            stringify!(height)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaResourceViewDesc>())).depth as *const _ as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceViewDesc),
            "::",
            stringify!(depth)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceViewDesc>())).firstMipmapLevel as *const _ as usize
        },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceViewDesc),
            "::",
            stringify!(firstMipmapLevel)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaResourceViewDesc>())).lastMipmapLevel as *const _ as usize
        },
        36usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceViewDesc),
            "::",
            stringify!(lastMipmapLevel)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaResourceViewDesc>())).firstLayer as *const _ as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceViewDesc),
            "::",
            stringify!(firstLayer)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaResourceViewDesc>())).lastLayer as *const _ as usize },
        44usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaResourceViewDesc),
            "::",
            stringify!(lastLayer)
        )
    );
}
/// CUDA pointer attributes
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaPointerAttributes {
    /// \deprecated
    ///
    /// The physical location of the memory, ::cudaMemoryTypeHost or
    /// ::cudaMemoryTypeDevice. Note that managed memory can return either
    /// ::cudaMemoryTypeDevice or ::cudaMemoryTypeHost regardless of it's
    /// physical location.
    pub memoryType: cudaMemoryType,
    /// The type of memory - ::cudaMemoryTypeUnregistered, ::cudaMemoryTypeHost,
    /// ::cudaMemoryTypeDevice or ::cudaMemoryTypeManaged.
    pub type_: cudaMemoryType,
    /// The device against which the memory was allocated or registered.
    /// If the memory type is ::cudaMemoryTypeDevice then this identifies
    /// the device on which the memory referred physically resides.  If
    /// the memory type is ::cudaMemoryTypeHost or::cudaMemoryTypeManaged then
    /// this identifies the device which was current when the memory was allocated
    /// or registered (and if that device is deinitialized then this allocation
    /// will vanish with that device's state).
    pub device: ::std::os::raw::c_int,
    /// The address which may be dereferenced on the current device to access
    /// the memory or NULL if no such address exists.
    pub devicePointer: *mut ::std::os::raw::c_void,
    /// The address which may be dereferenced on the host to access the
    /// memory or NULL if no such address exists.
    ///
    /// \note CUDA doesn't check if unregistered memory is allocated so this field
    /// may contain invalid pointer if an invalid pointer has been passed to CUDA.
    pub hostPointer: *mut ::std::os::raw::c_void,
    /// \deprecated
    ///
    /// Indicates if this pointer points to managed memory
    pub isManaged: ::std::os::raw::c_int,
}
#[test]
fn bindgen_test_layout_cudaPointerAttributes() {
    assert_eq!(
        ::std::mem::size_of::<cudaPointerAttributes>(),
        40usize,
        concat!("Size of: ", stringify!(cudaPointerAttributes))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaPointerAttributes>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaPointerAttributes))
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaPointerAttributes>())).memoryType as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPointerAttributes),
            "::",
            stringify!(memoryType)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaPointerAttributes>())).type_ as *const _ as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPointerAttributes),
            "::",
            stringify!(type_)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaPointerAttributes>())).device as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPointerAttributes),
            "::",
            stringify!(device)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaPointerAttributes>())).devicePointer as *const _ as usize
        },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPointerAttributes),
            "::",
            stringify!(devicePointer)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaPointerAttributes>())).hostPointer as *const _ as usize
        },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPointerAttributes),
            "::",
            stringify!(hostPointer)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaPointerAttributes>())).isManaged as *const _ as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaPointerAttributes),
            "::",
            stringify!(isManaged)
        )
    );
}
/// CUDA function attributes
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaFuncAttributes {
    /// The size in bytes of statically-allocated shared memory per block
    /// required by this function. This does not include dynamically-allocated
    /// shared memory requested by the user at runtime.
    pub sharedSizeBytes: usize,
    /// The size in bytes of user-allocated constant memory required by this
    /// function.
    pub constSizeBytes: usize,
    /// The size in bytes of local memory used by each thread of this function.
    pub localSizeBytes: usize,
    /// The maximum number of threads per block, beyond which a launch of the
    /// function would fail. This number depends on both the function and the
    /// device on which the function is currently loaded.
    pub maxThreadsPerBlock: ::std::os::raw::c_int,
    /// The number of registers used by each thread of this function.
    pub numRegs: ::std::os::raw::c_int,
    /// The PTX virtual architecture version for which the function was
    /// compiled. This value is the major PTX version * 10 + the minor PTX
    /// version, so a PTX version 1.3 function would return the value 13.
    pub ptxVersion: ::std::os::raw::c_int,
    /// The binary architecture version for which the function was compiled.
    /// This value is the major binary version * 10 + the minor binary version,
    /// so a binary version 1.3 function would return the value 13.
    pub binaryVersion: ::std::os::raw::c_int,
    /// The attribute to indicate whether the function has been compiled with
    /// user specified option "-Xptxas --dlcm=ca" set.
    pub cacheModeCA: ::std::os::raw::c_int,
    /// The maximum size in bytes of dynamic shared memory per block for
    /// this function. Any launch must have a dynamic shared memory size
    /// smaller than this value.
    pub maxDynamicSharedSizeBytes: ::std::os::raw::c_int,
    /// On devices where the L1 cache and shared memory use the same hardware resources,
    /// this sets the shared memory carveout preference, in percent of the maximum shared memory.
    /// This is only a hint, and the driver can choose a different ratio if required to execute the function.
    pub preferredShmemCarveout: ::std::os::raw::c_int,
}
#[test]
fn bindgen_test_layout_cudaFuncAttributes() {
    assert_eq!(
        ::std::mem::size_of::<cudaFuncAttributes>(),
        56usize,
        concat!("Size of: ", stringify!(cudaFuncAttributes))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaFuncAttributes>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaFuncAttributes))
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaFuncAttributes>())).sharedSizeBytes as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaFuncAttributes),
            "::",
            stringify!(sharedSizeBytes)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaFuncAttributes>())).constSizeBytes as *const _ as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaFuncAttributes),
            "::",
            stringify!(constSizeBytes)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaFuncAttributes>())).localSizeBytes as *const _ as usize
        },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaFuncAttributes),
            "::",
            stringify!(localSizeBytes)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaFuncAttributes>())).maxThreadsPerBlock as *const _ as usize
        },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaFuncAttributes),
            "::",
            stringify!(maxThreadsPerBlock)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaFuncAttributes>())).numRegs as *const _ as usize },
        28usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaFuncAttributes),
            "::",
            stringify!(numRegs)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaFuncAttributes>())).ptxVersion as *const _ as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaFuncAttributes),
            "::",
            stringify!(ptxVersion)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaFuncAttributes>())).binaryVersion as *const _ as usize
        },
        36usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaFuncAttributes),
            "::",
            stringify!(binaryVersion)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaFuncAttributes>())).cacheModeCA as *const _ as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaFuncAttributes),
            "::",
            stringify!(cacheModeCA)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaFuncAttributes>())).maxDynamicSharedSizeBytes as *const _
                as usize
        },
        44usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaFuncAttributes),
            "::",
            stringify!(maxDynamicSharedSizeBytes)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaFuncAttributes>())).preferredShmemCarveout as *const _
                as usize
        },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaFuncAttributes),
            "::",
            stringify!(preferredShmemCarveout)
        )
    );
}
///< Maximum dynamic shared memory size
pub const cudaFuncAttribute_cudaFuncAttributeMaxDynamicSharedMemorySize: cudaFuncAttribute = 8;
///< Preferred shared memory-L1 cache split ratio
pub const cudaFuncAttribute_cudaFuncAttributePreferredSharedMemoryCarveout: cudaFuncAttribute = 9;
pub const cudaFuncAttribute_cudaFuncAttributeMax: cudaFuncAttribute = 10;
/// CUDA function attributes that can be set using cudaFuncSetAttribute
pub type cudaFuncAttribute = u32;
///< Default function cache configuration, no preference
pub const cudaFuncCache_cudaFuncCachePreferNone: cudaFuncCache = 0;
///< Prefer larger shared memory and smaller L1 cache
pub const cudaFuncCache_cudaFuncCachePreferShared: cudaFuncCache = 1;
///< Prefer larger L1 cache and smaller shared memory
pub const cudaFuncCache_cudaFuncCachePreferL1: cudaFuncCache = 2;
///< Prefer equal size L1 cache and shared memory
pub const cudaFuncCache_cudaFuncCachePreferEqual: cudaFuncCache = 3;
/// CUDA function cache configurations
pub type cudaFuncCache = u32;
pub const cudaSharedMemConfig_cudaSharedMemBankSizeDefault: cudaSharedMemConfig = 0;
pub const cudaSharedMemConfig_cudaSharedMemBankSizeFourByte: cudaSharedMemConfig = 1;
pub const cudaSharedMemConfig_cudaSharedMemBankSizeEightByte: cudaSharedMemConfig = 2;
/// CUDA shared memory configuration
pub type cudaSharedMemConfig = u32;
pub const cudaSharedCarveout_cudaSharedmemCarveoutDefault: cudaSharedCarveout = -1;
pub const cudaSharedCarveout_cudaSharedmemCarveoutMaxShared: cudaSharedCarveout = 100;
pub const cudaSharedCarveout_cudaSharedmemCarveoutMaxL1: cudaSharedCarveout = 0;
/// Shared memory carveout configurations
pub type cudaSharedCarveout = i32;
///< Default compute mode (Multiple threads can use ::cudaSetDevice() with this device)
pub const cudaComputeMode_cudaComputeModeDefault: cudaComputeMode = 0;
///< Compute-exclusive-thread mode (Only one thread in one process will be able to use ::cudaSetDevice() with this device)
pub const cudaComputeMode_cudaComputeModeExclusive: cudaComputeMode = 1;
///< Compute-prohibited mode (No threads can use ::cudaSetDevice() with this device)
pub const cudaComputeMode_cudaComputeModeProhibited: cudaComputeMode = 2;
///< Compute-exclusive-process mode (Many threads in one process will be able to use ::cudaSetDevice() with this device)
pub const cudaComputeMode_cudaComputeModeExclusiveProcess: cudaComputeMode = 3;
/// CUDA device compute modes
pub type cudaComputeMode = u32;
///< GPU thread stack size
pub const cudaLimit_cudaLimitStackSize: cudaLimit = 0;
///< GPU printf FIFO size
pub const cudaLimit_cudaLimitPrintfFifoSize: cudaLimit = 1;
///< GPU malloc heap size
pub const cudaLimit_cudaLimitMallocHeapSize: cudaLimit = 2;
///< GPU device runtime synchronize depth
pub const cudaLimit_cudaLimitDevRuntimeSyncDepth: cudaLimit = 3;
///< GPU device runtime pending launch count
pub const cudaLimit_cudaLimitDevRuntimePendingLaunchCount: cudaLimit = 4;
///< A value between 0 and 128 that indicates the maximum fetch granularity of L2 (in Bytes). This is a hint
pub const cudaLimit_cudaLimitMaxL2FetchGranularity: cudaLimit = 5;
/// CUDA Limits
pub type cudaLimit = u32;
///< Data will mostly be read and only occassionally be written to
pub const cudaMemoryAdvise_cudaMemAdviseSetReadMostly: cudaMemoryAdvise = 1;
///< Undo the effect of ::cudaMemAdviseSetReadMostly
pub const cudaMemoryAdvise_cudaMemAdviseUnsetReadMostly: cudaMemoryAdvise = 2;
///< Set the preferred location for the data as the specified device
pub const cudaMemoryAdvise_cudaMemAdviseSetPreferredLocation: cudaMemoryAdvise = 3;
///< Clear the preferred location for the data
pub const cudaMemoryAdvise_cudaMemAdviseUnsetPreferredLocation: cudaMemoryAdvise = 4;
///< Data will be accessed by the specified device, so prevent page faults as much as possible
pub const cudaMemoryAdvise_cudaMemAdviseSetAccessedBy: cudaMemoryAdvise = 5;
///< Let the Unified Memory subsystem decide on the page faulting policy for the specified device
pub const cudaMemoryAdvise_cudaMemAdviseUnsetAccessedBy: cudaMemoryAdvise = 6;
/// CUDA Memory Advise values
pub type cudaMemoryAdvise = u32;
///< Whether the range will mostly be read and only occassionally be written to
pub const cudaMemRangeAttribute_cudaMemRangeAttributeReadMostly: cudaMemRangeAttribute = 1;
///< The preferred location of the range
pub const cudaMemRangeAttribute_cudaMemRangeAttributePreferredLocation: cudaMemRangeAttribute = 2;
///< Memory range has ::cudaMemAdviseSetAccessedBy set for specified device
pub const cudaMemRangeAttribute_cudaMemRangeAttributeAccessedBy: cudaMemRangeAttribute = 3;
///< The last location to which the range was prefetched
pub const cudaMemRangeAttribute_cudaMemRangeAttributeLastPrefetchLocation: cudaMemRangeAttribute =
    4;
/// CUDA range attributes
pub type cudaMemRangeAttribute = u32;
///< Output mode Key-Value pair format.
pub const cudaOutputMode_cudaKeyValuePair: cudaOutputMode = 0;
///< Output mode Comma separated values format.
pub const cudaOutputMode_cudaCSV: cudaOutputMode = 1;
/// CUDA Profiler Output modes
pub type cudaOutputMode = u32;
///< Maximum number of threads per block
pub const cudaDeviceAttr_cudaDevAttrMaxThreadsPerBlock: cudaDeviceAttr = 1;
///< Maximum block dimension X
pub const cudaDeviceAttr_cudaDevAttrMaxBlockDimX: cudaDeviceAttr = 2;
///< Maximum block dimension Y
pub const cudaDeviceAttr_cudaDevAttrMaxBlockDimY: cudaDeviceAttr = 3;
///< Maximum block dimension Z
pub const cudaDeviceAttr_cudaDevAttrMaxBlockDimZ: cudaDeviceAttr = 4;
///< Maximum grid dimension X
pub const cudaDeviceAttr_cudaDevAttrMaxGridDimX: cudaDeviceAttr = 5;
///< Maximum grid dimension Y
pub const cudaDeviceAttr_cudaDevAttrMaxGridDimY: cudaDeviceAttr = 6;
///< Maximum grid dimension Z
pub const cudaDeviceAttr_cudaDevAttrMaxGridDimZ: cudaDeviceAttr = 7;
///< Maximum shared memory available per block in bytes
pub const cudaDeviceAttr_cudaDevAttrMaxSharedMemoryPerBlock: cudaDeviceAttr = 8;
///< Memory available on device for __constant__ variables in a CUDA C kernel in bytes
pub const cudaDeviceAttr_cudaDevAttrTotalConstantMemory: cudaDeviceAttr = 9;
///< Warp size in threads
pub const cudaDeviceAttr_cudaDevAttrWarpSize: cudaDeviceAttr = 10;
///< Maximum pitch in bytes allowed by memory copies
pub const cudaDeviceAttr_cudaDevAttrMaxPitch: cudaDeviceAttr = 11;
///< Maximum number of 32-bit registers available per block
pub const cudaDeviceAttr_cudaDevAttrMaxRegistersPerBlock: cudaDeviceAttr = 12;
///< Peak clock frequency in kilohertz
pub const cudaDeviceAttr_cudaDevAttrClockRate: cudaDeviceAttr = 13;
///< Alignment requirement for textures
pub const cudaDeviceAttr_cudaDevAttrTextureAlignment: cudaDeviceAttr = 14;
///< Device can possibly copy memory and execute a kernel concurrently
pub const cudaDeviceAttr_cudaDevAttrGpuOverlap: cudaDeviceAttr = 15;
///< Number of multiprocessors on device
pub const cudaDeviceAttr_cudaDevAttrMultiProcessorCount: cudaDeviceAttr = 16;
///< Specifies whether there is a run time limit on kernels
pub const cudaDeviceAttr_cudaDevAttrKernelExecTimeout: cudaDeviceAttr = 17;
///< Device is integrated with host memory
pub const cudaDeviceAttr_cudaDevAttrIntegrated: cudaDeviceAttr = 18;
///< Device can map host memory into CUDA address space
pub const cudaDeviceAttr_cudaDevAttrCanMapHostMemory: cudaDeviceAttr = 19;
///< Compute mode (See ::cudaComputeMode for details)
pub const cudaDeviceAttr_cudaDevAttrComputeMode: cudaDeviceAttr = 20;
///< Maximum 1D texture width
pub const cudaDeviceAttr_cudaDevAttrMaxTexture1DWidth: cudaDeviceAttr = 21;
///< Maximum 2D texture width
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DWidth: cudaDeviceAttr = 22;
///< Maximum 2D texture height
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DHeight: cudaDeviceAttr = 23;
///< Maximum 3D texture width
pub const cudaDeviceAttr_cudaDevAttrMaxTexture3DWidth: cudaDeviceAttr = 24;
///< Maximum 3D texture height
pub const cudaDeviceAttr_cudaDevAttrMaxTexture3DHeight: cudaDeviceAttr = 25;
///< Maximum 3D texture depth
pub const cudaDeviceAttr_cudaDevAttrMaxTexture3DDepth: cudaDeviceAttr = 26;
///< Maximum 2D layered texture width
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DLayeredWidth: cudaDeviceAttr = 27;
///< Maximum 2D layered texture height
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DLayeredHeight: cudaDeviceAttr = 28;
///< Maximum layers in a 2D layered texture
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DLayeredLayers: cudaDeviceAttr = 29;
///< Alignment requirement for surfaces
pub const cudaDeviceAttr_cudaDevAttrSurfaceAlignment: cudaDeviceAttr = 30;
///< Device can possibly execute multiple kernels concurrently
pub const cudaDeviceAttr_cudaDevAttrConcurrentKernels: cudaDeviceAttr = 31;
///< Device has ECC support enabled
pub const cudaDeviceAttr_cudaDevAttrEccEnabled: cudaDeviceAttr = 32;
///< PCI bus ID of the device
pub const cudaDeviceAttr_cudaDevAttrPciBusId: cudaDeviceAttr = 33;
///< PCI device ID of the device
pub const cudaDeviceAttr_cudaDevAttrPciDeviceId: cudaDeviceAttr = 34;
///< Device is using TCC driver model
pub const cudaDeviceAttr_cudaDevAttrTccDriver: cudaDeviceAttr = 35;
///< Peak memory clock frequency in kilohertz
pub const cudaDeviceAttr_cudaDevAttrMemoryClockRate: cudaDeviceAttr = 36;
///< Global memory bus width in bits
pub const cudaDeviceAttr_cudaDevAttrGlobalMemoryBusWidth: cudaDeviceAttr = 37;
///< Size of L2 cache in bytes
pub const cudaDeviceAttr_cudaDevAttrL2CacheSize: cudaDeviceAttr = 38;
///< Maximum resident threads per multiprocessor
pub const cudaDeviceAttr_cudaDevAttrMaxThreadsPerMultiProcessor: cudaDeviceAttr = 39;
///< Number of asynchronous engines
pub const cudaDeviceAttr_cudaDevAttrAsyncEngineCount: cudaDeviceAttr = 40;
///< Device shares a unified address space with the host
pub const cudaDeviceAttr_cudaDevAttrUnifiedAddressing: cudaDeviceAttr = 41;
///< Maximum 1D layered texture width
pub const cudaDeviceAttr_cudaDevAttrMaxTexture1DLayeredWidth: cudaDeviceAttr = 42;
///< Maximum layers in a 1D layered texture
pub const cudaDeviceAttr_cudaDevAttrMaxTexture1DLayeredLayers: cudaDeviceAttr = 43;
///< Maximum 2D texture width if cudaArrayTextureGather is set
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DGatherWidth: cudaDeviceAttr = 45;
///< Maximum 2D texture height if cudaArrayTextureGather is set
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DGatherHeight: cudaDeviceAttr = 46;
///< Alternate maximum 3D texture width
pub const cudaDeviceAttr_cudaDevAttrMaxTexture3DWidthAlt: cudaDeviceAttr = 47;
///< Alternate maximum 3D texture height
pub const cudaDeviceAttr_cudaDevAttrMaxTexture3DHeightAlt: cudaDeviceAttr = 48;
///< Alternate maximum 3D texture depth
pub const cudaDeviceAttr_cudaDevAttrMaxTexture3DDepthAlt: cudaDeviceAttr = 49;
///< PCI domain ID of the device
pub const cudaDeviceAttr_cudaDevAttrPciDomainId: cudaDeviceAttr = 50;
///< Pitch alignment requirement for textures
pub const cudaDeviceAttr_cudaDevAttrTexturePitchAlignment: cudaDeviceAttr = 51;
///< Maximum cubemap texture width/height
pub const cudaDeviceAttr_cudaDevAttrMaxTextureCubemapWidth: cudaDeviceAttr = 52;
///< Maximum cubemap layered texture width/height
pub const cudaDeviceAttr_cudaDevAttrMaxTextureCubemapLayeredWidth: cudaDeviceAttr = 53;
///< Maximum layers in a cubemap layered texture
pub const cudaDeviceAttr_cudaDevAttrMaxTextureCubemapLayeredLayers: cudaDeviceAttr = 54;
///< Maximum 1D surface width
pub const cudaDeviceAttr_cudaDevAttrMaxSurface1DWidth: cudaDeviceAttr = 55;
///< Maximum 2D surface width
pub const cudaDeviceAttr_cudaDevAttrMaxSurface2DWidth: cudaDeviceAttr = 56;
///< Maximum 2D surface height
pub const cudaDeviceAttr_cudaDevAttrMaxSurface2DHeight: cudaDeviceAttr = 57;
///< Maximum 3D surface width
pub const cudaDeviceAttr_cudaDevAttrMaxSurface3DWidth: cudaDeviceAttr = 58;
///< Maximum 3D surface height
pub const cudaDeviceAttr_cudaDevAttrMaxSurface3DHeight: cudaDeviceAttr = 59;
///< Maximum 3D surface depth
pub const cudaDeviceAttr_cudaDevAttrMaxSurface3DDepth: cudaDeviceAttr = 60;
///< Maximum 1D layered surface width
pub const cudaDeviceAttr_cudaDevAttrMaxSurface1DLayeredWidth: cudaDeviceAttr = 61;
///< Maximum layers in a 1D layered surface
pub const cudaDeviceAttr_cudaDevAttrMaxSurface1DLayeredLayers: cudaDeviceAttr = 62;
///< Maximum 2D layered surface width
pub const cudaDeviceAttr_cudaDevAttrMaxSurface2DLayeredWidth: cudaDeviceAttr = 63;
///< Maximum 2D layered surface height
pub const cudaDeviceAttr_cudaDevAttrMaxSurface2DLayeredHeight: cudaDeviceAttr = 64;
///< Maximum layers in a 2D layered surface
pub const cudaDeviceAttr_cudaDevAttrMaxSurface2DLayeredLayers: cudaDeviceAttr = 65;
///< Maximum cubemap surface width
pub const cudaDeviceAttr_cudaDevAttrMaxSurfaceCubemapWidth: cudaDeviceAttr = 66;
///< Maximum cubemap layered surface width
pub const cudaDeviceAttr_cudaDevAttrMaxSurfaceCubemapLayeredWidth: cudaDeviceAttr = 67;
///< Maximum layers in a cubemap layered surface
pub const cudaDeviceAttr_cudaDevAttrMaxSurfaceCubemapLayeredLayers: cudaDeviceAttr = 68;
///< Maximum 1D linear texture width
pub const cudaDeviceAttr_cudaDevAttrMaxTexture1DLinearWidth: cudaDeviceAttr = 69;
///< Maximum 2D linear texture width
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DLinearWidth: cudaDeviceAttr = 70;
///< Maximum 2D linear texture height
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DLinearHeight: cudaDeviceAttr = 71;
///< Maximum 2D linear texture pitch in bytes
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DLinearPitch: cudaDeviceAttr = 72;
///< Maximum mipmapped 2D texture width
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DMipmappedWidth: cudaDeviceAttr = 73;
///< Maximum mipmapped 2D texture height
pub const cudaDeviceAttr_cudaDevAttrMaxTexture2DMipmappedHeight: cudaDeviceAttr = 74;
///< Major compute capability version number
pub const cudaDeviceAttr_cudaDevAttrComputeCapabilityMajor: cudaDeviceAttr = 75;
///< Minor compute capability version number
pub const cudaDeviceAttr_cudaDevAttrComputeCapabilityMinor: cudaDeviceAttr = 76;
///< Maximum mipmapped 1D texture width
pub const cudaDeviceAttr_cudaDevAttrMaxTexture1DMipmappedWidth: cudaDeviceAttr = 77;
///< Device supports stream priorities
pub const cudaDeviceAttr_cudaDevAttrStreamPrioritiesSupported: cudaDeviceAttr = 78;
///< Device supports caching globals in L1
pub const cudaDeviceAttr_cudaDevAttrGlobalL1CacheSupported: cudaDeviceAttr = 79;
///< Device supports caching locals in L1
pub const cudaDeviceAttr_cudaDevAttrLocalL1CacheSupported: cudaDeviceAttr = 80;
///< Maximum shared memory available per multiprocessor in bytes
pub const cudaDeviceAttr_cudaDevAttrMaxSharedMemoryPerMultiprocessor: cudaDeviceAttr = 81;
///< Maximum number of 32-bit registers available per multiprocessor
pub const cudaDeviceAttr_cudaDevAttrMaxRegistersPerMultiprocessor: cudaDeviceAttr = 82;
///< Device can allocate managed memory on this system
pub const cudaDeviceAttr_cudaDevAttrManagedMemory: cudaDeviceAttr = 83;
///< Device is on a multi-GPU board
pub const cudaDeviceAttr_cudaDevAttrIsMultiGpuBoard: cudaDeviceAttr = 84;
///< Unique identifier for a group of devices on the same multi-GPU board
pub const cudaDeviceAttr_cudaDevAttrMultiGpuBoardGroupID: cudaDeviceAttr = 85;
///< Link between the device and the host supports native atomic operations
pub const cudaDeviceAttr_cudaDevAttrHostNativeAtomicSupported: cudaDeviceAttr = 86;
///< Ratio of single precision performance (in floating-point operations per second) to double precision performance
pub const cudaDeviceAttr_cudaDevAttrSingleToDoublePrecisionPerfRatio: cudaDeviceAttr = 87;
///< Device supports coherently accessing pageable memory without calling cudaHostRegister on it
pub const cudaDeviceAttr_cudaDevAttrPageableMemoryAccess: cudaDeviceAttr = 88;
///< Device can coherently access managed memory concurrently with the CPU
pub const cudaDeviceAttr_cudaDevAttrConcurrentManagedAccess: cudaDeviceAttr = 89;
///< Device supports Compute Preemption
pub const cudaDeviceAttr_cudaDevAttrComputePreemptionSupported: cudaDeviceAttr = 90;
///< Device can access host registered memory at the same virtual address as the CPU
pub const cudaDeviceAttr_cudaDevAttrCanUseHostPointerForRegisteredMem: cudaDeviceAttr = 91;
pub const cudaDeviceAttr_cudaDevAttrReserved92: cudaDeviceAttr = 92;
pub const cudaDeviceAttr_cudaDevAttrReserved93: cudaDeviceAttr = 93;
pub const cudaDeviceAttr_cudaDevAttrReserved94: cudaDeviceAttr = 94;
///< Device supports launching cooperative kernels via ::cudaLaunchCooperativeKernel
pub const cudaDeviceAttr_cudaDevAttrCooperativeLaunch: cudaDeviceAttr = 95;
///< Device can participate in cooperative kernels launched via ::cudaLaunchCooperativeKernelMultiDevice
pub const cudaDeviceAttr_cudaDevAttrCooperativeMultiDeviceLaunch: cudaDeviceAttr = 96;
///< The maximum optin shared memory per block. This value may vary by chip. See ::cudaFuncSetAttribute
pub const cudaDeviceAttr_cudaDevAttrMaxSharedMemoryPerBlockOptin: cudaDeviceAttr = 97;
///< Device supports flushing of outstanding remote writes.
pub const cudaDeviceAttr_cudaDevAttrCanFlushRemoteWrites: cudaDeviceAttr = 98;
///< Device supports host memory registration via ::cudaHostRegister.
pub const cudaDeviceAttr_cudaDevAttrHostRegisterSupported: cudaDeviceAttr = 99;
///< Device accesses pageable memory via the host's page tables.
pub const cudaDeviceAttr_cudaDevAttrPageableMemoryAccessUsesHostPageTables: cudaDeviceAttr = 100;
///< Host can directly access managed memory on the device without migration.
pub const cudaDeviceAttr_cudaDevAttrDirectManagedMemAccessFromHost: cudaDeviceAttr = 101;
/// CUDA device attributes
pub type cudaDeviceAttr = u32;
///< A relative value indicating the performance of the link between two devices
pub const cudaDeviceP2PAttr_cudaDevP2PAttrPerformanceRank: cudaDeviceP2PAttr = 1;
///< Peer access is enabled
pub const cudaDeviceP2PAttr_cudaDevP2PAttrAccessSupported: cudaDeviceP2PAttr = 2;
///< Native atomic operation over the link supported
pub const cudaDeviceP2PAttr_cudaDevP2PAttrNativeAtomicSupported: cudaDeviceP2PAttr = 3;
///< Accessing CUDA arrays over the link supported
pub const cudaDeviceP2PAttr_cudaDevP2PAttrCudaArrayAccessSupported: cudaDeviceP2PAttr = 4;
/// CUDA device P2P attributes
pub type cudaDeviceP2PAttr = u32;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUuuid_st {
    pub bytes: [::std::os::raw::c_char; 16usize],
}
#[test]
fn bindgen_test_layout_CUuuid_st() {
    assert_eq!(
        ::std::mem::size_of::<CUuuid_st>(),
        16usize,
        concat!("Size of: ", stringify!(CUuuid_st))
    );
    assert_eq!(
        ::std::mem::align_of::<CUuuid_st>(),
        1usize,
        concat!("Alignment of ", stringify!(CUuuid_st))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<CUuuid_st>())).bytes as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(CUuuid_st),
            "::",
            stringify!(bytes)
        )
    );
}
pub type CUuuid = CUuuid_st;
pub type cudaUUID_t = CUuuid_st;
/// CUDA device properties
#[repr(C)]
#[derive(Copy, Clone)]
pub struct cudaDeviceProp {
    ///< ASCII string identifying device
    pub name: [::std::os::raw::c_char; 256usize],
    ///< 16-byte unique identifier
    pub uuid: cudaUUID_t,
    ///< 8-byte locally unique identifier. Value is undefined on TCC and non-Windows platforms
    pub luid: [::std::os::raw::c_char; 8usize],
    ///< LUID device node mask. Value is undefined on TCC and non-Windows platforms
    pub luidDeviceNodeMask: ::std::os::raw::c_uint,
    ///< Global memory available on device in bytes
    pub totalGlobalMem: usize,
    ///< Shared memory available per block in bytes
    pub sharedMemPerBlock: usize,
    ///< 32-bit registers available per block
    pub regsPerBlock: ::std::os::raw::c_int,
    ///< Warp size in threads
    pub warpSize: ::std::os::raw::c_int,
    ///< Maximum pitch in bytes allowed by memory copies
    pub memPitch: usize,
    ///< Maximum number of threads per block
    pub maxThreadsPerBlock: ::std::os::raw::c_int,
    ///< Maximum size of each dimension of a block
    pub maxThreadsDim: [::std::os::raw::c_int; 3usize],
    ///< Maximum size of each dimension of a grid
    pub maxGridSize: [::std::os::raw::c_int; 3usize],
    ///< Clock frequency in kilohertz
    pub clockRate: ::std::os::raw::c_int,
    ///< Constant memory available on device in bytes
    pub totalConstMem: usize,
    ///< Major compute capability
    pub major: ::std::os::raw::c_int,
    ///< Minor compute capability
    pub minor: ::std::os::raw::c_int,
    ///< Alignment requirement for textures
    pub textureAlignment: usize,
    ///< Pitch alignment requirement for texture references bound to pitched memory
    pub texturePitchAlignment: usize,
    ///< Device can concurrently copy memory and execute a kernel. Deprecated. Use instead asyncEngineCount.
    pub deviceOverlap: ::std::os::raw::c_int,
    ///< Number of multiprocessors on device
    pub multiProcessorCount: ::std::os::raw::c_int,
    ///< Specified whether there is a run time limit on kernels
    pub kernelExecTimeoutEnabled: ::std::os::raw::c_int,
    ///< Device is integrated as opposed to discrete
    pub integrated: ::std::os::raw::c_int,
    ///< Device can map host memory with cudaHostAlloc/cudaHostGetDevicePointer
    pub canMapHostMemory: ::std::os::raw::c_int,
    ///< Compute mode (See ::cudaComputeMode)
    pub computeMode: ::std::os::raw::c_int,
    ///< Maximum 1D texture size
    pub maxTexture1D: ::std::os::raw::c_int,
    ///< Maximum 1D mipmapped texture size
    pub maxTexture1DMipmap: ::std::os::raw::c_int,
    ///< Maximum size for 1D textures bound to linear memory
    pub maxTexture1DLinear: ::std::os::raw::c_int,
    ///< Maximum 2D texture dimensions
    pub maxTexture2D: [::std::os::raw::c_int; 2usize],
    ///< Maximum 2D mipmapped texture dimensions
    pub maxTexture2DMipmap: [::std::os::raw::c_int; 2usize],
    ///< Maximum dimensions (width, height, pitch) for 2D textures bound to pitched memory
    pub maxTexture2DLinear: [::std::os::raw::c_int; 3usize],
    ///< Maximum 2D texture dimensions if texture gather operations have to be performed
    pub maxTexture2DGather: [::std::os::raw::c_int; 2usize],
    ///< Maximum 3D texture dimensions
    pub maxTexture3D: [::std::os::raw::c_int; 3usize],
    ///< Maximum alternate 3D texture dimensions
    pub maxTexture3DAlt: [::std::os::raw::c_int; 3usize],
    ///< Maximum Cubemap texture dimensions
    pub maxTextureCubemap: ::std::os::raw::c_int,
    ///< Maximum 1D layered texture dimensions
    pub maxTexture1DLayered: [::std::os::raw::c_int; 2usize],
    ///< Maximum 2D layered texture dimensions
    pub maxTexture2DLayered: [::std::os::raw::c_int; 3usize],
    ///< Maximum Cubemap layered texture dimensions
    pub maxTextureCubemapLayered: [::std::os::raw::c_int; 2usize],
    ///< Maximum 1D surface size
    pub maxSurface1D: ::std::os::raw::c_int,
    ///< Maximum 2D surface dimensions
    pub maxSurface2D: [::std::os::raw::c_int; 2usize],
    ///< Maximum 3D surface dimensions
    pub maxSurface3D: [::std::os::raw::c_int; 3usize],
    ///< Maximum 1D layered surface dimensions
    pub maxSurface1DLayered: [::std::os::raw::c_int; 2usize],
    ///< Maximum 2D layered surface dimensions
    pub maxSurface2DLayered: [::std::os::raw::c_int; 3usize],
    ///< Maximum Cubemap surface dimensions
    pub maxSurfaceCubemap: ::std::os::raw::c_int,
    ///< Maximum Cubemap layered surface dimensions
    pub maxSurfaceCubemapLayered: [::std::os::raw::c_int; 2usize],
    ///< Alignment requirements for surfaces
    pub surfaceAlignment: usize,
    ///< Device can possibly execute multiple kernels concurrently
    pub concurrentKernels: ::std::os::raw::c_int,
    ///< Device has ECC support enabled
    pub ECCEnabled: ::std::os::raw::c_int,
    ///< PCI bus ID of the device
    pub pciBusID: ::std::os::raw::c_int,
    ///< PCI device ID of the device
    pub pciDeviceID: ::std::os::raw::c_int,
    ///< PCI domain ID of the device
    pub pciDomainID: ::std::os::raw::c_int,
    ///< 1 if device is a Tesla device using TCC driver, 0 otherwise
    pub tccDriver: ::std::os::raw::c_int,
    ///< Number of asynchronous engines
    pub asyncEngineCount: ::std::os::raw::c_int,
    ///< Device shares a unified address space with the host
    pub unifiedAddressing: ::std::os::raw::c_int,
    ///< Peak memory clock frequency in kilohertz
    pub memoryClockRate: ::std::os::raw::c_int,
    ///< Global memory bus width in bits
    pub memoryBusWidth: ::std::os::raw::c_int,
    ///< Size of L2 cache in bytes
    pub l2CacheSize: ::std::os::raw::c_int,
    ///< Maximum resident threads per multiprocessor
    pub maxThreadsPerMultiProcessor: ::std::os::raw::c_int,
    ///< Device supports stream priorities
    pub streamPrioritiesSupported: ::std::os::raw::c_int,
    ///< Device supports caching globals in L1
    pub globalL1CacheSupported: ::std::os::raw::c_int,
    ///< Device supports caching locals in L1
    pub localL1CacheSupported: ::std::os::raw::c_int,
    ///< Shared memory available per multiprocessor in bytes
    pub sharedMemPerMultiprocessor: usize,
    ///< 32-bit registers available per multiprocessor
    pub regsPerMultiprocessor: ::std::os::raw::c_int,
    ///< Device supports allocating managed memory on this system
    pub managedMemory: ::std::os::raw::c_int,
    ///< Device is on a multi-GPU board
    pub isMultiGpuBoard: ::std::os::raw::c_int,
    ///< Unique identifier for a group of devices on the same multi-GPU board
    pub multiGpuBoardGroupID: ::std::os::raw::c_int,
    ///< Link between the device and the host supports native atomic operations
    pub hostNativeAtomicSupported: ::std::os::raw::c_int,
    ///< Ratio of single precision performance (in floating-point operations per second) to double precision performance
    pub singleToDoublePrecisionPerfRatio: ::std::os::raw::c_int,
    ///< Device supports coherently accessing pageable memory without calling cudaHostRegister on it
    pub pageableMemoryAccess: ::std::os::raw::c_int,
    ///< Device can coherently access managed memory concurrently with the CPU
    pub concurrentManagedAccess: ::std::os::raw::c_int,
    ///< Device supports Compute Preemption
    pub computePreemptionSupported: ::std::os::raw::c_int,
    ///< Device can access host registered memory at the same virtual address as the CPU
    pub canUseHostPointerForRegisteredMem: ::std::os::raw::c_int,
    ///< Device supports launching cooperative kernels via ::cudaLaunchCooperativeKernel
    pub cooperativeLaunch: ::std::os::raw::c_int,
    ///< Device can participate in cooperative kernels launched via ::cudaLaunchCooperativeKernelMultiDevice
    pub cooperativeMultiDeviceLaunch: ::std::os::raw::c_int,
    ///< Per device maximum shared memory per block usable by special opt in
    pub sharedMemPerBlockOptin: usize,
    ///< Device accesses pageable memory via the host's page tables
    pub pageableMemoryAccessUsesHostPageTables: ::std::os::raw::c_int,
    ///< Host can directly access managed memory on the device without migration.
    pub directManagedMemAccessFromHost: ::std::os::raw::c_int,
}
#[test]
fn bindgen_test_layout_cudaDeviceProp() {
    assert_eq!(
        ::std::mem::size_of::<cudaDeviceProp>(),
        712usize,
        concat!("Size of: ", stringify!(cudaDeviceProp))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaDeviceProp>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaDeviceProp))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).name as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(name)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).uuid as *const _ as usize },
        256usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(uuid)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).luid as *const _ as usize },
        272usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(luid)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).luidDeviceNodeMask as *const _ as usize
        },
        280usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(luidDeviceNodeMask)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).totalGlobalMem as *const _ as usize },
        288usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(totalGlobalMem)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).sharedMemPerBlock as *const _ as usize
        },
        296usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(sharedMemPerBlock)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).regsPerBlock as *const _ as usize },
        304usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(regsPerBlock)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).warpSize as *const _ as usize },
        308usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(warpSize)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).memPitch as *const _ as usize },
        312usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(memPitch)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxThreadsPerBlock as *const _ as usize
        },
        320usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxThreadsPerBlock)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).maxThreadsDim as *const _ as usize },
        324usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxThreadsDim)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).maxGridSize as *const _ as usize },
        336usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxGridSize)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).clockRate as *const _ as usize },
        348usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(clockRate)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).totalConstMem as *const _ as usize },
        352usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(totalConstMem)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).major as *const _ as usize },
        360usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(major)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).minor as *const _ as usize },
        364usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(minor)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).textureAlignment as *const _ as usize },
        368usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(textureAlignment)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).texturePitchAlignment as *const _ as usize
        },
        376usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(texturePitchAlignment)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).deviceOverlap as *const _ as usize },
        384usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(deviceOverlap)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).multiProcessorCount as *const _ as usize
        },
        388usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(multiProcessorCount)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).kernelExecTimeoutEnabled as *const _ as usize
        },
        392usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(kernelExecTimeoutEnabled)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).integrated as *const _ as usize },
        396usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(integrated)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).canMapHostMemory as *const _ as usize },
        400usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(canMapHostMemory)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).computeMode as *const _ as usize },
        404usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(computeMode)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture1D as *const _ as usize },
        408usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture1D)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture1DMipmap as *const _ as usize
        },
        412usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture1DMipmap)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture1DLinear as *const _ as usize
        },
        416usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture1DLinear)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture2D as *const _ as usize },
        420usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture2D)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture2DMipmap as *const _ as usize
        },
        428usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture2DMipmap)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture2DLinear as *const _ as usize
        },
        436usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture2DLinear)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture2DGather as *const _ as usize
        },
        448usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture2DGather)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture3D as *const _ as usize },
        456usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture3D)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture3DAlt as *const _ as usize },
        468usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture3DAlt)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxTextureCubemap as *const _ as usize
        },
        480usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTextureCubemap)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture1DLayered as *const _ as usize
        },
        484usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture1DLayered)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxTexture2DLayered as *const _ as usize
        },
        492usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTexture2DLayered)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxTextureCubemapLayered as *const _ as usize
        },
        504usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxTextureCubemapLayered)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).maxSurface1D as *const _ as usize },
        512usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxSurface1D)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).maxSurface2D as *const _ as usize },
        516usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxSurface2D)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).maxSurface3D as *const _ as usize },
        524usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxSurface3D)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxSurface1DLayered as *const _ as usize
        },
        536usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxSurface1DLayered)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxSurface2DLayered as *const _ as usize
        },
        544usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxSurface2DLayered)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxSurfaceCubemap as *const _ as usize
        },
        556usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxSurfaceCubemap)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxSurfaceCubemapLayered as *const _ as usize
        },
        560usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxSurfaceCubemapLayered)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).surfaceAlignment as *const _ as usize },
        568usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(surfaceAlignment)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).concurrentKernels as *const _ as usize
        },
        576usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(concurrentKernels)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).ECCEnabled as *const _ as usize },
        580usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(ECCEnabled)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).pciBusID as *const _ as usize },
        584usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(pciBusID)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).pciDeviceID as *const _ as usize },
        588usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(pciDeviceID)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).pciDomainID as *const _ as usize },
        592usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(pciDomainID)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).tccDriver as *const _ as usize },
        596usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(tccDriver)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).asyncEngineCount as *const _ as usize },
        600usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(asyncEngineCount)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).unifiedAddressing as *const _ as usize
        },
        604usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(unifiedAddressing)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).memoryClockRate as *const _ as usize },
        608usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(memoryClockRate)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).memoryBusWidth as *const _ as usize },
        612usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(memoryBusWidth)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).l2CacheSize as *const _ as usize },
        616usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(l2CacheSize)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).maxThreadsPerMultiProcessor as *const _
                as usize
        },
        620usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(maxThreadsPerMultiProcessor)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).streamPrioritiesSupported as *const _
                as usize
        },
        624usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(streamPrioritiesSupported)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).globalL1CacheSupported as *const _ as usize
        },
        628usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(globalL1CacheSupported)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).localL1CacheSupported as *const _ as usize
        },
        632usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(localL1CacheSupported)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).sharedMemPerMultiprocessor as *const _
                as usize
        },
        640usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(sharedMemPerMultiprocessor)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).regsPerMultiprocessor as *const _ as usize
        },
        648usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(regsPerMultiprocessor)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).managedMemory as *const _ as usize },
        652usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(managedMemory)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaDeviceProp>())).isMultiGpuBoard as *const _ as usize },
        656usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(isMultiGpuBoard)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).multiGpuBoardGroupID as *const _ as usize
        },
        660usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(multiGpuBoardGroupID)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).hostNativeAtomicSupported as *const _
                as usize
        },
        664usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(hostNativeAtomicSupported)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).singleToDoublePrecisionPerfRatio as *const _
                as usize
        },
        668usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(singleToDoublePrecisionPerfRatio)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).pageableMemoryAccess as *const _ as usize
        },
        672usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(pageableMemoryAccess)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).concurrentManagedAccess as *const _ as usize
        },
        676usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(concurrentManagedAccess)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).computePreemptionSupported as *const _
                as usize
        },
        680usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(computePreemptionSupported)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).canUseHostPointerForRegisteredMem as *const _
                as usize
        },
        684usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(canUseHostPointerForRegisteredMem)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).cooperativeLaunch as *const _ as usize
        },
        688usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(cooperativeLaunch)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).cooperativeMultiDeviceLaunch as *const _
                as usize
        },
        692usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(cooperativeMultiDeviceLaunch)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).sharedMemPerBlockOptin as *const _ as usize
        },
        696usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(sharedMemPerBlockOptin)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).pageableMemoryAccessUsesHostPageTables
                as *const _ as usize
        },
        704usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(pageableMemoryAccessUsesHostPageTables)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaDeviceProp>())).directManagedMemAccessFromHost as *const _
                as usize
        },
        708usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaDeviceProp),
            "::",
            stringify!(directManagedMemAccessFromHost)
        )
    );
}
/// CUDA IPC event handle
#[repr(C)]
#[derive(Copy, Clone)]
pub struct cudaIpcEventHandle_st {
    pub reserved: [::std::os::raw::c_char; 64usize],
}
#[test]
fn bindgen_test_layout_cudaIpcEventHandle_st() {
    assert_eq!(
        ::std::mem::size_of::<cudaIpcEventHandle_st>(),
        64usize,
        concat!("Size of: ", stringify!(cudaIpcEventHandle_st))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaIpcEventHandle_st>(),
        1usize,
        concat!("Alignment of ", stringify!(cudaIpcEventHandle_st))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaIpcEventHandle_st>())).reserved as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaIpcEventHandle_st),
            "::",
            stringify!(reserved)
        )
    );
}
pub type cudaIpcEventHandle_t = cudaIpcEventHandle_st;
/// CUDA IPC memory handle
#[repr(C)]
#[derive(Copy, Clone)]
pub struct cudaIpcMemHandle_st {
    pub reserved: [::std::os::raw::c_char; 64usize],
}
#[test]
fn bindgen_test_layout_cudaIpcMemHandle_st() {
    assert_eq!(
        ::std::mem::size_of::<cudaIpcMemHandle_st>(),
        64usize,
        concat!("Size of: ", stringify!(cudaIpcMemHandle_st))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaIpcMemHandle_st>(),
        1usize,
        concat!("Alignment of ", stringify!(cudaIpcMemHandle_st))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaIpcMemHandle_st>())).reserved as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaIpcMemHandle_st),
            "::",
            stringify!(reserved)
        )
    );
}
pub type cudaIpcMemHandle_t = cudaIpcMemHandle_st;
/// Handle is an opaque file descriptor
pub const cudaExternalMemoryHandleType_cudaExternalMemoryHandleTypeOpaqueFd:
    cudaExternalMemoryHandleType = 1;
/// Handle is an opaque shared NT handle
pub const cudaExternalMemoryHandleType_cudaExternalMemoryHandleTypeOpaqueWin32:
    cudaExternalMemoryHandleType = 2;
/// Handle is an opaque, globally shared handle
pub const cudaExternalMemoryHandleType_cudaExternalMemoryHandleTypeOpaqueWin32Kmt:
    cudaExternalMemoryHandleType = 3;
/// Handle is a D3D12 heap object
pub const cudaExternalMemoryHandleType_cudaExternalMemoryHandleTypeD3D12Heap:
    cudaExternalMemoryHandleType = 4;
/// Handle is a D3D12 committed resource
pub const cudaExternalMemoryHandleType_cudaExternalMemoryHandleTypeD3D12Resource:
    cudaExternalMemoryHandleType = 5;
/// External memory handle types
pub type cudaExternalMemoryHandleType = u32;
/// External memory handle descriptor
#[repr(C)]
#[derive(Copy, Clone)]
pub struct cudaExternalMemoryHandleDesc {
    /// Type of the handle
    pub type_: cudaExternalMemoryHandleType,
    pub handle: cudaExternalMemoryHandleDesc__bindgen_ty_1,
    /// Size of the memory allocation
    pub size: ::std::os::raw::c_ulonglong,
    /// Flags must either be zero or ::cudaExternalMemoryDedicated
    pub flags: ::std::os::raw::c_uint,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union cudaExternalMemoryHandleDesc__bindgen_ty_1 {
    /// File descriptor referencing the memory object. Valid
    /// when type is
    /// ::cudaExternalMemoryHandleTypeOpaqueFd
    pub fd: ::std::os::raw::c_int,
    pub win32: cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1,
    _bindgen_union_align: [u64; 2usize],
}
/// Win32 handle referencing the semaphore object. Valid when
/// type is one of the following:
/// - ::cudaExternalMemoryHandleTypeOpaqueWin32
/// - ::cudaExternalMemoryHandleTypeOpaqueWin32Kmt
/// - ::cudaExternalMemoryHandleTypeD3D12Heap
/// - ::cudaExternalMemoryHandleTypeD3D12Resource
/// Exactly one of 'handle' and 'name' must be non-NULL. If
/// type is ::cudaExternalMemoryHandleTypeOpaqueWin32Kmt
/// then 'name' must be NULL.
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1 {
    /// Valid NT handle. Must be NULL if 'name' is non-NULL
    pub handle: *mut ::std::os::raw::c_void,
    /// Name of a valid memory object.
    /// Must be NULL if 'handle' is non-NULL.
    pub name: *const ::std::os::raw::c_void,
}
#[test]
fn bindgen_test_layout_cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1>(),
        16usize,
        concat!(
            "Size of: ",
            stringify!(cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1>()))
                .handle as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1),
            "::",
            stringify!(handle)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1>()))
                .name as *const _ as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryHandleDesc__bindgen_ty_1__bindgen_ty_1),
            "::",
            stringify!(name)
        )
    );
}
#[test]
fn bindgen_test_layout_cudaExternalMemoryHandleDesc__bindgen_ty_1() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalMemoryHandleDesc__bindgen_ty_1>(),
        16usize,
        concat!(
            "Size of: ",
            stringify!(cudaExternalMemoryHandleDesc__bindgen_ty_1)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalMemoryHandleDesc__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaExternalMemoryHandleDesc__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryHandleDesc__bindgen_ty_1>())).fd as *const _
                as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryHandleDesc__bindgen_ty_1),
            "::",
            stringify!(fd)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryHandleDesc__bindgen_ty_1>())).win32 as *const _
                as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryHandleDesc__bindgen_ty_1),
            "::",
            stringify!(win32)
        )
    );
}
#[test]
fn bindgen_test_layout_cudaExternalMemoryHandleDesc() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalMemoryHandleDesc>(),
        40usize,
        concat!("Size of: ", stringify!(cudaExternalMemoryHandleDesc))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalMemoryHandleDesc>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaExternalMemoryHandleDesc))
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryHandleDesc>())).type_ as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryHandleDesc),
            "::",
            stringify!(type_)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryHandleDesc>())).handle as *const _ as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryHandleDesc),
            "::",
            stringify!(handle)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryHandleDesc>())).size as *const _ as usize
        },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryHandleDesc),
            "::",
            stringify!(size)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryHandleDesc>())).flags as *const _ as usize
        },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryHandleDesc),
            "::",
            stringify!(flags)
        )
    );
}
/// External memory buffer descriptor
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaExternalMemoryBufferDesc {
    /// Offset into the memory object where the buffer's base is
    pub offset: ::std::os::raw::c_ulonglong,
    /// Size of the buffer
    pub size: ::std::os::raw::c_ulonglong,
    /// Flags reserved for future use. Must be zero.
    pub flags: ::std::os::raw::c_uint,
}
#[test]
fn bindgen_test_layout_cudaExternalMemoryBufferDesc() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalMemoryBufferDesc>(),
        24usize,
        concat!("Size of: ", stringify!(cudaExternalMemoryBufferDesc))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalMemoryBufferDesc>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaExternalMemoryBufferDesc))
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryBufferDesc>())).offset as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryBufferDesc),
            "::",
            stringify!(offset)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryBufferDesc>())).size as *const _ as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryBufferDesc),
            "::",
            stringify!(size)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryBufferDesc>())).flags as *const _ as usize
        },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryBufferDesc),
            "::",
            stringify!(flags)
        )
    );
}
/// External memory mipmap descriptor
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaExternalMemoryMipmappedArrayDesc {
    /// Offset into the memory object where the base level of the
    /// mipmap chain is.
    pub offset: ::std::os::raw::c_ulonglong,
    /// Format of base level of the mipmap chain
    pub formatDesc: cudaChannelFormatDesc,
    /// Dimensions of base level of the mipmap chain
    pub extent: cudaExtent,
    /// Flags associated with CUDA mipmapped arrays.
    /// See ::cudaMallocMipmappedArray
    pub flags: ::std::os::raw::c_uint,
    /// Total number of levels in the mipmap chain
    pub numLevels: ::std::os::raw::c_uint,
}
#[test]
fn bindgen_test_layout_cudaExternalMemoryMipmappedArrayDesc() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalMemoryMipmappedArrayDesc>(),
        64usize,
        concat!(
            "Size of: ",
            stringify!(cudaExternalMemoryMipmappedArrayDesc)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalMemoryMipmappedArrayDesc>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaExternalMemoryMipmappedArrayDesc)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryMipmappedArrayDesc>())).offset as *const _
                as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryMipmappedArrayDesc),
            "::",
            stringify!(offset)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryMipmappedArrayDesc>())).formatDesc as *const _
                as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryMipmappedArrayDesc),
            "::",
            stringify!(formatDesc)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryMipmappedArrayDesc>())).extent as *const _
                as usize
        },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryMipmappedArrayDesc),
            "::",
            stringify!(extent)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryMipmappedArrayDesc>())).flags as *const _
                as usize
        },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryMipmappedArrayDesc),
            "::",
            stringify!(flags)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalMemoryMipmappedArrayDesc>())).numLevels as *const _
                as usize
        },
        60usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalMemoryMipmappedArrayDesc),
            "::",
            stringify!(numLevels)
        )
    );
}
/// Handle is an opaque file descriptor
pub const cudaExternalSemaphoreHandleType_cudaExternalSemaphoreHandleTypeOpaqueFd:
    cudaExternalSemaphoreHandleType = 1;
/// Handle is an opaque shared NT handle
pub const cudaExternalSemaphoreHandleType_cudaExternalSemaphoreHandleTypeOpaqueWin32:
    cudaExternalSemaphoreHandleType = 2;
/// Handle is an opaque, globally shared handle
pub const cudaExternalSemaphoreHandleType_cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt:
    cudaExternalSemaphoreHandleType = 3;
/// Handle is a shared NT handle referencing a D3D12 fence object
pub const cudaExternalSemaphoreHandleType_cudaExternalSemaphoreHandleTypeD3D12Fence:
    cudaExternalSemaphoreHandleType = 4;
/// External semaphore handle types
pub type cudaExternalSemaphoreHandleType = u32;
/// External semaphore handle descriptor
#[repr(C)]
#[derive(Copy, Clone)]
pub struct cudaExternalSemaphoreHandleDesc {
    /// Type of the handle
    pub type_: cudaExternalSemaphoreHandleType,
    pub handle: cudaExternalSemaphoreHandleDesc__bindgen_ty_1,
    /// Flags reserved for the future. Must be zero.
    pub flags: ::std::os::raw::c_uint,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union cudaExternalSemaphoreHandleDesc__bindgen_ty_1 {
    /// File descriptor referencing the semaphore object. Valid
    /// when type is ::cudaExternalSemaphoreHandleTypeOpaqueFd
    pub fd: ::std::os::raw::c_int,
    pub win32: cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1,
    _bindgen_union_align: [u64; 2usize],
}
/// Win32 handle referencing the semaphore object. Valid when
/// type is one of the following:
/// - ::cudaExternalSemaphoreHandleTypeOpaqueWin32
/// - ::cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt
/// - ::cudaExternalSemaphoreHandleTypeD3D12Fence
/// Exactly one of 'handle' and 'name' must be non-NULL. If
/// type is ::cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt
/// then 'name' must be NULL.
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1 {
    /// Valid NT handle. Must be NULL if 'name' is non-NULL
    pub handle: *mut ::std::os::raw::c_void,
    /// Name of a valid synchronization primitive.
    /// Must be NULL if 'handle' is non-NULL.
    pub name: *const ::std::os::raw::c_void,
}
#[test]
fn bindgen_test_layout_cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1>(),
        16usize,
        concat!(
            "Size of: ",
            stringify!(cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1>()))
                .handle as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1),
            "::",
            stringify!(handle)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1>()))
                .name as *const _ as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreHandleDesc__bindgen_ty_1__bindgen_ty_1),
            "::",
            stringify!(name)
        )
    );
}
#[test]
fn bindgen_test_layout_cudaExternalSemaphoreHandleDesc__bindgen_ty_1() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalSemaphoreHandleDesc__bindgen_ty_1>(),
        16usize,
        concat!(
            "Size of: ",
            stringify!(cudaExternalSemaphoreHandleDesc__bindgen_ty_1)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalSemaphoreHandleDesc__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaExternalSemaphoreHandleDesc__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreHandleDesc__bindgen_ty_1>())).fd as *const _
                as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreHandleDesc__bindgen_ty_1),
            "::",
            stringify!(fd)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreHandleDesc__bindgen_ty_1>())).win32
                as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreHandleDesc__bindgen_ty_1),
            "::",
            stringify!(win32)
        )
    );
}
#[test]
fn bindgen_test_layout_cudaExternalSemaphoreHandleDesc() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalSemaphoreHandleDesc>(),
        32usize,
        concat!("Size of: ", stringify!(cudaExternalSemaphoreHandleDesc))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalSemaphoreHandleDesc>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaExternalSemaphoreHandleDesc))
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreHandleDesc>())).type_ as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreHandleDesc),
            "::",
            stringify!(type_)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreHandleDesc>())).handle as *const _ as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreHandleDesc),
            "::",
            stringify!(handle)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreHandleDesc>())).flags as *const _ as usize
        },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreHandleDesc),
            "::",
            stringify!(flags)
        )
    );
}
/// External semaphore  signal parameters
#[repr(C)]
#[derive(Copy, Clone)]
pub struct cudaExternalSemaphoreSignalParams {
    pub params: cudaExternalSemaphoreSignalParams__bindgen_ty_1,
    /// Flags reserved for the future. Must be zero.
    pub flags: ::std::os::raw::c_uint,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union cudaExternalSemaphoreSignalParams__bindgen_ty_1 {
    pub fence: cudaExternalSemaphoreSignalParams__bindgen_ty_1__bindgen_ty_1,
    _bindgen_union_align: u64,
}
/// Parameters for fence objects
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaExternalSemaphoreSignalParams__bindgen_ty_1__bindgen_ty_1 {
    /// Value of fence to be signaled
    pub value: ::std::os::raw::c_ulonglong,
}
#[test]
fn bindgen_test_layout_cudaExternalSemaphoreSignalParams__bindgen_ty_1__bindgen_ty_1() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalSemaphoreSignalParams__bindgen_ty_1__bindgen_ty_1>(),
        8usize,
        concat!(
            "Size of: ",
            stringify!(cudaExternalSemaphoreSignalParams__bindgen_ty_1__bindgen_ty_1)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalSemaphoreSignalParams__bindgen_ty_1__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaExternalSemaphoreSignalParams__bindgen_ty_1__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreSignalParams__bindgen_ty_1__bindgen_ty_1>(
            )))
            .value as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreSignalParams__bindgen_ty_1__bindgen_ty_1),
            "::",
            stringify!(value)
        )
    );
}
#[test]
fn bindgen_test_layout_cudaExternalSemaphoreSignalParams__bindgen_ty_1() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalSemaphoreSignalParams__bindgen_ty_1>(),
        8usize,
        concat!(
            "Size of: ",
            stringify!(cudaExternalSemaphoreSignalParams__bindgen_ty_1)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalSemaphoreSignalParams__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaExternalSemaphoreSignalParams__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreSignalParams__bindgen_ty_1>())).fence
                as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreSignalParams__bindgen_ty_1),
            "::",
            stringify!(fence)
        )
    );
}
#[test]
fn bindgen_test_layout_cudaExternalSemaphoreSignalParams() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalSemaphoreSignalParams>(),
        16usize,
        concat!("Size of: ", stringify!(cudaExternalSemaphoreSignalParams))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalSemaphoreSignalParams>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaExternalSemaphoreSignalParams)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreSignalParams>())).params as *const _
                as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreSignalParams),
            "::",
            stringify!(params)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreSignalParams>())).flags as *const _ as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreSignalParams),
            "::",
            stringify!(flags)
        )
    );
}
/// External semaphore wait parameters
#[repr(C)]
#[derive(Copy, Clone)]
pub struct cudaExternalSemaphoreWaitParams {
    pub params: cudaExternalSemaphoreWaitParams__bindgen_ty_1,
    /// Flags reserved for the future. Must be zero.
    pub flags: ::std::os::raw::c_uint,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union cudaExternalSemaphoreWaitParams__bindgen_ty_1 {
    pub fence: cudaExternalSemaphoreWaitParams__bindgen_ty_1__bindgen_ty_1,
    _bindgen_union_align: u64,
}
/// Parameters for fence objects
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaExternalSemaphoreWaitParams__bindgen_ty_1__bindgen_ty_1 {
    /// Value of fence to be waited on
    pub value: ::std::os::raw::c_ulonglong,
}
#[test]
fn bindgen_test_layout_cudaExternalSemaphoreWaitParams__bindgen_ty_1__bindgen_ty_1() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalSemaphoreWaitParams__bindgen_ty_1__bindgen_ty_1>(),
        8usize,
        concat!(
            "Size of: ",
            stringify!(cudaExternalSemaphoreWaitParams__bindgen_ty_1__bindgen_ty_1)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalSemaphoreWaitParams__bindgen_ty_1__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaExternalSemaphoreWaitParams__bindgen_ty_1__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreWaitParams__bindgen_ty_1__bindgen_ty_1>()))
                .value as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreWaitParams__bindgen_ty_1__bindgen_ty_1),
            "::",
            stringify!(value)
        )
    );
}
#[test]
fn bindgen_test_layout_cudaExternalSemaphoreWaitParams__bindgen_ty_1() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalSemaphoreWaitParams__bindgen_ty_1>(),
        8usize,
        concat!(
            "Size of: ",
            stringify!(cudaExternalSemaphoreWaitParams__bindgen_ty_1)
        )
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalSemaphoreWaitParams__bindgen_ty_1>(),
        8usize,
        concat!(
            "Alignment of ",
            stringify!(cudaExternalSemaphoreWaitParams__bindgen_ty_1)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreWaitParams__bindgen_ty_1>())).fence
                as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreWaitParams__bindgen_ty_1),
            "::",
            stringify!(fence)
        )
    );
}
#[test]
fn bindgen_test_layout_cudaExternalSemaphoreWaitParams() {
    assert_eq!(
        ::std::mem::size_of::<cudaExternalSemaphoreWaitParams>(),
        16usize,
        concat!("Size of: ", stringify!(cudaExternalSemaphoreWaitParams))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaExternalSemaphoreWaitParams>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaExternalSemaphoreWaitParams))
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreWaitParams>())).params as *const _ as usize
        },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreWaitParams),
            "::",
            stringify!(params)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaExternalSemaphoreWaitParams>())).flags as *const _ as usize
        },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaExternalSemaphoreWaitParams),
            "::",
            stringify!(flags)
        )
    );
}
/// CUDA Error types
pub use self::cudaError as cudaError_t;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUstream_st {
    _unused: [u8; 0],
}
/// CUDA stream
pub type cudaStream_t = *mut CUstream_st;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUevent_st {
    _unused: [u8; 0],
}
/// CUDA event types
pub type cudaEvent_t = *mut CUevent_st;
/// CUDA graphics resource types
pub type cudaGraphicsResource_t = *mut cudaGraphicsResource;
/// CUDA output file modes
pub use self::cudaOutputMode as cudaOutputMode_t;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUexternalMemory_st {
    _unused: [u8; 0],
}
/// CUDA external memory
pub type cudaExternalMemory_t = *mut CUexternalMemory_st;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUexternalSemaphore_st {
    _unused: [u8; 0],
}
/// CUDA external semaphore
pub type cudaExternalSemaphore_t = *mut CUexternalSemaphore_st;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUgraph_st {
    _unused: [u8; 0],
}
/// CUDA graph
pub type cudaGraph_t = *mut CUgraph_st;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUgraphNode_st {
    _unused: [u8; 0],
}
/// CUDA graph node.
pub type cudaGraphNode_t = *mut CUgraphNode_st;
///< Invalid cooperative group scope
pub const cudaCGScope_cudaCGScopeInvalid: cudaCGScope = 0;
///< Scope represented by a grid_group
pub const cudaCGScope_cudaCGScopeGrid: cudaCGScope = 1;
///< Scope represented by a multi_grid_group
pub const cudaCGScope_cudaCGScopeMultiGrid: cudaCGScope = 2;
/// CUDA cooperative group scope
pub type cudaCGScope = u32;
/// CUDA launch parameters
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaLaunchParams {
    ///< Device function symbol
    pub func: *mut ::std::os::raw::c_void,
    ///< Grid dimentions
    pub gridDim: dim3,
    ///< Block dimentions
    pub blockDim: dim3,
    ///< Arguments
    pub args: *mut *mut ::std::os::raw::c_void,
    ///< Shared memory
    pub sharedMem: usize,
    ///< Stream identifier
    pub stream: cudaStream_t,
}
#[test]
fn bindgen_test_layout_cudaLaunchParams() {
    assert_eq!(
        ::std::mem::size_of::<cudaLaunchParams>(),
        56usize,
        concat!("Size of: ", stringify!(cudaLaunchParams))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaLaunchParams>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaLaunchParams))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaLaunchParams>())).func as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaLaunchParams),
            "::",
            stringify!(func)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaLaunchParams>())).gridDim as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaLaunchParams),
            "::",
            stringify!(gridDim)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaLaunchParams>())).blockDim as *const _ as usize },
        20usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaLaunchParams),
            "::",
            stringify!(blockDim)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaLaunchParams>())).args as *const _ as usize },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaLaunchParams),
            "::",
            stringify!(args)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaLaunchParams>())).sharedMem as *const _ as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaLaunchParams),
            "::",
            stringify!(sharedMem)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaLaunchParams>())).stream as *const _ as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaLaunchParams),
            "::",
            stringify!(stream)
        )
    );
}
/// CUDA GPU kernel node parameters
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaKernelNodeParams {
    ///< Kernel to launch
    pub func: *mut ::std::os::raw::c_void,
    ///< Grid dimensions
    pub gridDim: dim3,
    ///< Block dimensions
    pub blockDim: dim3,
    ///< Dynamic shared-memory size per thread block in bytes
    pub sharedMemBytes: ::std::os::raw::c_uint,
    ///< Array of pointers to individual kernel arguments
    pub kernelParams: *mut *mut ::std::os::raw::c_void,
    ///< Pointer to kernel arguments in the "extra" format
    pub extra: *mut *mut ::std::os::raw::c_void,
}
#[test]
fn bindgen_test_layout_cudaKernelNodeParams() {
    assert_eq!(
        ::std::mem::size_of::<cudaKernelNodeParams>(),
        56usize,
        concat!("Size of: ", stringify!(cudaKernelNodeParams))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaKernelNodeParams>(),
        8usize,
        concat!("Alignment of ", stringify!(cudaKernelNodeParams))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaKernelNodeParams>())).func as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaKernelNodeParams),
            "::",
            stringify!(func)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaKernelNodeParams>())).gridDim as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaKernelNodeParams),
            "::",
            stringify!(gridDim)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaKernelNodeParams>())).blockDim as *const _ as usize },
        20usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaKernelNodeParams),
            "::",
            stringify!(blockDim)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaKernelNodeParams>())).sharedMemBytes as *const _ as usize
        },
        32usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaKernelNodeParams),
            "::",
            stringify!(sharedMemBytes)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaKernelNodeParams>())).kernelParams as *const _ as usize
        },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaKernelNodeParams),
            "::",
            stringify!(kernelParams)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaKernelNodeParams>())).extra as *const _ as usize },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaKernelNodeParams),
            "::",
            stringify!(extra)
        )
    );
}
///< GPU kernel node
pub const cudaGraphNodeType_cudaGraphNodeTypeKernel: cudaGraphNodeType = 0;
///< Memcpy node
pub const cudaGraphNodeType_cudaGraphNodeTypeMemcpy: cudaGraphNodeType = 1;
///< Memset node
pub const cudaGraphNodeType_cudaGraphNodeTypeMemset: cudaGraphNodeType = 2;
///< Host (executable) node
pub const cudaGraphNodeType_cudaGraphNodeTypeHost: cudaGraphNodeType = 3;
///< Node which executes an embedded graph
pub const cudaGraphNodeType_cudaGraphNodeTypeGraph: cudaGraphNodeType = 4;
///< Empty (no-op) node
pub const cudaGraphNodeType_cudaGraphNodeTypeEmpty: cudaGraphNodeType = 5;
pub const cudaGraphNodeType_cudaGraphNodeTypeCount: cudaGraphNodeType = 6;
/// CUDA Graph node types
pub type cudaGraphNodeType = u32;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct CUgraphExec_st {
    _unused: [u8; 0],
}
/// CUDA executable (launchable) graph
pub type cudaGraphExec_t = *mut CUgraphExec_st;
///< Zero boundary mode
pub const cudaSurfaceBoundaryMode_cudaBoundaryModeZero: cudaSurfaceBoundaryMode = 0;
///< Clamp boundary mode
pub const cudaSurfaceBoundaryMode_cudaBoundaryModeClamp: cudaSurfaceBoundaryMode = 1;
///< Trap boundary mode
pub const cudaSurfaceBoundaryMode_cudaBoundaryModeTrap: cudaSurfaceBoundaryMode = 2;
/// CUDA Surface boundary modes
pub type cudaSurfaceBoundaryMode = u32;
///< Forced format mode
pub const cudaSurfaceFormatMode_cudaFormatModeForced: cudaSurfaceFormatMode = 0;
///< Auto format mode
pub const cudaSurfaceFormatMode_cudaFormatModeAuto: cudaSurfaceFormatMode = 1;
/// CUDA Surface format modes
pub type cudaSurfaceFormatMode = u32;
/// CUDA Surface reference
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct surfaceReference {
    /// Channel descriptor for surface reference
    pub channelDesc: cudaChannelFormatDesc,
}
#[test]
fn bindgen_test_layout_surfaceReference() {
    assert_eq!(
        ::std::mem::size_of::<surfaceReference>(),
        20usize,
        concat!("Size of: ", stringify!(surfaceReference))
    );
    assert_eq!(
        ::std::mem::align_of::<surfaceReference>(),
        4usize,
        concat!("Alignment of ", stringify!(surfaceReference))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<surfaceReference>())).channelDesc as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(surfaceReference),
            "::",
            stringify!(channelDesc)
        )
    );
}
/// An opaque value that represents a CUDA Surface object
pub type cudaSurfaceObject_t = ::std::os::raw::c_ulonglong;
///< Wrapping address mode
pub const cudaTextureAddressMode_cudaAddressModeWrap: cudaTextureAddressMode = 0;
///< Clamp to edge address mode
pub const cudaTextureAddressMode_cudaAddressModeClamp: cudaTextureAddressMode = 1;
///< Mirror address mode
pub const cudaTextureAddressMode_cudaAddressModeMirror: cudaTextureAddressMode = 2;
///< Border address mode
pub const cudaTextureAddressMode_cudaAddressModeBorder: cudaTextureAddressMode = 3;
/// CUDA texture address modes
pub type cudaTextureAddressMode = u32;
///< Point filter mode
pub const cudaTextureFilterMode_cudaFilterModePoint: cudaTextureFilterMode = 0;
///< Linear filter mode
pub const cudaTextureFilterMode_cudaFilterModeLinear: cudaTextureFilterMode = 1;
/// CUDA texture filter modes
pub type cudaTextureFilterMode = u32;
///< Read texture as specified element type
pub const cudaTextureReadMode_cudaReadModeElementType: cudaTextureReadMode = 0;
///< Read texture as normalized float
pub const cudaTextureReadMode_cudaReadModeNormalizedFloat: cudaTextureReadMode = 1;
/// CUDA texture read modes
pub type cudaTextureReadMode = u32;
/// CUDA texture reference
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct textureReference {
    /// Indicates whether texture reads are normalized or not
    pub normalized: ::std::os::raw::c_int,
    /// Texture filter mode
    pub filterMode: cudaTextureFilterMode,
    /// Texture address mode for up to 3 dimensions
    pub addressMode: [cudaTextureAddressMode; 3usize],
    /// Channel descriptor for the texture reference
    pub channelDesc: cudaChannelFormatDesc,
    /// Perform sRGB->linear conversion during texture read
    pub sRGB: ::std::os::raw::c_int,
    /// Limit to the anisotropy ratio
    pub maxAnisotropy: ::std::os::raw::c_uint,
    /// Mipmap filter mode
    pub mipmapFilterMode: cudaTextureFilterMode,
    /// Offset applied to the supplied mipmap level
    pub mipmapLevelBias: f32,
    /// Lower end of the mipmap level range to clamp access to
    pub minMipmapLevelClamp: f32,
    /// Upper end of the mipmap level range to clamp access to
    pub maxMipmapLevelClamp: f32,
    pub __cudaReserved: [::std::os::raw::c_int; 15usize],
}
#[test]
fn bindgen_test_layout_textureReference() {
    assert_eq!(
        ::std::mem::size_of::<textureReference>(),
        124usize,
        concat!("Size of: ", stringify!(textureReference))
    );
    assert_eq!(
        ::std::mem::align_of::<textureReference>(),
        4usize,
        concat!("Alignment of ", stringify!(textureReference))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<textureReference>())).normalized as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(normalized)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<textureReference>())).filterMode as *const _ as usize },
        4usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(filterMode)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<textureReference>())).addressMode as *const _ as usize },
        8usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(addressMode)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<textureReference>())).channelDesc as *const _ as usize },
        20usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(channelDesc)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<textureReference>())).sRGB as *const _ as usize },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(sRGB)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<textureReference>())).maxAnisotropy as *const _ as usize },
        44usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(maxAnisotropy)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<textureReference>())).mipmapFilterMode as *const _ as usize
        },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(mipmapFilterMode)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<textureReference>())).mipmapLevelBias as *const _ as usize
        },
        52usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(mipmapLevelBias)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<textureReference>())).minMipmapLevelClamp as *const _ as usize
        },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(minMipmapLevelClamp)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<textureReference>())).maxMipmapLevelClamp as *const _ as usize
        },
        60usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(maxMipmapLevelClamp)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<textureReference>())).__cudaReserved as *const _ as usize },
        64usize,
        concat!(
            "Offset of field: ",
            stringify!(textureReference),
            "::",
            stringify!(__cudaReserved)
        )
    );
}
/// CUDA texture descriptor
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct cudaTextureDesc {
    /// Texture address mode for up to 3 dimensions
    pub addressMode: [cudaTextureAddressMode; 3usize],
    /// Texture filter mode
    pub filterMode: cudaTextureFilterMode,
    /// Texture read mode
    pub readMode: cudaTextureReadMode,
    /// Perform sRGB->linear conversion during texture read
    pub sRGB: ::std::os::raw::c_int,
    /// Texture Border Color
    pub borderColor: [f32; 4usize],
    /// Indicates whether texture reads are normalized or not
    pub normalizedCoords: ::std::os::raw::c_int,
    /// Limit to the anisotropy ratio
    pub maxAnisotropy: ::std::os::raw::c_uint,
    /// Mipmap filter mode
    pub mipmapFilterMode: cudaTextureFilterMode,
    /// Offset applied to the supplied mipmap level
    pub mipmapLevelBias: f32,
    /// Lower end of the mipmap level range to clamp access to
    pub minMipmapLevelClamp: f32,
    /// Upper end of the mipmap level range to clamp access to
    pub maxMipmapLevelClamp: f32,
}
#[test]
fn bindgen_test_layout_cudaTextureDesc() {
    assert_eq!(
        ::std::mem::size_of::<cudaTextureDesc>(),
        64usize,
        concat!("Size of: ", stringify!(cudaTextureDesc))
    );
    assert_eq!(
        ::std::mem::align_of::<cudaTextureDesc>(),
        4usize,
        concat!("Alignment of ", stringify!(cudaTextureDesc))
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaTextureDesc>())).addressMode as *const _ as usize },
        0usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(addressMode)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaTextureDesc>())).filterMode as *const _ as usize },
        12usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(filterMode)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaTextureDesc>())).readMode as *const _ as usize },
        16usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(readMode)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaTextureDesc>())).sRGB as *const _ as usize },
        20usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(sRGB)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaTextureDesc>())).borderColor as *const _ as usize },
        24usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(borderColor)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaTextureDesc>())).normalizedCoords as *const _ as usize
        },
        40usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(normalizedCoords)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaTextureDesc>())).maxAnisotropy as *const _ as usize },
        44usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(maxAnisotropy)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaTextureDesc>())).mipmapFilterMode as *const _ as usize
        },
        48usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(mipmapFilterMode)
        )
    );
    assert_eq!(
        unsafe { &(*(::std::ptr::null::<cudaTextureDesc>())).mipmapLevelBias as *const _ as usize },
        52usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(mipmapLevelBias)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaTextureDesc>())).minMipmapLevelClamp as *const _ as usize
        },
        56usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(minMipmapLevelClamp)
        )
    );
    assert_eq!(
        unsafe {
            &(*(::std::ptr::null::<cudaTextureDesc>())).maxMipmapLevelClamp as *const _ as usize
        },
        60usize,
        concat!(
            "Offset of field: ",
            stringify!(cudaTextureDesc),
            "::",
            stringify!(maxMipmapLevelClamp)
        )
    );
}
/// An opaque value that represents a CUDA texture object
pub type cudaTextureObject_t = ::std::os::raw::c_ulonglong;
pub const cudaDataType_t_CUDA_R_16F: cudaDataType_t = 2;
pub const cudaDataType_t_CUDA_C_16F: cudaDataType_t = 6;
pub const cudaDataType_t_CUDA_R_32F: cudaDataType_t = 0;
pub const cudaDataType_t_CUDA_C_32F: cudaDataType_t = 4;
pub const cudaDataType_t_CUDA_R_64F: cudaDataType_t = 1;
pub const cudaDataType_t_CUDA_C_64F: cudaDataType_t = 5;
pub const cudaDataType_t_CUDA_R_8I: cudaDataType_t = 3;
pub const cudaDataType_t_CUDA_C_8I: cudaDataType_t = 7;
pub const cudaDataType_t_CUDA_R_8U: cudaDataType_t = 8;
pub const cudaDataType_t_CUDA_C_8U: cudaDataType_t = 9;
pub const cudaDataType_t_CUDA_R_32I: cudaDataType_t = 10;
pub const cudaDataType_t_CUDA_C_32I: cudaDataType_t = 11;
pub const cudaDataType_t_CUDA_R_32U: cudaDataType_t = 12;
pub const cudaDataType_t_CUDA_C_32U: cudaDataType_t = 13;
pub type cudaDataType_t = u32;
pub use self::cudaDataType_t as cudaDataType;
pub const libraryPropertyType_t_MAJOR_VERSION: libraryPropertyType_t = 0;
pub const libraryPropertyType_t_MINOR_VERSION: libraryPropertyType_t = 1;
pub const libraryPropertyType_t_PATCH_LEVEL: libraryPropertyType_t = 2;
pub type libraryPropertyType_t = u32;
pub use self::libraryPropertyType_t as libraryPropertyType;
extern "C" {
    /// \brief Destroy all allocations and reset all state on the current device
    /// in the current process.
    ///
    /// Explicitly destroys and cleans up all resources associated with the current
    /// device in the current process.  Any subsequent API call to this device will
    /// reinitialize the device.
    ///
    /// Note that this function will reset the device immediately.  It is the caller's
    /// responsibility to ensure that the device is not being accessed by any
    /// other host threads from the process when this function is called.
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceSynchronize
    pub fn cudaDeviceReset() -> cudaError_t;
}
extern "C" {
    /// \brief Wait for compute device to finish
    ///
    /// Blocks until the device has completed all preceding requested tasks.
    /// ::cudaDeviceSynchronize() returns an error if one of the preceding tasks
    /// has failed. If the ::cudaDeviceScheduleBlockingSync flag was set for
    /// this device, the host thread will block until the device has finished
    /// its work.
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaDeviceReset,
    /// ::cuCtxSynchronize
    pub fn cudaDeviceSynchronize() -> cudaError_t;
}
extern "C" {
    /// \brief Set resource limits
    ///
    /// Setting \p limit to \p value is a request by the application to update
    /// the current limit maintained by the device.  The driver is free to
    /// modify the requested value to meet h/w requirements (this could be
    /// clamping to minimum or maximum values, rounding up to nearest element
    /// size, etc).  The application can use ::cudaDeviceGetLimit() to find out
    /// exactly what the limit has been set to.
    ///
    /// Setting each ::cudaLimit has its own specific restrictions, so each is
    /// discussed here.
    ///
    /// - ::cudaLimitStackSize controls the stack size in bytes of each GPU thread.
    ///
    /// - ::cudaLimitPrintfFifoSize controls the size in bytes of the shared FIFO
    ///   used by the ::printf() device system call. Setting
    ///   ::cudaLimitPrintfFifoSize must not be performed after launching any kernel
    ///   that uses the ::printf() device system call - in such case
    ///   ::cudaErrorInvalidValue will be returned.
    ///
    /// - ::cudaLimitMallocHeapSize controls the size in bytes of the heap used by
    ///   the ::malloc() and ::free() device system calls. Setting
    ///   ::cudaLimitMallocHeapSize must not be performed after launching any kernel
    ///   that uses the ::malloc() or ::free() device system calls - in such case
    ///   ::cudaErrorInvalidValue will be returned.
    ///
    /// - ::cudaLimitDevRuntimeSyncDepth controls the maximum nesting depth of a
    ///   grid at which a thread can safely call ::cudaDeviceSynchronize(). Setting
    ///   this limit must be performed before any launch of a kernel that uses the
    ///   device runtime and calls ::cudaDeviceSynchronize() above the default sync
    ///   depth, two levels of grids. Calls to ::cudaDeviceSynchronize() will fail
    ///   with error code ::cudaErrorSyncDepthExceeded if the limitation is
    ///   violated. This limit can be set smaller than the default or up the maximum
    ///   launch depth of 24. When setting this limit, keep in mind that additional
    ///   levels of sync depth require the runtime to reserve large amounts of
    ///   device memory which can no longer be used for user allocations. If these
    ///   reservations of device memory fail, ::cudaDeviceSetLimit will return
    ///   ::cudaErrorMemoryAllocation, and the limit can be reset to a lower value.
    ///   This limit is only applicable to devices of compute capability 3.5 and
    ///   higher. Attempting to set this limit on devices of compute capability less
    ///   than 3.5 will result in the error ::cudaErrorUnsupportedLimit being
    ///   returned.
    ///
    /// - ::cudaLimitDevRuntimePendingLaunchCount controls the maximum number of
    ///   outstanding device runtime launches that can be made from the current
    ///   device. A grid is outstanding from the point of launch up until the grid
    ///   is known to have been completed. Device runtime launches which violate
    ///   this limitation fail and return ::cudaErrorLaunchPendingCountExceeded when
    ///   ::cudaGetLastError() is called after launch. If more pending launches than
    ///   the default (2048 launches) are needed for a module using the device
    ///   runtime, this limit can be increased. Keep in mind that being able to
    ///   sustain additional pending launches will require the runtime to reserve
    ///   larger amounts of device memory upfront which can no longer be used for
    ///   allocations. If these reservations fail, ::cudaDeviceSetLimit will return
    ///   ::cudaErrorMemoryAllocation, and the limit can be reset to a lower value.
    ///   This limit is only applicable to devices of compute capability 3.5 and
    ///   higher. Attempting to set this limit on devices of compute capability less
    ///   than 3.5 will result in the error ::cudaErrorUnsupportedLimit being
    ///   returned.
    ///
    /// \param limit - Limit to set
    /// \param value - Size of limit
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorUnsupportedLimit,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaDeviceGetLimit,
    /// ::cuCtxSetLimit
    pub fn cudaDeviceSetLimit(limit: cudaLimit, value: usize) -> cudaError_t;
}
extern "C" {
    /// \brief Returns resource limits
    ///
    /// Returns in \p *pValue the current size of \p limit.  The supported
    /// ::cudaLimit values are:
    /// - ::cudaLimitStackSize: stack size in bytes of each GPU thread;
    /// - ::cudaLimitPrintfFifoSize: size in bytes of the shared FIFO used by the
    ///   ::printf() device system call.
    /// - ::cudaLimitMallocHeapSize: size in bytes of the heap used by the
    ///   ::malloc() and ::free() device system calls;
    /// - ::cudaLimitDevRuntimeSyncDepth: maximum grid depth at which a
    ///   thread can isssue the device runtime call ::cudaDeviceSynchronize()
    ///   to wait on child grid launches to complete.
    /// - ::cudaLimitDevRuntimePendingLaunchCount: maximum number of outstanding
    ///   device runtime launches.
    ///
    /// \param limit  - Limit to query
    /// \param pValue - Returned size of the limit
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorUnsupportedLimit,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaDeviceSetLimit,
    /// ::cuCtxGetLimit
    pub fn cudaDeviceGetLimit(pValue: *mut usize, limit: cudaLimit) -> cudaError_t;
}
extern "C" {
    /// \brief Returns the preferred cache configuration for the current device.
    ///
    /// On devices where the L1 cache and shared memory use the same hardware
    /// resources, this returns through \p pCacheConfig the preferred cache
    /// configuration for the current device. This is only a preference. The
    /// runtime will use the requested configuration if possible, but it is free to
    /// choose a different configuration if required to execute functions.
    ///
    /// This will return a \p pCacheConfig of ::cudaFuncCachePreferNone on devices
    /// where the size of the L1 cache and shared memory are fixed.
    ///
    /// The supported cache configurations are:
    /// - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
    /// - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
    /// - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
    /// - ::cudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory
    ///
    /// \param pCacheConfig - Returned cache configuration
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa cudaDeviceSetCacheConfig,
    /// \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
    /// \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)",
    /// ::cuCtxGetCacheConfig
    pub fn cudaDeviceGetCacheConfig(pCacheConfig: *mut cudaFuncCache) -> cudaError_t;
}
extern "C" {
    /// \brief Returns numerical values that correspond to the least and
    /// greatest stream priorities.
    ///
    /// Returns in \p *leastPriority and \p *greatestPriority the numerical values that correspond
    /// to the least and greatest stream priorities respectively. Stream priorities
    /// follow a convention where lower numbers imply greater priorities. The range of
    /// meaningful stream priorities is given by [\p *greatestPriority, \p *leastPriority].
    /// If the user attempts to create a stream with a priority value that is
    /// outside the the meaningful range as specified by this API, the priority is
    /// automatically clamped down or up to either \p *leastPriority or \p *greatestPriority
    /// respectively. See ::cudaStreamCreateWithPriority for details on creating a
    /// priority stream.
    /// A NULL may be passed in for \p *leastPriority or \p *greatestPriority if the value
    /// is not desired.
    ///
    /// This function will return '0' in both \p *leastPriority and \p *greatestPriority if
    /// the current context's device does not support stream priorities
    /// (see ::cudaDeviceGetAttribute).
    ///
    /// \param leastPriority    - Pointer to an int in which the numerical value for least
    ///                           stream priority is returned
    /// \param greatestPriority - Pointer to an int in which the numerical value for greatest
    ///                           stream priority is returned
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreateWithPriority,
    /// ::cudaStreamGetPriority,
    /// ::cuCtxGetStreamPriorityRange
    pub fn cudaDeviceGetStreamPriorityRange(
        leastPriority: *mut ::std::os::raw::c_int,
        greatestPriority: *mut ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Sets the preferred cache configuration for the current device.
    ///
    /// On devices where the L1 cache and shared memory use the same hardware
    /// resources, this sets through \p cacheConfig the preferred cache
    /// configuration for the current device. This is only a preference. The
    /// runtime will use the requested configuration if possible, but it is free to
    /// choose a different configuration if required to execute the function. Any
    /// function preference set via
    /// \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)"
    /// or
    /// \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)"
    /// will be preferred over this device-wide setting. Setting the device-wide
    /// cache configuration to ::cudaFuncCachePreferNone will cause subsequent
    /// kernel launches to prefer to not change the cache configuration unless
    /// required to launch the kernel.
    ///
    /// This setting does nothing on devices where the size of the L1 cache and
    /// shared memory are fixed.
    ///
    /// Launching a kernel with a different preference than the most recent
    /// preference setting may insert a device-side synchronization point.
    ///
    /// The supported cache configurations are:
    /// - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
    /// - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
    /// - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
    /// - ::cudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory
    ///
    /// \param cacheConfig - Requested cache configuration
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceGetCacheConfig,
    /// \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
    /// \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)",
    /// ::cuCtxSetCacheConfig
    pub fn cudaDeviceSetCacheConfig(cacheConfig: cudaFuncCache) -> cudaError_t;
}
extern "C" {
    /// \brief Returns the shared memory configuration for the current device.
    ///
    /// This function will return in \p pConfig the current size of shared memory banks
    /// on the current device. On devices with configurable shared memory banks,
    /// ::cudaDeviceSetSharedMemConfig can be used to change this setting, so that all
    /// subsequent kernel launches will by default use the new bank size. When
    /// ::cudaDeviceGetSharedMemConfig is called on devices without configurable shared
    /// memory, it will return the fixed bank size of the hardware.
    ///
    /// The returned bank configurations can be either:
    /// - ::cudaSharedMemBankSizeFourByte - shared memory bank width is four bytes.
    /// - ::cudaSharedMemBankSizeEightByte - shared memory bank width is eight bytes.
    ///
    /// \param pConfig - Returned cache configuration
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceSetCacheConfig,
    /// ::cudaDeviceGetCacheConfig,
    /// ::cudaDeviceSetSharedMemConfig,
    /// ::cudaFuncSetCacheConfig,
    /// ::cuCtxGetSharedMemConfig
    pub fn cudaDeviceGetSharedMemConfig(pConfig: *mut cudaSharedMemConfig) -> cudaError_t;
}
extern "C" {
    /// \brief Sets the shared memory configuration for the current device.
    ///
    /// On devices with configurable shared memory banks, this function will set
    /// the shared memory bank size which is used for all subsequent kernel launches.
    /// Any per-function setting of shared memory set via ::cudaFuncSetSharedMemConfig
    /// will override the device wide setting.
    ///
    /// Changing the shared memory configuration between launches may introduce
    /// a device side synchronization point.
    ///
    /// Changing the shared memory bank size will not increase shared memory usage
    /// or affect occupancy of kernels, but may have major effects on performance.
    /// Larger bank sizes will allow for greater potential bandwidth to shared memory,
    /// but will change what kinds of accesses to shared memory will result in bank
    /// conflicts.
    ///
    /// This function will do nothing on devices with fixed shared memory bank size.
    ///
    /// The supported bank configurations are:
    /// - ::cudaSharedMemBankSizeDefault: set bank width the device default (currently,
    ///   four bytes)
    /// - ::cudaSharedMemBankSizeFourByte: set shared memory bank width to be four bytes
    ///   natively.
    /// - ::cudaSharedMemBankSizeEightByte: set shared memory bank width to be eight
    ///   bytes natively.
    ///
    /// \param config - Requested cache configuration
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceSetCacheConfig,
    /// ::cudaDeviceGetCacheConfig,
    /// ::cudaDeviceGetSharedMemConfig,
    /// ::cudaFuncSetCacheConfig,
    /// ::cuCtxSetSharedMemConfig
    pub fn cudaDeviceSetSharedMemConfig(config: cudaSharedMemConfig) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a handle to a compute device
    ///
    /// Returns in \p *device a device ordinal given a PCI bus ID string.
    ///
    /// \param device   - Returned device ordinal
    ///
    /// \param pciBusId - String in one of the following forms:
    /// [domain]:[bus]:[device].[function]
    /// [domain]:[bus]:[device]
    /// [bus]:[device].[function]
    /// where \p domain, \p bus, \p device, and \p function are all hexadecimal values
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaDeviceGetPCIBusId,
    /// ::cuDeviceGetByPCIBusId
    pub fn cudaDeviceGetByPCIBusId(
        device: *mut ::std::os::raw::c_int,
        pciBusId: *const ::std::os::raw::c_char,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a PCI Bus Id string for the device
    ///
    /// Returns an ASCII string identifying the device \p dev in the NULL-terminated
    /// string pointed to by \p pciBusId. \p len specifies the maximum length of the
    /// string that may be returned.
    ///
    /// \param pciBusId - Returned identifier string for the device in the following format
    /// [domain]:[bus]:[device].[function]
    /// where \p domain, \p bus, \p device, and \p function are all hexadecimal values.
    /// pciBusId should be large enough to store 13 characters including the NULL-terminator.
    ///
    /// \param len      - Maximum length of string to store in \p name
    ///
    /// \param device   - Device to get identifier string for
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaDeviceGetByPCIBusId,
    /// ::cuDeviceGetPCIBusId
    pub fn cudaDeviceGetPCIBusId(
        pciBusId: *mut ::std::os::raw::c_char,
        len: ::std::os::raw::c_int,
        device: ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Gets an interprocess handle for a previously allocated event
    ///
    /// Takes as input a previously allocated event. This event must have been
    /// created with the ::cudaEventInterprocess and ::cudaEventDisableTiming
    /// flags set. This opaque handle may be copied into other processes and
    /// opened with ::cudaIpcOpenEventHandle to allow efficient hardware
    /// synchronization between GPU work in different processes.
    ///
    /// After the event has been been opened in the importing process,
    /// ::cudaEventRecord, ::cudaEventSynchronize, ::cudaStreamWaitEvent and
    /// ::cudaEventQuery may be used in either process. Performing operations
    /// on the imported event after the exported event has been freed
    /// with ::cudaEventDestroy will result in undefined behavior.
    ///
    /// IPC functionality is restricted to devices with support for unified
    /// addressing on Linux operating systems. IPC functionality is not supported
    /// on Tegra platforms.
    ///
    /// \param handle - Pointer to a user allocated cudaIpcEventHandle
    ///                    in which to return the opaque event handle
    /// \param event   - Event allocated with ::cudaEventInterprocess and
    ///                    ::cudaEventDisableTiming flags.
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorMemoryAllocation,
    /// ::cudaErrorMapBufferObjectFailed,
    /// ::cudaErrorNotSupported
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaEventCreate,
    /// ::cudaEventDestroy,
    /// ::cudaEventSynchronize,
    /// ::cudaEventQuery,
    /// ::cudaStreamWaitEvent,
    /// ::cudaIpcOpenEventHandle,
    /// ::cudaIpcGetMemHandle,
    /// ::cudaIpcOpenMemHandle,
    /// ::cudaIpcCloseMemHandle,
    /// ::cuIpcGetEventHandle
    pub fn cudaIpcGetEventHandle(
        handle: *mut cudaIpcEventHandle_t,
        event: cudaEvent_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Opens an interprocess event handle for use in the current process
    ///
    /// Opens an interprocess event handle exported from another process with
    /// ::cudaIpcGetEventHandle. This function returns a ::cudaEvent_t that behaves like
    /// a locally created event with the ::cudaEventDisableTiming flag specified.
    /// This event must be freed with ::cudaEventDestroy.
    ///
    /// Performing operations on the imported event after the exported event has
    /// been freed with ::cudaEventDestroy will result in undefined behavior.
    ///
    /// IPC functionality is restricted to devices with support for unified
    /// addressing on Linux operating systems. IPC functionality is not supported
    /// on Tegra platforms.
    ///
    /// \param event - Returns the imported event
    /// \param handle  - Interprocess handle to open
    ///
    /// \returns
    /// ::cudaSuccess,
    /// ::cudaErrorMapBufferObjectFailed,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorNotSupported
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaEventCreate,
    /// ::cudaEventDestroy,
    /// ::cudaEventSynchronize,
    /// ::cudaEventQuery,
    /// ::cudaStreamWaitEvent,
    /// ::cudaIpcGetEventHandle,
    /// ::cudaIpcGetMemHandle,
    /// ::cudaIpcOpenMemHandle,
    /// ::cudaIpcCloseMemHandle,
    /// ::cuIpcOpenEventHandle
    pub fn cudaIpcOpenEventHandle(
        event: *mut cudaEvent_t,
        handle: cudaIpcEventHandle_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Gets an interprocess memory handle for an existing device memory
    ///          allocation
    ///
    /// Takes a pointer to the base of an existing device memory allocation created
    /// with ::cudaMalloc and exports it for use in another process. This is a
    /// lightweight operation and may be called multiple times on an allocation
    /// without adverse effects.
    ///
    /// If a region of memory is freed with ::cudaFree and a subsequent call
    /// to ::cudaMalloc returns memory with the same device address,
    /// ::cudaIpcGetMemHandle will return a unique handle for the
    /// new memory.
    ///
    /// IPC functionality is restricted to devices with support for unified
    /// addressing on Linux operating systems. IPC functionality is not supported
    /// on Tegra platforms.
    ///
    /// \param handle - Pointer to user allocated ::cudaIpcMemHandle to return
    ///                    the handle in.
    /// \param devPtr - Base pointer to previously allocated device memory
    ///
    /// \returns
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorMemoryAllocation,
    /// ::cudaErrorMapBufferObjectFailed,
    /// ::cudaErrorNotSupported
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaMalloc,
    /// ::cudaFree,
    /// ::cudaIpcGetEventHandle,
    /// ::cudaIpcOpenEventHandle,
    /// ::cudaIpcOpenMemHandle,
    /// ::cudaIpcCloseMemHandle,
    /// ::cuIpcGetMemHandle
    pub fn cudaIpcGetMemHandle(
        handle: *mut cudaIpcMemHandle_t,
        devPtr: *mut ::std::os::raw::c_void,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Opens an interprocess memory handle exported from another process
    ///          and returns a device pointer usable in the local process.
    ///
    /// Maps memory exported from another process with ::cudaIpcGetMemHandle into
    /// the current device address space. For contexts on different devices
    /// ::cudaIpcOpenMemHandle can attempt to enable peer access between the
    /// devices as if the user called ::cudaDeviceEnablePeerAccess. This behavior is
    /// controlled by the ::cudaIpcMemLazyEnablePeerAccess flag.
    /// ::cudaDeviceCanAccessPeer can determine if a mapping is possible.
    ///
    /// Contexts that may open ::cudaIpcMemHandles are restricted in the following way.
    /// ::cudaIpcMemHandles from each device in a given process may only be opened
    /// by one context per device per other process.
    ///
    /// Memory returned from ::cudaIpcOpenMemHandle must be freed with
    /// ::cudaIpcCloseMemHandle.
    ///
    /// Calling ::cudaFree on an exported memory region before calling
    /// ::cudaIpcCloseMemHandle in the importing context will result in undefined
    /// behavior.
    ///
    /// IPC functionality is restricted to devices with support for unified
    /// addressing on Linux operating systems. IPC functionality is not supported
    /// on Tegra platforms.
    ///
    /// \param devPtr - Returned device pointer
    /// \param handle - ::cudaIpcMemHandle to open
    /// \param flags  - Flags for this operation. Must be specified as ::cudaIpcMemLazyEnablePeerAccess
    ///
    /// \returns
    /// ::cudaSuccess,
    /// ::cudaErrorMapBufferObjectFailed,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorTooManyPeers,
    /// ::cudaErrorNotSupported
    /// \note_init_rt
    /// \note_callback
    ///
    /// \note No guarantees are made about the address returned in \p *devPtr.
    /// In particular, multiple processes may not receive the same address for the same \p handle.
    ///
    /// \sa
    /// ::cudaMalloc,
    /// ::cudaFree,
    /// ::cudaIpcGetEventHandle,
    /// ::cudaIpcOpenEventHandle,
    /// ::cudaIpcGetMemHandle,
    /// ::cudaIpcCloseMemHandle,
    /// ::cudaDeviceEnablePeerAccess,
    /// ::cudaDeviceCanAccessPeer,
    /// ::cuIpcOpenMemHandle
    pub fn cudaIpcOpenMemHandle(
        devPtr: *mut *mut ::std::os::raw::c_void,
        handle: cudaIpcMemHandle_t,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Close memory mapped with cudaIpcOpenMemHandle
    ///
    /// Unmaps memory returnd by ::cudaIpcOpenMemHandle. The original allocation
    /// in the exporting process as well as imported mappings in other processes
    /// will be unaffected.
    ///
    /// Any resources used to enable peer access will be freed if this is the
    /// last mapping using them.
    ///
    /// IPC functionality is restricted to devices with support for unified
    /// addressing on Linux operating systems. IPC functionality is not supported
    /// on Tegra platforms.
    ///
    /// \param devPtr - Device pointer returned by ::cudaIpcOpenMemHandle
    ///
    /// \returns
    /// ::cudaSuccess,
    /// ::cudaErrorMapBufferObjectFailed,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorNotSupported
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaMalloc,
    /// ::cudaFree,
    /// ::cudaIpcGetEventHandle,
    /// ::cudaIpcOpenEventHandle,
    /// ::cudaIpcGetMemHandle,
    /// ::cudaIpcOpenMemHandle,
    /// ::cuIpcCloseMemHandle
    pub fn cudaIpcCloseMemHandle(devPtr: *mut ::std::os::raw::c_void) -> cudaError_t;
}
extern "C" {
    /// \brief Exit and clean up from CUDA launches
    ///
    /// \deprecated
    ///
    /// Note that this function is deprecated because its name does not
    /// reflect its behavior.  Its functionality is identical to the
    /// non-deprecated function ::cudaDeviceReset(), which should be used
    /// instead.
    ///
    /// Explicitly destroys all cleans up all resources associated with the current
    /// device in the current process.  Any subsequent API call to this device will
    /// reinitialize the device.
    ///
    /// Note that this function will reset the device immediately.  It is the caller's
    /// responsibility to ensure that the device is not being accessed by any
    /// other host threads from the process when this function is called.
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceReset
    pub fn cudaThreadExit() -> cudaError_t;
}
extern "C" {
    /// \brief Wait for compute device to finish
    ///
    /// \deprecated
    ///
    /// Note that this function is deprecated because its name does not
    /// reflect its behavior.  Its functionality is similar to the
    /// non-deprecated function ::cudaDeviceSynchronize(), which should be used
    /// instead.
    ///
    /// Blocks until the device has completed all preceding requested tasks.
    /// ::cudaThreadSynchronize() returns an error if one of the preceding tasks
    /// has failed. If the ::cudaDeviceScheduleBlockingSync flag was set for
    /// this device, the host thread will block until the device has finished
    /// its work.
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceSynchronize
    pub fn cudaThreadSynchronize() -> cudaError_t;
}
extern "C" {
    /// \brief Set resource limits
    ///
    /// \deprecated
    ///
    /// Note that this function is deprecated because its name does not
    /// reflect its behavior.  Its functionality is identical to the
    /// non-deprecated function ::cudaDeviceSetLimit(), which should be used
    /// instead.
    ///
    /// Setting \p limit to \p value is a request by the application to update
    /// the current limit maintained by the device.  The driver is free to
    /// modify the requested value to meet h/w requirements (this could be
    /// clamping to minimum or maximum values, rounding up to nearest element
    /// size, etc).  The application can use ::cudaThreadGetLimit() to find out
    /// exactly what the limit has been set to.
    ///
    /// Setting each ::cudaLimit has its own specific restrictions, so each is
    /// discussed here.
    ///
    /// - ::cudaLimitStackSize controls the stack size of each GPU thread.
    ///
    /// - ::cudaLimitPrintfFifoSize controls the size of the shared FIFO
    ///   used by the ::printf() device system call.
    ///   Setting ::cudaLimitPrintfFifoSize must be performed before
    ///   launching any kernel that uses the ::printf() device
    ///   system call, otherwise ::cudaErrorInvalidValue will be returned.
    ///
    /// - ::cudaLimitMallocHeapSize controls the size of the heap used
    ///   by the ::malloc() and ::free() device system calls.  Setting
    ///   ::cudaLimitMallocHeapSize must be performed before launching
    ///   any kernel that uses the ::malloc() or ::free() device system calls,
    ///   otherwise ::cudaErrorInvalidValue will be returned.
    ///
    /// \param limit - Limit to set
    /// \param value - Size in bytes of limit
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorUnsupportedLimit,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceSetLimit
    pub fn cudaThreadSetLimit(limit: cudaLimit, value: usize) -> cudaError_t;
}
extern "C" {
    /// \brief Returns resource limits
    ///
    /// \deprecated
    ///
    /// Note that this function is deprecated because its name does not
    /// reflect its behavior.  Its functionality is identical to the
    /// non-deprecated function ::cudaDeviceGetLimit(), which should be used
    /// instead.
    ///
    /// Returns in \p *pValue the current size of \p limit.  The supported
    /// ::cudaLimit values are:
    /// - ::cudaLimitStackSize: stack size of each GPU thread;
    /// - ::cudaLimitPrintfFifoSize: size of the shared FIFO used by the
    ///   ::printf() device system call.
    /// - ::cudaLimitMallocHeapSize: size of the heap used by the
    ///   ::malloc() and ::free() device system calls;
    ///
    /// \param limit  - Limit to query
    /// \param pValue - Returned size in bytes of limit
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorUnsupportedLimit,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceGetLimit
    pub fn cudaThreadGetLimit(pValue: *mut usize, limit: cudaLimit) -> cudaError_t;
}
extern "C" {
    /// \brief Returns the preferred cache configuration for the current device.
    ///
    /// \deprecated
    ///
    /// Note that this function is deprecated because its name does not
    /// reflect its behavior.  Its functionality is identical to the
    /// non-deprecated function ::cudaDeviceGetCacheConfig(), which should be
    /// used instead.
    ///
    /// On devices where the L1 cache and shared memory use the same hardware
    /// resources, this returns through \p pCacheConfig the preferred cache
    /// configuration for the current device. This is only a preference. The
    /// runtime will use the requested configuration if possible, but it is free to
    /// choose a different configuration if required to execute functions.
    ///
    /// This will return a \p pCacheConfig of ::cudaFuncCachePreferNone on devices
    /// where the size of the L1 cache and shared memory are fixed.
    ///
    /// The supported cache configurations are:
    /// - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
    /// - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
    /// - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
    ///
    /// \param pCacheConfig - Returned cache configuration
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceGetCacheConfig
    pub fn cudaThreadGetCacheConfig(pCacheConfig: *mut cudaFuncCache) -> cudaError_t;
}
extern "C" {
    /// \brief Sets the preferred cache configuration for the current device.
    ///
    /// \deprecated
    ///
    /// Note that this function is deprecated because its name does not
    /// reflect its behavior.  Its functionality is identical to the
    /// non-deprecated function ::cudaDeviceSetCacheConfig(), which should be
    /// used instead.
    ///
    /// On devices where the L1 cache and shared memory use the same hardware
    /// resources, this sets through \p cacheConfig the preferred cache
    /// configuration for the current device. This is only a preference. The
    /// runtime will use the requested configuration if possible, but it is free to
    /// choose a different configuration if required to execute the function. Any
    /// function preference set via
    /// \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)"
    /// or
    /// \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)"
    /// will be preferred over this device-wide setting. Setting the device-wide
    /// cache configuration to ::cudaFuncCachePreferNone will cause subsequent
    /// kernel launches to prefer to not change the cache configuration unless
    /// required to launch the kernel.
    ///
    /// This setting does nothing on devices where the size of the L1 cache and
    /// shared memory are fixed.
    ///
    /// Launching a kernel with a different preference than the most recent
    /// preference setting may insert a device-side synchronization point.
    ///
    /// The supported cache configurations are:
    /// - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
    /// - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
    /// - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
    ///
    /// \param cacheConfig - Requested cache configuration
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceSetCacheConfig
    pub fn cudaThreadSetCacheConfig(cacheConfig: cudaFuncCache) -> cudaError_t;
}
extern "C" {
    /// \brief Returns the last error from a runtime call
    ///
    /// Returns the last error that has been produced by any of the runtime calls
    /// in the same host thread and resets it to ::cudaSuccess.
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorMissingConfiguration,
    /// ::cudaErrorMemoryAllocation,
    /// ::cudaErrorInitializationError,
    /// ::cudaErrorLaunchFailure,
    /// ::cudaErrorLaunchTimeout,
    /// ::cudaErrorLaunchOutOfResources,
    /// ::cudaErrorInvalidDeviceFunction,
    /// ::cudaErrorInvalidConfiguration,
    /// ::cudaErrorInvalidDevice,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidPitchValue,
    /// ::cudaErrorInvalidSymbol,
    /// ::cudaErrorUnmapBufferObjectFailed,
    /// ::cudaErrorInvalidDevicePointer,
    /// ::cudaErrorInvalidTexture,
    /// ::cudaErrorInvalidTextureBinding,
    /// ::cudaErrorInvalidChannelDescriptor,
    /// ::cudaErrorInvalidMemcpyDirection,
    /// ::cudaErrorInvalidFilterSetting,
    /// ::cudaErrorInvalidNormSetting,
    /// ::cudaErrorUnknown,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorInsufficientDriver,
    /// ::cudaErrorNoDevice,
    /// ::cudaErrorSetOnActiveProcess,
    /// ::cudaErrorStartupFailure,
    /// ::cudaErrorInvalidPtx,
    /// ::cudaErrorNoKernelImageForDevice,
    /// ::cudaErrorJitCompilerNotFound
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaPeekAtLastError, ::cudaGetErrorName, ::cudaGetErrorString, ::cudaError
    pub fn cudaGetLastError() -> cudaError_t;
}
extern "C" {
    /// \brief Returns the last error from a runtime call
    ///
    /// Returns the last error that has been produced by any of the runtime calls
    /// in the same host thread. Note that this call does not reset the error to
    /// ::cudaSuccess like ::cudaGetLastError().
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorMissingConfiguration,
    /// ::cudaErrorMemoryAllocation,
    /// ::cudaErrorInitializationError,
    /// ::cudaErrorLaunchFailure,
    /// ::cudaErrorLaunchTimeout,
    /// ::cudaErrorLaunchOutOfResources,
    /// ::cudaErrorInvalidDeviceFunction,
    /// ::cudaErrorInvalidConfiguration,
    /// ::cudaErrorInvalidDevice,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidPitchValue,
    /// ::cudaErrorInvalidSymbol,
    /// ::cudaErrorUnmapBufferObjectFailed,
    /// ::cudaErrorInvalidDevicePointer,
    /// ::cudaErrorInvalidTexture,
    /// ::cudaErrorInvalidTextureBinding,
    /// ::cudaErrorInvalidChannelDescriptor,
    /// ::cudaErrorInvalidMemcpyDirection,
    /// ::cudaErrorInvalidFilterSetting,
    /// ::cudaErrorInvalidNormSetting,
    /// ::cudaErrorUnknown,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorInsufficientDriver,
    /// ::cudaErrorNoDevice,
    /// ::cudaErrorSetOnActiveProcess,
    /// ::cudaErrorStartupFailure,
    /// ::cudaErrorInvalidPtx,
    /// ::cudaErrorNoKernelImageForDevice,
    /// ::cudaErrorJitCompilerNotFound
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetLastError, ::cudaGetErrorName, ::cudaGetErrorString, ::cudaError
    pub fn cudaPeekAtLastError() -> cudaError_t;
}
extern "C" {
    /// \brief Returns the string representation of an error code enum name
    ///
    /// Returns a string containing the name of an error code in the enum.  If the error
    /// code is not recognized, "unrecognized error code" is returned.
    ///
    /// \param error - Error code to convert to string
    ///
    /// \return
    /// \p char* pointer to a NULL-terminated string
    ///
    /// \sa ::cudaGetErrorString, ::cudaGetLastError, ::cudaPeekAtLastError, ::cudaError,
    /// ::cuGetErrorName
    pub fn cudaGetErrorName(error: cudaError_t) -> *const ::std::os::raw::c_char;
}
extern "C" {
    /// \brief Returns the description string for an error code
    ///
    /// Returns the description string for an error code.  If the error
    /// code is not recognized, "unrecognized error code" is returned.
    ///
    /// \param error - Error code to convert to string
    ///
    /// \return
    /// \p char* pointer to a NULL-terminated string
    ///
    /// \sa ::cudaGetErrorName, ::cudaGetLastError, ::cudaPeekAtLastError, ::cudaError,
    /// ::cuGetErrorString
    pub fn cudaGetErrorString(error: cudaError_t) -> *const ::std::os::raw::c_char;
}
extern "C" {
    /// \brief Returns the number of compute-capable devices
    ///
    /// Returns in \p *count the number of devices with compute capability greater
    /// or equal to 2.0 that are available for execution.
    ///
    /// \param count - Returns the number of devices with compute capability
    /// greater or equal to 2.0
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetDevice, ::cudaSetDevice, ::cudaGetDeviceProperties,
    /// ::cudaChooseDevice,
    /// ::cuDeviceGetCount
    pub fn cudaGetDeviceCount(count: *mut ::std::os::raw::c_int) -> cudaError_t;
}
extern "C" {
    /// \brief Returns information about the compute-device
    ///
    /// Returns in \p *prop the properties of device \p dev. The ::cudaDeviceProp
    /// structure is defined as:
    /// \code
    ///struct cudaDeviceProp {
    ///char name[256];
    ///cudaUUID_t uuid;
    ///size_t totalGlobalMem;
    ///size_t sharedMemPerBlock;
    ///int regsPerBlock;
    ///int warpSize;
    ///size_t memPitch;
    ///int maxThreadsPerBlock;
    ///int maxThreadsDim[3];
    ///int maxGridSize[3];
    ///int clockRate;
    ///size_t totalConstMem;
    ///int major;
    ///int minor;
    ///size_t textureAlignment;
    ///size_t texturePitchAlignment;
    ///int deviceOverlap;
    ///int multiProcessorCount;
    ///int kernelExecTimeoutEnabled;
    ///int integrated;
    ///int canMapHostMemory;
    ///int computeMode;
    ///int maxTexture1D;
    ///int maxTexture1DMipmap;
    ///int maxTexture1DLinear;
    ///int maxTexture2D[2];
    ///int maxTexture2DMipmap[2];
    ///int maxTexture2DLinear[3];
    ///int maxTexture2DGather[2];
    ///int maxTexture3D[3];
    ///int maxTexture3DAlt[3];
    ///int maxTextureCubemap;
    ///int maxTexture1DLayered[2];
    ///int maxTexture2DLayered[3];
    ///int maxTextureCubemapLayered[2];
    ///int maxSurface1D;
    ///int maxSurface2D[2];
    ///int maxSurface3D[3];
    ///int maxSurface1DLayered[2];
    ///int maxSurface2DLayered[3];
    ///int maxSurfaceCubemap;
    ///int maxSurfaceCubemapLayered[2];
    ///size_t surfaceAlignment;
    ///int concurrentKernels;
    ///int ECCEnabled;
    ///int pciBusID;
    ///int pciDeviceID;
    ///int pciDomainID;
    ///int tccDriver;
    ///int asyncEngineCount;
    ///int unifiedAddressing;
    ///int memoryClockRate;
    ///int memoryBusWidth;
    ///int l2CacheSize;
    ///int maxThreadsPerMultiProcessor;
    ///int streamPrioritiesSupported;
    ///int globalL1CacheSupported;
    ///int localL1CacheSupported;
    ///size_t sharedMemPerMultiprocessor;
    ///int regsPerMultiprocessor;
    ///int managedMemory;
    ///int isMultiGpuBoard;
    ///int multiGpuBoardGroupID;
    ///int singleToDoublePrecisionPerfRatio;
    ///int pageableMemoryAccess;
    ///int concurrentManagedAccess;
    ///int computePreemptionSupported;
    ///int canUseHostPointerForRegisteredMem;
    ///int cooperativeLaunch;
    ///int cooperativeMultiDeviceLaunch;
    ///int pageableMemoryAccessUsesHostPageTables;
    ///int directManagedMemAccessFromHost;
    ///}
    ///\endcode
    /// where:
    /// - \ref ::cudaDeviceProp::name "name[256]" is an ASCII string identifying
    ///   the device;
    /// - \ref ::cudaDeviceProp::uuid "uuid" is a 16-byte unique identifier.
    /// - \ref ::cudaDeviceProp::totalGlobalMem "totalGlobalMem" is the total
    ///   amount of global memory available on the device in bytes;
    /// - \ref ::cudaDeviceProp::sharedMemPerBlock "sharedMemPerBlock" is the
    ///   maximum amount of shared memory available to a thread block in bytes;
    /// - \ref ::cudaDeviceProp::regsPerBlock "regsPerBlock" is the maximum number
    ///   of 32-bit registers available to a thread block;
    /// - \ref ::cudaDeviceProp::warpSize "warpSize" is the warp size in threads;
    /// - \ref ::cudaDeviceProp::memPitch "memPitch" is the maximum pitch in
    ///   bytes allowed by the memory copy functions that involve memory regions
    ///   allocated through ::cudaMallocPitch();
    /// - \ref ::cudaDeviceProp::maxThreadsPerBlock "maxThreadsPerBlock" is the
    ///   maximum number of threads per block;
    /// - \ref ::cudaDeviceProp::maxThreadsDim "maxThreadsDim[3]" contains the
    ///   maximum size of each dimension of a block;
    /// - \ref ::cudaDeviceProp::maxGridSize "maxGridSize[3]" contains the
    ///   maximum size of each dimension of a grid;
    /// - \ref ::cudaDeviceProp::clockRate "clockRate" is the clock frequency in
    ///   kilohertz;
    /// - \ref ::cudaDeviceProp::totalConstMem "totalConstMem" is the total amount
    ///   of constant memory available on the device in bytes;
    /// - \ref ::cudaDeviceProp::major "major",
    ///   \ref ::cudaDeviceProp::minor "minor" are the major and minor revision
    ///   numbers defining the device's compute capability;
    /// - \ref ::cudaDeviceProp::textureAlignment "textureAlignment" is the
    ///   alignment requirement; texture base addresses that are aligned to
    ///   \ref ::cudaDeviceProp::textureAlignment "textureAlignment" bytes do not
    ///   need an offset applied to texture fetches;
    /// - \ref ::cudaDeviceProp::texturePitchAlignment "texturePitchAlignment" is the
    ///   pitch alignment requirement for 2D texture references that are bound to
    ///   pitched memory;
    /// - \ref ::cudaDeviceProp::deviceOverlap "deviceOverlap" is 1 if the device
    ///   can concurrently copy memory between host and device while executing a
    ///   kernel, or 0 if not.  Deprecated, use instead asyncEngineCount.
    /// - \ref ::cudaDeviceProp::multiProcessorCount "multiProcessorCount" is the
    ///   number of multiprocessors on the device;
    /// - \ref ::cudaDeviceProp::kernelExecTimeoutEnabled "kernelExecTimeoutEnabled"
    ///   is 1 if there is a run time limit for kernels executed on the device, or
    ///   0 if not.
    /// - \ref ::cudaDeviceProp::integrated "integrated" is 1 if the device is an
    ///   integrated (motherboard) GPU and 0 if it is a discrete (card) component.
    /// - \ref ::cudaDeviceProp::canMapHostMemory "canMapHostMemory" is 1 if the
    ///   device can map host memory into the CUDA address space for use with
    ///   ::cudaHostAlloc()/::cudaHostGetDevicePointer(), or 0 if not;
    /// - \ref ::cudaDeviceProp::computeMode "computeMode" is the compute mode
    ///   that the device is currently in. Available modes are as follows:
    ///   - cudaComputeModeDefault: Default mode - Device is not restricted and
    ///     multiple threads can use ::cudaSetDevice() with this device.
    ///   - cudaComputeModeExclusive: Compute-exclusive mode - Only one thread will
    ///     be able to use ::cudaSetDevice() with this device.
    ///   - cudaComputeModeProhibited: Compute-prohibited mode - No threads can use
    ///     ::cudaSetDevice() with this device.
    ///   - cudaComputeModeExclusiveProcess: Compute-exclusive-process mode - Many
    ///     threads in one process will be able to use ::cudaSetDevice() with this device.
    ///   <br> If ::cudaSetDevice() is called on an already occupied \p device with
    ///   computeMode ::cudaComputeModeExclusive, ::cudaErrorDeviceAlreadyInUse
    ///   will be immediately returned indicating the device cannot be used.
    ///   When an occupied exclusive mode device is chosen with ::cudaSetDevice,
    ///   all subsequent non-device management runtime functions will return
    ///   ::cudaErrorDevicesUnavailable.
    /// - \ref ::cudaDeviceProp::maxTexture1D "maxTexture1D" is the maximum 1D
    ///   texture size.
    /// - \ref ::cudaDeviceProp::maxTexture1DMipmap "maxTexture1DMipmap" is the maximum
    ///   1D mipmapped texture texture size.
    /// - \ref ::cudaDeviceProp::maxTexture1DLinear "maxTexture1DLinear" is the maximum
    ///   1D texture size for textures bound to linear memory.
    /// - \ref ::cudaDeviceProp::maxTexture2D "maxTexture2D[2]" contains the maximum
    ///   2D texture dimensions.
    /// - \ref ::cudaDeviceProp::maxTexture2DMipmap "maxTexture2DMipmap[2]" contains the
    ///   maximum 2D mipmapped texture dimensions.
    /// - \ref ::cudaDeviceProp::maxTexture2DLinear "maxTexture2DLinear[3]" contains the
    ///   maximum 2D texture dimensions for 2D textures bound to pitch linear memory.
    /// - \ref ::cudaDeviceProp::maxTexture2DGather "maxTexture2DGather[2]" contains the
    ///   maximum 2D texture dimensions if texture gather operations have to be performed.
    /// - \ref ::cudaDeviceProp::maxTexture3D "maxTexture3D[3]" contains the maximum
    ///   3D texture dimensions.
    /// - \ref ::cudaDeviceProp::maxTexture3DAlt "maxTexture3DAlt[3]"
    ///   contains the maximum alternate 3D texture dimensions.
    /// - \ref ::cudaDeviceProp::maxTextureCubemap "maxTextureCubemap" is the
    ///   maximum cubemap texture width or height.
    /// - \ref ::cudaDeviceProp::maxTexture1DLayered "maxTexture1DLayered[2]" contains
    ///   the maximum 1D layered texture dimensions.
    /// - \ref ::cudaDeviceProp::maxTexture2DLayered "maxTexture2DLayered[3]" contains
    ///   the maximum 2D layered texture dimensions.
    /// - \ref ::cudaDeviceProp::maxTextureCubemapLayered "maxTextureCubemapLayered[2]"
    ///   contains the maximum cubemap layered texture dimensions.
    /// - \ref ::cudaDeviceProp::maxSurface1D "maxSurface1D" is the maximum 1D
    ///   surface size.
    /// - \ref ::cudaDeviceProp::maxSurface2D "maxSurface2D[2]" contains the maximum
    ///   2D surface dimensions.
    /// - \ref ::cudaDeviceProp::maxSurface3D "maxSurface3D[3]" contains the maximum
    ///   3D surface dimensions.
    /// - \ref ::cudaDeviceProp::maxSurface1DLayered "maxSurface1DLayered[2]" contains
    ///   the maximum 1D layered surface dimensions.
    /// - \ref ::cudaDeviceProp::maxSurface2DLayered "maxSurface2DLayered[3]" contains
    ///   the maximum 2D layered surface dimensions.
    /// - \ref ::cudaDeviceProp::maxSurfaceCubemap "maxSurfaceCubemap" is the maximum
    ///   cubemap surface width or height.
    /// - \ref ::cudaDeviceProp::maxSurfaceCubemapLayered "maxSurfaceCubemapLayered[2]"
    ///   contains the maximum cubemap layered surface dimensions.
    /// - \ref ::cudaDeviceProp::surfaceAlignment "surfaceAlignment" specifies the
    ///   alignment requirements for surfaces.
    /// - \ref ::cudaDeviceProp::concurrentKernels "concurrentKernels" is 1 if the
    ///   device supports executing multiple kernels within the same context
    ///   simultaneously, or 0 if not. It is not guaranteed that multiple kernels
    ///   will be resident on the device concurrently so this feature should not be
    ///   relied upon for correctness;
    /// - \ref ::cudaDeviceProp::ECCEnabled "ECCEnabled" is 1 if the device has ECC
    ///   support turned on, or 0 if not.
    /// - \ref ::cudaDeviceProp::pciBusID "pciBusID" is the PCI bus identifier of
    ///   the device.
    /// - \ref ::cudaDeviceProp::pciDeviceID "pciDeviceID" is the PCI device
    ///   (sometimes called slot) identifier of the device.
    /// - \ref ::cudaDeviceProp::pciDomainID "pciDomainID" is the PCI domain identifier
    ///   of the device.
    /// - \ref ::cudaDeviceProp::tccDriver "tccDriver" is 1 if the device is using a
    ///   TCC driver or 0 if not.
    /// - \ref ::cudaDeviceProp::asyncEngineCount "asyncEngineCount" is 1 when the
    ///   device can concurrently copy memory between host and device while executing
    ///   a kernel. It is 2 when the device can concurrently copy memory between host
    ///   and device in both directions and execute a kernel at the same time. It is
    ///   0 if neither of these is supported.
    /// - \ref ::cudaDeviceProp::unifiedAddressing "unifiedAddressing" is 1 if the device
    ///   shares a unified address space with the host and 0 otherwise.
    /// - \ref ::cudaDeviceProp::memoryClockRate "memoryClockRate" is the peak memory
    ///   clock frequency in kilohertz.
    /// - \ref ::cudaDeviceProp::memoryBusWidth "memoryBusWidth" is the memory bus width
    ///   in bits.
    /// - \ref ::cudaDeviceProp::l2CacheSize "l2CacheSize" is L2 cache size in bytes.
    /// - \ref ::cudaDeviceProp::maxThreadsPerMultiProcessor "maxThreadsPerMultiProcessor"
    ///   is the number of maximum resident threads per multiprocessor.
    /// - \ref ::cudaDeviceProp::streamPrioritiesSupported "streamPrioritiesSupported"
    ///   is 1 if the device supports stream priorities, or 0 if it is not supported.
    /// - \ref ::cudaDeviceProp::globalL1CacheSupported "globalL1CacheSupported"
    ///   is 1 if the device supports caching of globals in L1 cache, or 0 if it is not supported.
    /// - \ref ::cudaDeviceProp::localL1CacheSupported "localL1CacheSupported"
    ///   is 1 if the device supports caching of locals in L1 cache, or 0 if it is not supported.
    /// - \ref ::cudaDeviceProp::sharedMemPerMultiprocessor "sharedMemPerMultiprocessor" is the
    ///   maximum amount of shared memory available to a multiprocessor in bytes; this amount is
    ///   shared by all thread blocks simultaneously resident on a multiprocessor;
    /// - \ref ::cudaDeviceProp::regsPerMultiprocessor "regsPerMultiprocessor" is the maximum number
    ///   of 32-bit registers available to a multiprocessor; this number is shared
    ///   by all thread blocks simultaneously resident on a multiprocessor;
    /// - \ref ::cudaDeviceProp::managedMemory "managedMemory"
    ///   is 1 if the device supports allocating managed memory on this system, or 0 if it is not supported.
    /// - \ref ::cudaDeviceProp::isMultiGpuBoard "isMultiGpuBoard"
    ///   is 1 if the device is on a multi-GPU board (e.g. Gemini cards), and 0 if not;
    /// - \ref ::cudaDeviceProp::multiGpuBoardGroupID "multiGpuBoardGroupID" is a unique identifier
    ///   for a group of devices associated with the same board.
    ///   Devices on the same multi-GPU board will share the same identifier;
    /// - \ref ::cudaDeviceProp::singleToDoublePrecisionPerfRatio "singleToDoublePrecisionPerfRatio"
    ///   is the ratio of single precision performance (in floating-point operations per second)
    ///   to double precision performance.
    /// - \ref ::cudaDeviceProp::pageableMemoryAccess "pageableMemoryAccess" is 1 if the device supports
    ///   coherently accessing pageable memory without calling cudaHostRegister on it, and 0 otherwise.
    /// - \ref ::cudaDeviceProp::concurrentManagedAccess "concurrentManagedAccess" is 1 if the device can
    ///   coherently access managed memory concurrently with the CPU, and 0 otherwise.
    /// - \ref ::cudaDeviceProp::computePreemptionSupported "computePreemptionSupported" is 1 if the device
    ///   supports Compute Preemption, and 0 otherwise.
    /// - \ref ::cudaDeviceProp::canUseHostPointerForRegisteredMem "canUseHostPointerForRegisteredMem" is 1 if
    ///   the device can access host registered memory at the same virtual address as the CPU, and 0 otherwise.
    /// - \ref ::cudaDeviceProp::cooperativeLaunch "cooperativeLaunch" is 1 if the device supports launching
    ///   cooperative kernels via ::cudaLaunchCooperativeKernel, and 0 otherwise.
    /// - \ref ::cudaDeviceProp::cooperativeMultiDeviceLaunch "cooperativeMultiDeviceLaunch" is 1 if the device
    ///   supports launching cooperative kernels via ::cudaLaunchCooperativeKernelMultiDevice, and 0 otherwise.
    /// - \ref ::cudaDeviceProp::pageableMemoryAccessUsesHostPageTables "pageableMemoryAccessUsesHostPageTables" is 1 if the device accesses
    ///   pageable memory via the host's page tables, and 0 otherwise.
    /// - \ref ::cudaDeviceProp::directManagedMemAccessFromHost "directManagedMemAccessFromHost" is 1 if the host can directly access managed
    ///   memory on the device without migration, and 0 otherwise.
    ///
    /// \param prop   - Properties for the specified device
    /// \param device - Device number to get properties for
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice, ::cudaChooseDevice,
    /// ::cudaDeviceGetAttribute,
    /// ::cuDeviceGetAttribute,
    /// ::cuDeviceGetName
    pub fn cudaGetDeviceProperties(
        prop: *mut cudaDeviceProp,
        device: ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns information about the device
    ///
    /// Returns in \p *value the integer value of the attribute \p attr on device
    /// \p device. The supported attributes are:
    /// - ::cudaDevAttrMaxThreadsPerBlock: Maximum number of threads per block;
    /// - ::cudaDevAttrMaxBlockDimX: Maximum x-dimension of a block;
    /// - ::cudaDevAttrMaxBlockDimY: Maximum y-dimension of a block;
    /// - ::cudaDevAttrMaxBlockDimZ: Maximum z-dimension of a block;
    /// - ::cudaDevAttrMaxGridDimX: Maximum x-dimension of a grid;
    /// - ::cudaDevAttrMaxGridDimY: Maximum y-dimension of a grid;
    /// - ::cudaDevAttrMaxGridDimZ: Maximum z-dimension of a grid;
    /// - ::cudaDevAttrMaxSharedMemoryPerBlock: Maximum amount of shared memory
    ///   available to a thread block in bytes;
    /// - ::cudaDevAttrTotalConstantMemory: Memory available on device for
    ///   __constant__ variables in a CUDA C kernel in bytes;
    /// - ::cudaDevAttrWarpSize: Warp size in threads;
    /// - ::cudaDevAttrMaxPitch: Maximum pitch in bytes allowed by the memory copy
    ///   functions that involve memory regions allocated through ::cudaMallocPitch();
    /// - ::cudaDevAttrMaxTexture1DWidth: Maximum 1D texture width;
    /// - ::cudaDevAttrMaxTexture1DLinearWidth: Maximum width for a 1D texture bound
    ///   to linear memory;
    /// - ::cudaDevAttrMaxTexture1DMipmappedWidth: Maximum mipmapped 1D texture width;
    /// - ::cudaDevAttrMaxTexture2DWidth: Maximum 2D texture width;
    /// - ::cudaDevAttrMaxTexture2DHeight: Maximum 2D texture height;
    /// - ::cudaDevAttrMaxTexture2DLinearWidth: Maximum width for a 2D texture
    ///   bound to linear memory;
    /// - ::cudaDevAttrMaxTexture2DLinearHeight: Maximum height for a 2D texture
    ///   bound to linear memory;
    /// - ::cudaDevAttrMaxTexture2DLinearPitch: Maximum pitch in bytes for a 2D
    ///   texture bound to linear memory;
    /// - ::cudaDevAttrMaxTexture2DMipmappedWidth: Maximum mipmapped 2D texture
    ///   width;
    /// - ::cudaDevAttrMaxTexture2DMipmappedHeight: Maximum mipmapped 2D texture
    ///   height;
    /// - ::cudaDevAttrMaxTexture3DWidth: Maximum 3D texture width;
    /// - ::cudaDevAttrMaxTexture3DHeight: Maximum 3D texture height;
    /// - ::cudaDevAttrMaxTexture3DDepth: Maximum 3D texture depth;
    /// - ::cudaDevAttrMaxTexture3DWidthAlt: Alternate maximum 3D texture width,
    ///   0 if no alternate maximum 3D texture size is supported;
    /// - ::cudaDevAttrMaxTexture3DHeightAlt: Alternate maximum 3D texture height,
    ///   0 if no alternate maximum 3D texture size is supported;
    /// - ::cudaDevAttrMaxTexture3DDepthAlt: Alternate maximum 3D texture depth,
    ///   0 if no alternate maximum 3D texture size is supported;
    /// - ::cudaDevAttrMaxTextureCubemapWidth: Maximum cubemap texture width or
    ///   height;
    /// - ::cudaDevAttrMaxTexture1DLayeredWidth: Maximum 1D layered texture width;
    /// - ::cudaDevAttrMaxTexture1DLayeredLayers: Maximum layers in a 1D layered
    ///   texture;
    /// - ::cudaDevAttrMaxTexture2DLayeredWidth: Maximum 2D layered texture width;
    /// - ::cudaDevAttrMaxTexture2DLayeredHeight: Maximum 2D layered texture height;
    /// - ::cudaDevAttrMaxTexture2DLayeredLayers: Maximum layers in a 2D layered
    ///   texture;
    /// - ::cudaDevAttrMaxTextureCubemapLayeredWidth: Maximum cubemap layered
    ///   texture width or height;
    /// - ::cudaDevAttrMaxTextureCubemapLayeredLayers: Maximum layers in a cubemap
    ///   layered texture;
    /// - ::cudaDevAttrMaxSurface1DWidth: Maximum 1D surface width;
    /// - ::cudaDevAttrMaxSurface2DWidth: Maximum 2D surface width;
    /// - ::cudaDevAttrMaxSurface2DHeight: Maximum 2D surface height;
    /// - ::cudaDevAttrMaxSurface3DWidth: Maximum 3D surface width;
    /// - ::cudaDevAttrMaxSurface3DHeight: Maximum 3D surface height;
    /// - ::cudaDevAttrMaxSurface3DDepth: Maximum 3D surface depth;
    /// - ::cudaDevAttrMaxSurface1DLayeredWidth: Maximum 1D layered surface width;
    /// - ::cudaDevAttrMaxSurface1DLayeredLayers: Maximum layers in a 1D layered
    ///   surface;
    /// - ::cudaDevAttrMaxSurface2DLayeredWidth: Maximum 2D layered surface width;
    /// - ::cudaDevAttrMaxSurface2DLayeredHeight: Maximum 2D layered surface height;
    /// - ::cudaDevAttrMaxSurface2DLayeredLayers: Maximum layers in a 2D layered
    ///   surface;
    /// - ::cudaDevAttrMaxSurfaceCubemapWidth: Maximum cubemap surface width;
    /// - ::cudaDevAttrMaxSurfaceCubemapLayeredWidth: Maximum cubemap layered
    ///   surface width;
    /// - ::cudaDevAttrMaxSurfaceCubemapLayeredLayers: Maximum layers in a cubemap
    ///   layered surface;
    /// - ::cudaDevAttrMaxRegistersPerBlock: Maximum number of 32-bit registers
    ///   available to a thread block;
    /// - ::cudaDevAttrClockRate: Peak clock frequency in kilohertz;
    /// - ::cudaDevAttrTextureAlignment: Alignment requirement; texture base
    ///   addresses aligned to ::textureAlign bytes do not need an offset applied
    ///   to texture fetches;
    /// - ::cudaDevAttrTexturePitchAlignment: Pitch alignment requirement for 2D
    ///   texture references bound to pitched memory;
    /// - ::cudaDevAttrGpuOverlap: 1 if the device can concurrently copy memory
    ///   between host and device while executing a kernel, or 0 if not;
    /// - ::cudaDevAttrMultiProcessorCount: Number of multiprocessors on the device;
    /// - ::cudaDevAttrKernelExecTimeout: 1 if there is a run time limit for kernels
    ///   executed on the device, or 0 if not;
    /// - ::cudaDevAttrIntegrated: 1 if the device is integrated with the memory
    ///   subsystem, or 0 if not;
    /// - ::cudaDevAttrCanMapHostMemory: 1 if the device can map host memory into
    ///   the CUDA address space, or 0 if not;
    /// - ::cudaDevAttrComputeMode: Compute mode is the compute mode that the device
    ///   is currently in. Available modes are as follows:
    ///   - ::cudaComputeModeDefault: Default mode - Device is not restricted and
    ///     multiple threads can use ::cudaSetDevice() with this device.
    ///   - ::cudaComputeModeExclusive: Compute-exclusive mode - Only one thread will
    ///     be able to use ::cudaSetDevice() with this device.
    ///   - ::cudaComputeModeProhibited: Compute-prohibited mode - No threads can use
    ///     ::cudaSetDevice() with this device.
    ///   - ::cudaComputeModeExclusiveProcess: Compute-exclusive-process mode - Many
    ///     threads in one process will be able to use ::cudaSetDevice() with this
    ///     device.
    /// - ::cudaDevAttrConcurrentKernels: 1 if the device supports executing
    ///   multiple kernels within the same context simultaneously, or 0 if
    ///   not. It is not guaranteed that multiple kernels will be resident on the
    ///   device concurrently so this feature should not be relied upon for
    ///   correctness;
    /// - ::cudaDevAttrEccEnabled: 1 if error correction is enabled on the device,
    ///   0 if error correction is disabled or not supported by the device;
    /// - ::cudaDevAttrPciBusId: PCI bus identifier of the device;
    /// - ::cudaDevAttrPciDeviceId: PCI device (also known as slot) identifier of
    ///   the device;
    /// - ::cudaDevAttrTccDriver: 1 if the device is using a TCC driver. TCC is only
    ///   available on Tesla hardware running Windows Vista or later;
    /// - ::cudaDevAttrMemoryClockRate: Peak memory clock frequency in kilohertz;
    /// - ::cudaDevAttrGlobalMemoryBusWidth: Global memory bus width in bits;
    /// - ::cudaDevAttrL2CacheSize: Size of L2 cache in bytes. 0 if the device
    ///   doesn't have L2 cache;
    /// - ::cudaDevAttrMaxThreadsPerMultiProcessor: Maximum resident threads per
    ///   multiprocessor;
    /// - ::cudaDevAttrUnifiedAddressing: 1 if the device shares a unified address
    ///   space with the host, or 0 if not;
    /// - ::cudaDevAttrComputeCapabilityMajor: Major compute capability version
    ///   number;
    /// - ::cudaDevAttrComputeCapabilityMinor: Minor compute capability version
    ///   number;
    /// - ::cudaDevAttrStreamPrioritiesSupported: 1 if the device supports stream
    ///   priorities, or 0 if not;
    /// - ::cudaDevAttrGlobalL1CacheSupported: 1 if device supports caching globals
    ///    in L1 cache, 0 if not;
    /// - ::cudaDevAttrLocalL1CacheSupported: 1 if device supports caching locals
    ///    in L1 cache, 0 if not;
    /// - ::cudaDevAttrMaxSharedMemoryPerMultiprocessor: Maximum amount of shared memory
    ///   available to a multiprocessor in bytes; this amount is shared by all
    ///   thread blocks simultaneously resident on a multiprocessor;
    /// - ::cudaDevAttrMaxRegistersPerMultiprocessor: Maximum number of 32-bit registers
    ///   available to a multiprocessor; this number is shared by all thread blocks
    ///   simultaneously resident on a multiprocessor;
    /// - ::cudaDevAttrManagedMemory: 1 if device supports allocating
    ///   managed memory, 0 if not;
    /// - ::cudaDevAttrIsMultiGpuBoard: 1 if device is on a multi-GPU board, 0 if not;
    /// - ::cudaDevAttrMultiGpuBoardGroupID: Unique identifier for a group of devices on the
    ///   same multi-GPU board;
    /// - ::cudaDevAttrHostNativeAtomicSupported: 1 if the link between the device and the
    ///   host supports native atomic operations;
    /// - ::cudaDevAttrSingleToDoublePrecisionPerfRatio: Ratio of single precision performance
    ///   (in floating-point operations per second) to double precision performance;
    /// - ::cudaDevAttrPageableMemoryAccess: 1 if the device supports coherently accessing
    ///   pageable memory without calling cudaHostRegister on it, and 0 otherwise.
    /// - ::cudaDevAttrConcurrentManagedAccess: 1 if the device can coherently access managed
    ///   memory concurrently with the CPU, and 0 otherwise.
    /// - ::cudaDevAttrComputePreemptionSupported: 1 if the device supports
    ///   Compute Preemption, 0 if not.
    /// - ::cudaDevAttrCanUseHostPointerForRegisteredMem: 1 if the device can access host
    ///   registered memory at the same virtual address as the CPU, and 0 otherwise.
    /// - ::cudaDevAttrCooperativeLaunch: 1 if the device supports launching cooperative kernels
    ///   via ::cudaLaunchCooperativeKernel, and 0 otherwise.
    /// - ::cudaDevAttrCooperativeMultiDeviceLaunch: 1 if the device supports launching cooperative
    ///   kernels via ::cudaLaunchCooperativeKernelMultiDevice, and 0 otherwise.
    /// - ::cudaDevAttrCanFlushRemoteWrites: 1 if the device supports flushing of outstanding
    ///   remote writes, and 0 otherwise.
    /// - ::cudaDevAttrHostRegisterSupported: 1 if the device supports host memory registration
    ///   via ::cudaHostRegister, and 0 otherwise.
    /// - ::cudaDevAttrPageableMemoryAccessUsesHostPageTables: 1 if the device accesses pageable memory via the
    ///   host's page tables, and 0 otherwise.
    /// - ::cudaDevAttrDirectManagedMemAccessFromHost: 1 if the host can directly access managed memory on the device
    ///   without migration, and 0 otherwise.
    ///
    /// \param value  - Returned device attribute value
    /// \param attr   - Device attribute to query
    /// \param device - Device number to query
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevice,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice, ::cudaChooseDevice,
    /// ::cudaGetDeviceProperties,
    /// ::cuDeviceGetAttribute
    pub fn cudaDeviceGetAttribute(
        value: *mut ::std::os::raw::c_int,
        attr: cudaDeviceAttr,
        device: ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Queries attributes of the link between two devices.
    ///
    /// Returns in \p *value the value of the requested attribute \p attrib of the
    /// link between \p srcDevice and \p dstDevice. The supported attributes are:
    /// - ::cudaDevP2PAttrPerformanceRank: A relative value indicating the
    ///   performance of the link between two devices. Lower value means better
    ///   performance (0 being the value used for most performant link).
    /// - ::cudaDevP2PAttrAccessSupported: 1 if peer access is enabled.
    /// - ::cudaDevP2PAttrNativeAtomicSupported: 1 if native atomic operations over
    ///   the link are supported.
    /// - ::cudaDevP2PAttrCudaArrayAccessSupported: 1 if accessing CUDA arrays over
    ///   the link is supported.
    ///
    /// Returns ::cudaErrorInvalidDevice if \p srcDevice or \p dstDevice are not valid
    /// or if they represent the same device.
    ///
    /// Returns ::cudaErrorInvalidValue if \p attrib is not valid or if \p value is
    /// a null pointer.
    ///
    /// \param value         - Returned value of the requested attribute
    /// \param attrib        - The requested attribute of the link between \p srcDevice and \p dstDevice.
    /// \param srcDevice     - The source device of the target link.
    /// \param dstDevice     - The destination device of the target link.
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevice,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaCtxEnablePeerAccess,
    /// ::cudaCtxDisablePeerAccess,
    /// ::cudaCtxCanAccessPeer,
    /// ::cuDeviceGetP2PAttribute
    pub fn cudaDeviceGetP2PAttribute(
        value: *mut ::std::os::raw::c_int,
        attr: cudaDeviceP2PAttr,
        srcDevice: ::std::os::raw::c_int,
        dstDevice: ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Select compute-device which best matches criteria
    ///
    /// Returns in \p *device the device which has properties that best match
    /// \p *prop.
    ///
    /// \param device - Device with best match
    /// \param prop   - Desired device properties
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice,
    /// ::cudaGetDeviceProperties
    pub fn cudaChooseDevice(
        device: *mut ::std::os::raw::c_int,
        prop: *const cudaDeviceProp,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Set device to be used for GPU executions
    ///
    /// Sets \p device as the current device for the calling host thread.
    /// Valid device id's are 0 to (::cudaGetDeviceCount() - 1).
    ///
    /// Any device memory subsequently allocated from this host thread
    /// using ::cudaMalloc(), ::cudaMallocPitch() or ::cudaMallocArray()
    /// will be physically resident on \p device.  Any host memory allocated
    /// from this host thread using ::cudaMallocHost() or ::cudaHostAlloc()
    /// or ::cudaHostRegister() will have its lifetime associated  with
    /// \p device.  Any streams or events created from this host thread will
    /// be associated with \p device.  Any kernels launched from this host
    /// thread using the <<<>>> operator or ::cudaLaunchKernel() will be executed
    /// on \p device.
    ///
    /// This call may be made from any host thread, to any device, and at
    /// any time.  This function will do no synchronization with the previous
    /// or new device, and should be considered a very low overhead call.
    ///
    /// \param device - Device on which the active host thread should execute the
    /// device code.
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevice,
    /// ::cudaErrorDeviceAlreadyInUse
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaGetDeviceProperties,
    /// ::cudaChooseDevice,
    /// ::cuCtxSetCurrent
    pub fn cudaSetDevice(device: ::std::os::raw::c_int) -> cudaError_t;
}
extern "C" {
    /// \brief Returns which device is currently being used
    ///
    /// Returns in \p *device the current device for the calling host thread.
    ///
    /// \param device - Returns the device on which the active host thread
    /// executes the device code.
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetDeviceCount, ::cudaSetDevice, ::cudaGetDeviceProperties,
    /// ::cudaChooseDevice,
    /// ::cuCtxGetCurrent
    pub fn cudaGetDevice(device: *mut ::std::os::raw::c_int) -> cudaError_t;
}
extern "C" {
    /// \brief Set a list of devices that can be used for CUDA
    ///
    /// Sets a list of devices for CUDA execution in priority order using
    /// \p device_arr. The parameter \p len specifies the number of elements in the
    /// list.  CUDA will try devices from the list sequentially until it finds one
    /// that works.  If this function is not called, or if it is called with a \p len
    /// of 0, then CUDA will go back to its default behavior of trying devices
    /// sequentially from a default list containing all of the available CUDA
    /// devices in the system. If a specified device ID in the list does not exist,
    /// this function will return ::cudaErrorInvalidDevice. If \p len is not 0 and
    /// \p device_arr is NULL or if \p len exceeds the number of devices in
    /// the system, then ::cudaErrorInvalidValue is returned.
    ///
    /// \param device_arr - List of devices to try
    /// \param len        - Number of devices in specified list
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetDeviceCount, ::cudaSetDevice, ::cudaGetDeviceProperties,
    /// ::cudaSetDeviceFlags,
    /// ::cudaChooseDevice
    pub fn cudaSetValidDevices(
        device_arr: *mut ::std::os::raw::c_int,
        len: ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Sets flags to be used for device executions
    ///
    /// Records \p flags as the flags to use when initializing the current
    /// device.  If no device has been made current to the calling thread,
    /// then \p flags will be applied to the initialization of any device
    /// initialized by the calling host thread, unless that device has had
    /// its initialization flags set explicitly by this or any host thread.
    ///
    /// If the current device has been set and that device has already been
    /// initialized then this call will fail with the error
    /// ::cudaErrorSetOnActiveProcess.  In this case it is necessary
    /// to reset \p device using ::cudaDeviceReset() before the device's
    /// initialization flags may be set.
    ///
    /// The two LSBs of the \p flags parameter can be used to control how the CPU
    /// thread interacts with the OS scheduler when waiting for results from the
    /// device.
    ///
    /// - ::cudaDeviceScheduleAuto: The default value if the \p flags parameter is
    /// zero, uses a heuristic based on the number of active CUDA contexts in the
    /// process \p C and the number of logical processors in the system \p P. If
    /// \p C \> \p P, then CUDA will yield to other OS threads when waiting for the
    /// device, otherwise CUDA will not yield while waiting for results and
    /// actively spin on the processor.
    /// - ::cudaDeviceScheduleSpin: Instruct CUDA to actively spin when waiting for
    /// results from the device. This can decrease latency when waiting for the
    /// device, but may lower the performance of CPU threads if they are performing
    /// work in parallel with the CUDA thread.
    /// - ::cudaDeviceScheduleYield: Instruct CUDA to yield its thread when waiting
    /// for results from the device. This can increase latency when waiting for the
    /// device, but can increase the performance of CPU threads performing work in
    /// parallel with the device.
    /// - ::cudaDeviceScheduleBlockingSync: Instruct CUDA to block the CPU thread
    /// on a synchronization primitive when waiting for the device to finish work.
    /// - ::cudaDeviceBlockingSync: Instruct CUDA to block the CPU thread on a
    /// synchronization primitive when waiting for the device to finish work. <br>
    /// \ref deprecated "Deprecated:" This flag was deprecated as of CUDA 4.0 and
    /// replaced with ::cudaDeviceScheduleBlockingSync.
    /// - ::cudaDeviceMapHost: This flag enables allocating pinned
    /// host memory that is accessible to the device. It is implicit for the
    /// runtime but may be absent if a context is created using the driver API.
    /// If this flag is not set, ::cudaHostGetDevicePointer() will always return
    /// a failure code.
    /// - ::cudaDeviceLmemResizeToMax: Instruct CUDA to not reduce local memory
    /// after resizing local memory for a kernel. This can prevent thrashing by
    /// local memory allocations when launching many kernels with high local
    /// memory usage at the cost of potentially increased memory usage.
    ///
    /// \param flags - Parameters for device operation
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorSetOnActiveProcess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetDeviceFlags, ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaGetDeviceProperties,
    /// ::cudaSetDevice, ::cudaSetValidDevices,
    /// ::cudaChooseDevice,
    /// ::cuDevicePrimaryCtxSetFlags
    pub fn cudaSetDeviceFlags(flags: ::std::os::raw::c_uint) -> cudaError_t;
}
extern "C" {
    /// \brief Gets the flags for the current device
    ///
    /// Returns in \p flags the flags for the current device.  If there is a
    /// current device for the calling thread, and the device has been initialized
    /// or flags have been set on that device specifically, the flags for the
    /// device are returned.  If there is no current device, but flags have been
    /// set for the thread with ::cudaSetDeviceFlags, the thread flags are returned.
    /// Finally, if there is no current device and no thread flags, the flags for
    /// the first device are returned, which may be the default flags.  Compare
    /// to the behavior of ::cudaSetDeviceFlags.
    ///
    /// Typically, the flags returned should match the behavior that will be seen
    /// if the calling thread uses a device after this call, without any change to
    /// the flags or current device inbetween by this or another thread.  Note that
    /// if the device is not initialized, it is possible for another thread to
    /// change the flags for the current device before it is initialized.
    /// Additionally, when using exclusive mode, if this thread has not requested a
    /// specific device, it may use a device other than the first device, contrary
    /// to the assumption made by this function.
    ///
    /// If a context has been created via the driver API and is current to the
    /// calling thread, the flags for that context are always returned.
    ///
    /// Flags returned by this function may specifically include ::cudaDeviceMapHost
    /// even though it is not accepted by ::cudaSetDeviceFlags because it is
    /// implicit in runtime API flags.  The reason for this is that the current
    /// context may have been created via the driver API in which case the flag is
    /// not implicit and may be unset.
    ///
    /// \param flags - Pointer to store the device flags
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetDevice, ::cudaGetDeviceProperties,
    /// ::cudaSetDevice, ::cudaSetDeviceFlags,
    /// ::cuCtxGetFlags,
    /// ::cuDevicePrimaryCtxGetState
    pub fn cudaGetDeviceFlags(flags: *mut ::std::os::raw::c_uint) -> cudaError_t;
}
extern "C" {
    /// \brief Create an asynchronous stream
    ///
    /// Creates a new asynchronous stream.
    ///
    /// \param pStream - Pointer to new stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreateWithPriority,
    /// ::cudaStreamCreateWithFlags,
    /// ::cudaStreamGetPriority,
    /// ::cudaStreamGetFlags,
    /// ::cudaStreamQuery,
    /// ::cudaStreamSynchronize,
    /// ::cudaStreamWaitEvent,
    /// ::cudaStreamAddCallback,
    /// ::cudaStreamDestroy,
    /// ::cuStreamCreate
    pub fn cudaStreamCreate(pStream: *mut cudaStream_t) -> cudaError_t;
}
extern "C" {
    /// \brief Create an asynchronous stream
    ///
    /// Creates a new asynchronous stream.  The \p flags argument determines the
    /// behaviors of the stream.  Valid values for \p flags are
    /// - ::cudaStreamDefault: Default stream creation flag.
    /// - ::cudaStreamNonBlocking: Specifies that work running in the created
    ///   stream may run concurrently with work in stream 0 (the NULL stream), and that
    ///   the created stream should perform no implicit synchronization with stream 0.
    ///
    /// \param pStream - Pointer to new stream identifier
    /// \param flags   - Parameters for stream creation
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreate,
    /// ::cudaStreamCreateWithPriority,
    /// ::cudaStreamGetFlags,
    /// ::cudaStreamQuery,
    /// ::cudaStreamSynchronize,
    /// ::cudaStreamWaitEvent,
    /// ::cudaStreamAddCallback,
    /// ::cudaStreamDestroy,
    /// ::cuStreamCreate
    pub fn cudaStreamCreateWithFlags(
        pStream: *mut cudaStream_t,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Create an asynchronous stream with the specified priority
    ///
    /// Creates a stream with the specified priority and returns a handle in \p pStream.
    /// This API alters the scheduler priority of work in the stream. Work in a higher
    /// priority stream may preempt work already executing in a low priority stream.
    ///
    /// \p priority follows a convention where lower numbers represent higher priorities.
    /// '0' represents default priority. The range of meaningful numerical priorities can
    /// be queried using ::cudaDeviceGetStreamPriorityRange. If the specified priority is
    /// outside the numerical range returned by ::cudaDeviceGetStreamPriorityRange,
    /// it will automatically be clamped to the lowest or the highest number in the range.
    ///
    /// \param pStream  - Pointer to new stream identifier
    /// \param flags    - Flags for stream creation. See ::cudaStreamCreateWithFlags for a list of valid flags that can be passed
    /// \param priority - Priority of the stream. Lower numbers represent higher priorities.
    ///                   See ::cudaDeviceGetStreamPriorityRange for more information about
    ///                   the meaningful stream priorities that can be passed.
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \note Stream priorities are supported only on GPUs
    /// with compute capability 3.5 or higher.
    ///
    /// \note In the current implementation, only compute kernels launched in
    /// priority streams are affected by the stream's priority. Stream priorities have
    /// no effect on host-to-device and device-to-host memory operations.
    ///
    /// \sa ::cudaStreamCreate,
    /// ::cudaStreamCreateWithFlags,
    /// ::cudaDeviceGetStreamPriorityRange,
    /// ::cudaStreamGetPriority,
    /// ::cudaStreamQuery,
    /// ::cudaStreamWaitEvent,
    /// ::cudaStreamAddCallback,
    /// ::cudaStreamSynchronize,
    /// ::cudaStreamDestroy,
    /// ::cuStreamCreateWithPriority
    pub fn cudaStreamCreateWithPriority(
        pStream: *mut cudaStream_t,
        flags: ::std::os::raw::c_uint,
        priority: ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Query the priority of a stream
    ///
    /// Query the priority of a stream. The priority is returned in in \p priority.
    /// Note that if the stream was created with a priority outside the meaningful
    /// numerical range returned by ::cudaDeviceGetStreamPriorityRange,
    /// this function returns the clamped priority.
    /// See ::cudaStreamCreateWithPriority for details about priority clamping.
    ///
    /// \param hStream    - Handle to the stream to be queried
    /// \param priority   - Pointer to a signed integer in which the stream's priority is returned
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreateWithPriority,
    /// ::cudaDeviceGetStreamPriorityRange,
    /// ::cudaStreamGetFlags,
    /// ::cuStreamGetPriority
    pub fn cudaStreamGetPriority(
        hStream: cudaStream_t,
        priority: *mut ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Query the flags of a stream
    ///
    /// Query the flags of a stream. The flags are returned in \p flags.
    /// See ::cudaStreamCreateWithFlags for a list of valid flags.
    ///
    /// \param hStream - Handle to the stream to be queried
    /// \param flags   - Pointer to an unsigned integer in which the stream's flags are returned
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreateWithPriority,
    /// ::cudaStreamCreateWithFlags,
    /// ::cudaStreamGetPriority,
    /// ::cuStreamGetFlags
    pub fn cudaStreamGetFlags(
        hStream: cudaStream_t,
        flags: *mut ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Destroys and cleans up an asynchronous stream
    ///
    /// Destroys and cleans up the asynchronous stream specified by \p stream.
    ///
    /// In case the device is still doing work in the stream \p stream
    /// when ::cudaStreamDestroy() is called, the function will return immediately
    /// and the resources associated with \p stream will be released automatically
    /// once the device has completed all work in \p stream.
    ///
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreate,
    /// ::cudaStreamCreateWithFlags,
    /// ::cudaStreamQuery,
    /// ::cudaStreamWaitEvent,
    /// ::cudaStreamSynchronize,
    /// ::cudaStreamAddCallback,
    /// ::cuStreamDestroy
    pub fn cudaStreamDestroy(stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /// \brief Make a compute stream wait on an event
    ///
    /// Makes all future work submitted to \p stream wait for all work captured in
    /// \p event.  See ::cudaEventRecord() for details on what is captured by an event.
    /// The synchronization will be performed efficiently on the device when applicable.
    /// \p event may be from a different device than \p stream.
    ///
    /// \param stream - Stream to wait
    /// \param event  - Event to wait on
    /// \param flags  - Parameters for the operation (must be 0)
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamQuery, ::cudaStreamSynchronize, ::cudaStreamAddCallback, ::cudaStreamDestroy,
    /// ::cuStreamWaitEvent
    pub fn cudaStreamWaitEvent(
        stream: cudaStream_t,
        event: cudaEvent_t,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
/// Type of stream callback functions.
/// \param stream The stream as passed to ::cudaStreamAddCallback, may be NULL.
/// \param status ::cudaSuccess or any persistent error on the stream.
/// \param userData User parameter provided at registration.
pub type cudaStreamCallback_t = ::std::option::Option<
    unsafe extern "C" fn(
        stream: cudaStream_t,
        status: cudaError_t,
        userData: *mut ::std::os::raw::c_void,
    ),
>;
extern "C" {
    /// \brief Add a callback to a compute stream
    ///
    /// \note This function is slated for eventual deprecation and removal. If
    /// you do not require the callback to execute in case of a device error,
    /// consider using ::cudaLaunchHostFunc. Additionally, this function is not
    /// supported with ::cudaStreamBeginCapture and ::cudaStreamEndCapture, unlike
    /// ::cudaLaunchHostFunc.
    ///
    /// Adds a callback to be called on the host after all currently enqueued
    /// items in the stream have completed.  For each
    /// cudaStreamAddCallback call, a callback will be executed exactly once.
    /// The callback will block later work in the stream until it is finished.
    ///
    /// The callback may be passed ::cudaSuccess or an error code.  In the event
    /// of a device error, all subsequently executed callbacks will receive an
    /// appropriate ::cudaError_t.
    ///
    /// Callbacks must not make any CUDA API calls.  Attempting to use CUDA APIs
    /// may result in ::cudaErrorNotPermitted.  Callbacks must not perform any
    /// synchronization that may depend on outstanding device work or other callbacks
    /// that are not mandated to run earlier.  Callbacks without a mandated order
    /// (in independent streams) execute in undefined order and may be serialized.
    ///
    /// For the purposes of Unified Memory, callback execution makes a number of
    /// guarantees:
    /// <ul>
    ///   <li>The callback stream is considered idle for the duration of the
    ///   callback.  Thus, for example, a callback may always use memory attached
    ///   to the callback stream.</li>
    ///   <li>The start of execution of a callback has the same effect as
    ///   synchronizing an event recorded in the same stream immediately prior to
    ///   the callback.  It thus synchronizes streams which have been "joined"
    ///   prior to the callback.</li>
    ///   <li>Adding device work to any stream does not have the effect of making
    ///   the stream active until all preceding callbacks have executed.  Thus, for
    ///   example, a callback might use global attached memory even if work has
    ///   been added to another stream, if it has been properly ordered with an
    ///   event.</li>
    ///   <li>Completion of a callback does not cause a stream to become
    ///   active except as described above.  The callback stream will remain idle
    ///   if no device work follows the callback, and will remain idle across
    ///   consecutive callbacks without device work in between.  Thus, for example,
    ///   stream synchronization can be done by signaling from a callback at the
    ///   end of the stream.</li>
    /// </ul>
    ///
    /// \param stream   - Stream to add callback to
    /// \param callback - The function to call once preceding stream operations are complete
    /// \param userData - User specified data to be passed to the callback function
    /// \param flags    - Reserved for future use, must be 0
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorNotSupported
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamQuery, ::cudaStreamSynchronize, ::cudaStreamWaitEvent, ::cudaStreamDestroy, ::cudaMallocManaged, ::cudaStreamAttachMemAsync,
    /// ::cudaLaunchHostFunc, ::cuStreamAddCallback
    pub fn cudaStreamAddCallback(
        stream: cudaStream_t,
        callback: cudaStreamCallback_t,
        userData: *mut ::std::os::raw::c_void,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Waits for stream tasks to complete
    ///
    /// Blocks until \p stream has completed all operations. If the
    /// ::cudaDeviceScheduleBlockingSync flag was set for this device,
    /// the host thread will block until the stream is finished with
    /// all of its tasks.
    ///
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamQuery, ::cudaStreamWaitEvent, ::cudaStreamAddCallback, ::cudaStreamDestroy,
    /// ::cuStreamSynchronize
    pub fn cudaStreamSynchronize(stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /// \brief Queries an asynchronous stream for completion status
    ///
    /// Returns ::cudaSuccess if all operations in \p stream have
    /// completed, or ::cudaErrorNotReady if not.
    ///
    /// For the purposes of Unified Memory, a return value of ::cudaSuccess
    /// is equivalent to having called ::cudaStreamSynchronize().
    ///
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorNotReady,
    /// ::cudaErrorInvalidResourceHandle
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamWaitEvent, ::cudaStreamSynchronize, ::cudaStreamAddCallback, ::cudaStreamDestroy,
    /// ::cuStreamQuery
    pub fn cudaStreamQuery(stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /// \brief Attach memory to a stream asynchronously
    ///
    /// Enqueues an operation in \p stream to specify stream association of
    /// \p length bytes of memory starting from \p devPtr. This function is a
    /// stream-ordered operation, meaning that it is dependent on, and will
    /// only take effect when, previous work in stream has completed. Any
    /// previous association is automatically replaced.
    ///
    /// \p devPtr must point to an one of the following types of memories:
    /// - managed memory declared using the __managed__ keyword or allocated with
    ///   ::cudaMallocManaged.
    /// - a valid host-accessible region of system-allocated pageable memory. This
    ///   type of memory may only be specified if the device associated with the
    ///   stream reports a non-zero value for the device attribute
    ///   ::cudaDevAttrPageableMemoryAccess.
    ///
    /// For managed allocations, \p length must be either zero or the entire
    /// allocation's size. Both indicate that the entire allocation's stream
    /// association is being changed. Currently, it is not possible to change stream
    /// association for a portion of a managed allocation.
    ///
    /// For pageable allocations, \p length must be non-zero.
    ///
    /// The stream association is specified using \p flags which must be
    /// one of ::cudaMemAttachGlobal, ::cudaMemAttachHost or ::cudaMemAttachSingle.
    /// The default value for \p flags is ::cudaMemAttachSingle
    /// If the ::cudaMemAttachGlobal flag is specified, the memory can be accessed
    /// by any stream on any device.
    /// If the ::cudaMemAttachHost flag is specified, the program makes a guarantee
    /// that it won't access the memory on the device from any stream on a device that
    /// has a zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess.
    /// If the ::cudaMemAttachSingle flag is specified and \p stream is associated with
    /// a device that has a zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess,
    /// the program makes a guarantee that it will only access the memory on the device
    /// from \p stream. It is illegal to attach singly to the NULL stream, because the
    /// NULL stream is a virtual global stream and not a specific stream. An error will
    /// be returned in this case.
    ///
    /// When memory is associated with a single stream, the Unified Memory system will
    /// allow CPU access to this memory region so long as all operations in \p stream
    /// have completed, regardless of whether other streams are active. In effect,
    /// this constrains exclusive ownership of the managed memory region by
    /// an active GPU to per-stream activity instead of whole-GPU activity.
    ///
    /// Accessing memory on the device from streams that are not associated with
    /// it will produce undefined results. No error checking is performed by the
    /// Unified Memory system to ensure that kernels launched into other streams
    /// do not access this region.
    ///
    /// It is a program's responsibility to order calls to ::cudaStreamAttachMemAsync
    /// via events, synchronization or other means to ensure legal access to memory
    /// at all times. Data visibility and coherency will be changed appropriately
    /// for all kernels which follow a stream-association change.
    ///
    /// If \p stream is destroyed while data is associated with it, the association is
    /// removed and the association reverts to the default visibility of the allocation
    /// as specified at ::cudaMallocManaged. For __managed__ variables, the default
    /// association is always ::cudaMemAttachGlobal. Note that destroying a stream is an
    /// asynchronous operation, and as a result, the change to default association won't
    /// happen until all work in the stream has completed.
    ///
    /// \param stream  - Stream in which to enqueue the attach operation
    /// \param devPtr  - Pointer to memory (must be a pointer to managed memory or
    ///                  to a valid host-accessible region of system-allocated
    ///                  memory)
    /// \param length  - Length of memory (defaults to zero)
    /// \param flags   - Must be one of ::cudaMemAttachGlobal, ::cudaMemAttachHost or ::cudaMemAttachSingle (defaults to ::cudaMemAttachSingle)
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorNotReady,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreate, ::cudaStreamCreateWithFlags, ::cudaStreamWaitEvent, ::cudaStreamSynchronize, ::cudaStreamAddCallback, ::cudaStreamDestroy, ::cudaMallocManaged,
    /// ::cuStreamAttachMemAsync
    pub fn cudaStreamAttachMemAsync(
        stream: cudaStream_t,
        devPtr: *mut ::std::os::raw::c_void,
        length: usize,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Begins graph capture on a stream
    ///
    /// Begin graph capture on \p stream. When a stream is in capture mode, all operations
    /// pushed into the stream will not be executed, but will instead be captured into
    /// a graph, which will be returned via ::cudaStreamEndCapture. Capture may not be initiated
    /// if \p stream is ::cudaStreamLegacy. Capture must be ended on the same stream in which
    /// it was initiated, and it may only be initiated if the stream is not already in capture
    /// mode. The capture mode may be queried via ::cudaStreamIsCapturing.
    ///
    /// \note Kernels captured using this API must not use texture and surface references.
    ///       Reading or writing through any texture or surface reference is undefined
    ///       behavior. This restriction does not apply to texture and surface objects.
    ///
    /// \param stream - Stream in which to initiate capture
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_capture_thread_safety
    /// \notefnerr
    ///
    /// \sa
    /// ::cudaStreamCreate,
    /// ::cudaStreamIsCapturing,
    /// ::cudaStreamEndCapture
    pub fn cudaStreamBeginCapture(stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /// \brief Ends capture on a stream, returning the captured graph
    ///
    /// End capture on \p stream, returning the captured graph via \p pGraph.
    /// Capture must have been initiated on \p stream via a call to ::cudaStreamBeginCapture.
    /// If capture was invalidated, due to a violation of the rules of stream capture, then
    /// a NULL graph will be returned.
    ///
    /// \param stream - Stream to query
    /// \param pGraph - The captured graph
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_capture_thread_safety
    /// \notefnerr
    ///
    /// \sa
    /// ::cudaStreamCreate,
    /// ::cudaStreamBeginCapture,
    /// ::cudaStreamIsCapturing
    pub fn cudaStreamEndCapture(stream: cudaStream_t, pGraph: *mut cudaGraph_t) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a stream's capture status
    ///
    /// Return the capture status of \p stream via \p pCaptureStatus. After a successful
    /// call, \p *pCaptureStatus will contain one of the following:
    /// - ::cudaStreamCaptureStatusNone: The stream is not capturing.
    /// - ::cudaStreamCaptureStatusActive: The stream is capturing.
    /// - ::cudaStreamCaptureStatusInvalidated: The stream was capturing but an error
    ///   has invalidated the capture sequence. The capture sequence must be terminated
    ///   with ::cudaStreamEndCapture on the stream where it was initiated in order to
    ///   continue using \p stream.
    ///
    /// Note that, if this is called on ::cudaStreamLegacy (the "null stream") while
    /// a blocking stream on the same device is capturing, it will return
    /// ::cudaErrorStreamCaptureImplicit and \p *pCaptureStatus is unspecified
    /// after the call. The blocking stream capture is not invalidated.
    ///
    /// When a blocking stream is capturing, the legacy stream is in an
    /// unusable state until the blocking stream capture is terminated. The legacy
    /// stream is not supported for stream capture, but attempted use would have an
    /// implicit dependency on the capturing stream(s).
    ///
    /// \param stream         - Stream to query
    /// \param pCaptureStatus - Returns the stream's capture status
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorStreamCaptureImplicit
    /// \note_capture_thread_safety
    /// \notefnerr
    ///
    /// \sa
    /// ::cudaStreamCreate,
    /// ::cudaStreamBeginCapture,
    /// ::cudaStreamEndCapture
    pub fn cudaStreamIsCapturing(
        stream: cudaStream_t,
        pCaptureStatus: *mut cudaStreamCaptureStatus,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Creates an event object
    ///
    /// Creates an event object for the current device using ::cudaEventDefault.
    ///
    /// \param event - Newly created event
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorLaunchFailure,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaEventCreate(cudaEvent_t*, unsigned int) "cudaEventCreate (C++ API)",
    /// ::cudaEventCreateWithFlags, ::cudaEventRecord, ::cudaEventQuery,
    /// ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
    /// ::cudaStreamWaitEvent,
    /// ::cuEventCreate
    pub fn cudaEventCreate(event: *mut cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /// \brief Creates an event object with the specified flags
    ///
    /// Creates an event object for the current device with the specified flags. Valid
    /// flags include:
    /// - ::cudaEventDefault: Default event creation flag.
    /// - ::cudaEventBlockingSync: Specifies that event should use blocking
    ///   synchronization. A host thread that uses ::cudaEventSynchronize() to wait
    ///   on an event created with this flag will block until the event actually
    ///   completes.
    /// - ::cudaEventDisableTiming: Specifies that the created event does not need
    ///   to record timing data.  Events created with this flag specified and
    ///   the ::cudaEventBlockingSync flag not specified will provide the best
    ///   performance when used with ::cudaStreamWaitEvent() and ::cudaEventQuery().
    /// - ::cudaEventInterprocess: Specifies that the created event may be used as an
    ///   interprocess event by ::cudaIpcGetEventHandle(). ::cudaEventInterprocess must
    ///   be specified along with ::cudaEventDisableTiming.
    ///
    /// \param event - Newly created event
    /// \param flags - Flags for new event
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorLaunchFailure,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
    /// ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
    /// ::cudaStreamWaitEvent,
    /// ::cuEventCreate
    pub fn cudaEventCreateWithFlags(
        event: *mut cudaEvent_t,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Records an event
    ///
    /// Captures in \p event the contents of \p stream at the time of this call.
    /// \p event and \p stream must be on the same device.
    /// Calls such as ::cudaEventQuery() or ::cudaStreamWaitEvent() will then
    /// examine or wait for completion of the work that was captured. Uses of
    /// \p stream after this call do not modify \p event. See note on default
    /// stream behavior for what is captured in the default case.
    ///
    /// ::cudaEventRecord() can be called multiple times on the same event and
    /// will overwrite the previously captured state. Other APIs such as
    /// ::cudaStreamWaitEvent() use the most recently captured state at the time
    /// of the API call, and are not affected by later calls to
    /// ::cudaEventRecord(). Before the first call to ::cudaEventRecord(), an
    /// event represents an empty set of work, so for example ::cudaEventQuery()
    /// would return ::cudaSuccess.
    ///
    /// \param event  - Event to record
    /// \param stream - Stream in which to record event
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorLaunchFailure
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
    /// ::cudaEventCreateWithFlags, ::cudaEventQuery,
    /// ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
    /// ::cudaStreamWaitEvent,
    /// ::cuEventRecord
    pub fn cudaEventRecord(event: cudaEvent_t, stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /// \brief Queries an event's status
    ///
    /// Queries the status of all work currently captured by \p event. See
    /// ::cudaEventRecord() for details on what is captured by an event.
    ///
    /// Returns ::cudaSuccess if all captured work has been completed, or
    /// ::cudaErrorNotReady if any captured work is incomplete.
    ///
    /// For the purposes of Unified Memory, a return value of ::cudaSuccess
    /// is equivalent to having called ::cudaEventSynchronize().
    ///
    /// \param event - Event to query
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorNotReady,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorLaunchFailure
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
    /// ::cudaEventCreateWithFlags, ::cudaEventRecord,
    /// ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventElapsedTime,
    /// ::cuEventQuery
    pub fn cudaEventQuery(event: cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /// \brief Waits for an event to complete
    ///
    /// Waits until the completion of all work currently captured in \p event.
    /// See ::cudaEventRecord() for details on what is captured by an event.
    ///
    /// Waiting for an event that was created with the ::cudaEventBlockingSync
    /// flag will cause the calling CPU thread to block until the event has
    /// been completed by the device.  If the ::cudaEventBlockingSync flag has
    /// not been set, then the CPU thread will busy-wait until the event has
    /// been completed by the device.
    ///
    /// \param event - Event to wait for
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorLaunchFailure
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
    /// ::cudaEventCreateWithFlags, ::cudaEventRecord,
    /// ::cudaEventQuery, ::cudaEventDestroy, ::cudaEventElapsedTime,
    /// ::cuEventSynchronize
    pub fn cudaEventSynchronize(event: cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /// \brief Destroys an event object
    ///
    /// Destroys the event specified by \p event.
    ///
    /// An event may be destroyed before it is complete (i.e., while
    /// ::cudaEventQuery() would return ::cudaErrorNotReady). In this case, the
    /// call does not block on completion of the event, and any associated
    /// resources will automatically be released asynchronously at completion.
    ///
    /// \param event - Event to destroy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorLaunchFailure
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
    /// ::cudaEventCreateWithFlags, ::cudaEventQuery,
    /// ::cudaEventSynchronize, ::cudaEventRecord, ::cudaEventElapsedTime,
    /// ::cuEventDestroy
    pub fn cudaEventDestroy(event: cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /// \brief Computes the elapsed time between events
    ///
    /// Computes the elapsed time between two events (in milliseconds with a
    /// resolution of around 0.5 microseconds).
    ///
    /// If either event was last recorded in a non-NULL stream, the resulting time
    /// may be greater than expected (even if both used the same stream handle). This
    /// happens because the ::cudaEventRecord() operation takes place asynchronously
    /// and there is no guarantee that the measured latency is actually just between
    /// the two events. Any number of other different stream operations could execute
    /// in between the two measured events, thus altering the timing in a significant
    /// way.
    ///
    /// If ::cudaEventRecord() has not been called on either event, then
    /// ::cudaErrorInvalidResourceHandle is returned. If ::cudaEventRecord() has been
    /// called on both events but one or both of them has not yet been completed
    /// (that is, ::cudaEventQuery() would return ::cudaErrorNotReady on at least one
    /// of the events), ::cudaErrorNotReady is returned. If either event was created
    /// with the ::cudaEventDisableTiming flag, then this function will return
    /// ::cudaErrorInvalidResourceHandle.
    ///
    /// \param ms    - Time between \p start and \p end in ms
    /// \param start - Starting event
    /// \param end   - Ending event
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorNotReady,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorLaunchFailure
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaEventCreate(cudaEvent_t*) "cudaEventCreate (C API)",
    /// ::cudaEventCreateWithFlags, ::cudaEventQuery,
    /// ::cudaEventSynchronize, ::cudaEventDestroy, ::cudaEventRecord,
    /// ::cuEventElapsedTime
    pub fn cudaEventElapsedTime(ms: *mut f32, start: cudaEvent_t, end: cudaEvent_t) -> cudaError_t;
}
extern "C" {
    /// \brief Imports an external memory object
    ///
    /// Imports an externally allocated memory object and returns
    /// a handle to that in \p extMem_out.
    ///
    /// The properties of the handle being imported must be described in
    /// \p memHandleDesc. The ::cudaExternalMemoryHandleDesc structure
    /// is defined as follows:
    ///
    /// \code
    ///typedef struct cudaExternalMemoryHandleDesc_st {
    ///cudaExternalMemoryHandleType type;
    ///union {
    ///int fd;
    ///struct {
    ///void *handle;
    ///const void *name;
    ///} win32;
    ///} handle;
    ///unsigned int flags;
    ///} cudaExternalMemoryHandleDesc;
    /// \endcode
    ///
    /// where ::cudaExternalMemoryHandleDesc::type specifies the type
    /// of handle being imported. ::cudaExternalMemoryHandleType is
    /// defined as:
    ///
    /// \code
    ///typedef enum cudaExternalMemoryHandleType_enum {
    ///cudaExternalMemoryHandleTypeOpaqueFd        = 1,
    ///cudaExternalMemoryHandleTypeOpaqueWin32     = 2,
    ///cudaExternalMemoryHandleTypeOpaqueWin32Kmt  = 3,
    ///cudaExternalMemoryHandleTypeD3D12Heap       = 4,
    ///cudaExternalMemoryHandleTypeD3D12Resource   = 5
    ///} cudaExternalMemoryHandleType;
    /// \endcode
    ///
    /// If ::cudaExternalMemoryHandleDesc::type is
    /// ::cudaExternalMemoryHandleTypeOpaqueFd, then
    /// ::cudaExternalMemoryHandleDesc::handle::fd must be a valid
    /// file descriptor referencing a memory object. Ownership of
    /// the file descriptor is transferred to the CUDA driver when the
    /// handle is imported successfully. Performing any operations on the
    /// file descriptor after it is imported results in undefined behavior.
    ///
    /// If ::cudaExternalMemoryHandleDesc::type is
    /// ::cudaExternalMemoryHandleTypeOpaqueWin32, then exactly one
    /// of ::cudaExternalMemoryHandleDesc::handle::win32::handle and
    /// ::cudaExternalMemoryHandleDesc::handle::win32::name must not be
    /// NULL. If ::cudaExternalMemoryHandleDesc::handle::win32::handle
    /// is not NULL, then it must represent a valid shared NT handle that
    /// references a memory object. Ownership of this handle is
    /// not transferred to CUDA after the import operation, so the
    /// application must release the handle using the appropriate system
    /// call. If ::cudaExternalMemoryHandleDesc::handle::win32::name
    /// is not NULL, then it must point to a NULL-terminated array of
    /// UTF-16 characters that refers to a memory object.
    ///
    /// If ::cudaExternalMemoryHandleDesc::type is
    /// ::cudaExternalMemoryHandleTypeOpaqueWin32Kmt, then
    /// ::cudaExternalMemoryHandleDesc::handle::win32::handle must
    /// be non-NULL and
    /// ::cudaExternalMemoryHandleDesc::handle::win32::name
    /// must be NULL. The handle specified must be a globally shared KMT
    /// handle. This handle does not hold a reference to the underlying
    /// object, and thus will be invalid when all references to the
    /// memory object are destroyed.
    ///
    /// If ::cudaExternalMemoryHandleDesc::type is
    /// ::cudaExternalMemoryHandleTypeD3D12Heap, then exactly one
    /// of ::cudaExternalMemoryHandleDesc::handle::win32::handle and
    /// ::cudaExternalMemoryHandleDesc::handle::win32::name must not be
    /// NULL. If ::cudaExternalMemoryHandleDesc::handle::win32::handle
    /// is not NULL, then it must represent a valid shared NT handle that
    /// is returned by ID3DDevice::CreateSharedHandle when referring to a
    /// ID3D12Heap object. This handle holds a reference to the underlying
    /// object. If ::cudaExternalMemoryHandleDesc::handle::win32::name
    /// is not NULL, then it must point to a NULL-terminated array of
    /// UTF-16 characters that refers to a ID3D12Heap object.
    ///
    /// If ::cudaExternalMemoryHandleDesc::type is
    /// ::cudaExternalMemoryHandleTypeD3D12Resource, then exactly one
    /// of ::cudaExternalMemoryHandleDesc::handle::win32::handle and
    /// ::cudaExternalMemoryHandleDesc::handle::win32::name must not be
    /// NULL. If ::cudaExternalMemoryHandleDesc::handle::win32::handle
    /// is not NULL, then it must represent a valid shared NT handle that
    /// is returned by ID3DDevice::CreateSharedHandle when referring to a
    /// ID3D12Resource object. This handle holds a reference to the
    /// underlying object. If
    /// ::cudaExternalMemoryHandleDesc::handle::win32::name
    /// is not NULL, then it must point to a NULL-terminated array of
    /// UTF-16 characters that refers to a ID3D12Resource object.
    ///
    /// Specifying the flag ::cudaExternalMemoryDedicated in
    /// ::cudaExternalMemoryHandleDesc::flags indicates that the
    /// resource is a dedicated resource. The definition of what a
    /// dedicated resource is outside the scope of this extension.
    ///
    /// \param extMem_out    - Returned handle to an external memory object
    /// \param memHandleDesc - Memory import handle descriptor
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \note If the Vulkan memory imported into CUDA is mapped on the CPU then the
    /// application must use vkInvalidateMappedMemoryRanges/vkFlushMappedMemoryRanges
    /// as well as appropriate Vulkan pipeline barriers to maintain coherence between
    /// CPU and GPU. For more information on these APIs, please refer to "Synchronization
    /// and Cache Control" chapter from Vulkan specification.
    ///
    /// \sa ::cudaDestroyExternalMemory,
    /// ::cudaExternalMemoryGetMappedBuffer,
    /// ::cudaExternalMemoryGetMappedMipmappedArray
    pub fn cudaImportExternalMemory(
        extMem_out: *mut cudaExternalMemory_t,
        memHandleDesc: *const cudaExternalMemoryHandleDesc,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Maps a buffer onto an imported memory object
    ///
    /// Maps a buffer onto an imported memory object and returns a device
    /// pointer in \p devPtr.
    ///
    /// The properties of the buffer being mapped must be described in
    /// \p bufferDesc. The ::cudaExternalMemoryBufferDesc structure is
    /// defined as follows:
    ///
    /// \code
    ///typedef struct cudaExternalMemoryBufferDesc_st {
    ///unsigned long long offset;
    ///unsigned long long size;
    ///unsigned int flags;
    ///} cudaExternalMemoryBufferDesc;
    /// \endcode
    ///
    /// where ::cudaExternalMemoryBufferDesc::offset is the offset in
    /// the memory object where the buffer's base address is.
    /// ::cudaExternalMemoryBufferDesc::size is the size of the buffer.
    /// ::cudaExternalMemoryBufferDesc::flags must be zero.
    ///
    /// The offset and size have to be suitably aligned to match the
    /// requirements of the external API. Mapping two buffers whose ranges
    /// overlap may or may not result in the same virtual address being
    /// returned for the overlapped portion. In such cases, the application
    /// must ensure that all accesses to that region from the GPU are
    /// volatile. Otherwise writes made via one address are not guaranteed
    /// to be visible via the other address, even if they're issued by the
    /// same thread. It is recommended that applications map the combined
    /// range instead of mapping separate buffers and then apply the
    /// appropriate offsets to the returned pointer to derive the
    /// individual buffers.
    ///
    /// \param devPtr     - Returned device pointer to buffer
    /// \param extMem     - Handle to external memory object
    /// \param bufferDesc - Buffer descriptor
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaImportExternalMemory
    /// ::cudaDestroyExternalMemory,
    /// ::cudaExternalMemoryGetMappedMipmappedArray
    pub fn cudaExternalMemoryGetMappedBuffer(
        devPtr: *mut *mut ::std::os::raw::c_void,
        extMem: cudaExternalMemory_t,
        bufferDesc: *const cudaExternalMemoryBufferDesc,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Maps a CUDA mipmapped array onto an external memory object
    ///
    /// Maps a CUDA mipmapped array onto an external object and returns a
    /// handle to it in \p mipmap.
    ///
    /// The properties of the CUDA mipmapped array being mapped must be
    /// described in \p mipmapDesc. The structure
    /// ::cudaExternalMemoryMipmappedArrayDesc is defined as follows:
    ///
    /// \code
    ///typedef struct cudaExternalMemoryMipmappedArrayDesc_st {
    ///unsigned long long offset;
    ///cudaChannelFormatDesc formatDesc;
    ///cudaExtent extent;
    ///unsigned int flags;
    ///CUDA_ARRAY3D_DESCRIPTOR arrayDesc;
    ///unsigned int numLevels;
    ///} cudaExternalMemoryMipmappedArrayDesc;
    /// \endcode
    ///
    /// where ::cudaExternalMemoryMipmappedArrayDesc::offset is the
    /// offset in the memory object where the base level of the mipmap
    /// chain is.
    /// ::cudaExternalMemoryMipmappedArrayDesc::formatDesc describes the
    /// format of the data.
    /// ::cudaExternalMemoryMipmappedArrayDesc::extent specifies the
    /// dimensions of the base level of the mipmap chain.
    /// ::cudaExternalMemoryMipmappedArrayDesc::flags are flags associated
    /// with CUDA mipmapped arrays. For further details, please refer to
    /// the documentation for ::cudaMalloc3DArray. Note that if the mipmapped
    /// array is bound as a color target in the graphics API, then the flag
    /// ::cudaArrayColorAttachment must be specified in
    /// ::cudaExternalMemoryMipmappedArrayDesc::flags.
    /// ::cudaExternalMemoryMipmappedArrayDesc::numLevels specifies
    /// the total number of levels in the mipmap chain.
    ///
    /// \param mipmap     - Returned CUDA mipmapped array
    /// \param extMem     - Handle to external memory object
    /// \param mipmapDesc - CUDA array descriptor
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaImportExternalMemory
    /// ::cudaDestroyExternalMemory,
    /// ::cudaExternalMemoryGetMappedBuffer
    pub fn cudaExternalMemoryGetMappedMipmappedArray(
        mipmap: *mut cudaMipmappedArray_t,
        extMem: cudaExternalMemory_t,
        mipmapDesc: *const cudaExternalMemoryMipmappedArrayDesc,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Releases all resources associated with an external memory
    /// object.
    ///
    /// Frees all buffers, CUDA arrays and CUDA mipmapped arrays that were
    /// mapped onto this external memory object and releases any reference
    /// on the underlying memory itself.
    ///
    /// \param extMem - External memory object to be destroyed
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaImportExternalMemory
    /// ::cudaExternalMemoryGetMappedBuffer,
    /// ::cudaExternalMemoryGetMappedMipmappedArray
    pub fn cudaDestroyExternalMemory(extMem: cudaExternalMemory_t) -> cudaError_t;
}
extern "C" {
    /// \brief Imports an external semaphore
    ///
    /// Imports an externally allocated synchronization object and returns
    /// a handle to that in \p extSem_out.
    ///
    /// The properties of the handle being imported must be described in
    /// \p semHandleDesc. The ::cudaExternalSemaphoreHandleDesc is defined
    /// as follows:
    ///
    /// \code
    ///typedef struct cudaExternalSemaphoreHandleDesc_st {
    ///cudaExternalSemaphoreHandleType type;
    ///union {
    ///int fd;
    ///struct {
    ///void *handle;
    ///const void *name;
    ///} win32;
    ///} handle;
    ///unsigned int flags;
    ///} cudaExternalSemaphoreHandleDesc;
    /// \endcode
    ///
    /// where ::cudaExternalSemaphoreHandleDesc::type specifies the type of
    /// handle being imported. ::cudaExternalSemaphoreHandleType is defined
    /// as:
    ///
    /// \code
    ///typedef enum cudaExternalSemaphoreHandleType_enum {
    ///cudaExternalSemaphoreHandleTypeOpaqueFd       = 1,
    ///cudaExternalSemaphoreHandleTypeOpaqueWin32    = 2,
    ///cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt = 3,
    ///cudaExternalSemaphoreHandleTypeD3D12Fence     = 4
    ///} cudaExternalSemaphoreHandleType;
    /// \endcode
    ///
    /// If ::cudaExternalSemaphoreHandleDesc::type is
    /// ::cudaExternalSemaphoreHandleTypeOpaqueFd, then
    /// ::cudaExternalSemaphoreHandleDesc::handle::fd must be a valid file
    /// descriptor referencing a synchronization object. Ownership of the
    /// file descriptor is transferred to the CUDA driver when the handle
    /// is imported successfully. Performing any operations on the file
    /// descriptor after it is imported results in undefined behavior.
    ///
    /// If ::cudaExternalSemaphoreHandleDesc::type is
    /// ::cudaExternalSemaphoreHandleTypeOpaqueWin32, then exactly one of
    /// ::cudaExternalSemaphoreHandleDesc::handle::win32::handle and
    /// ::cudaExternalSemaphoreHandleDesc::handle::win32::name must not be
    /// NULL. If ::cudaExternalSemaphoreHandleDesc::handle::win32::handle
    /// is not NULL, then it must represent a valid shared NT handle that
    /// references a synchronization object. Ownership of this handle is
    /// not transferred to CUDA after the import operation, so the
    /// application must release the handle using the appropriate system
    /// call. If ::cudaExternalSemaphoreHandleDesc::handle::win32::name is
    /// not NULL, then it must name a valid synchronization object.
    ///
    /// If ::cudaExternalSemaphoreHandleDesc::type is
    /// ::cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt, then
    /// ::cudaExternalSemaphoreHandleDesc::handle::win32::handle must be
    /// non-NULL and ::cudaExternalSemaphoreHandleDesc::handle::win32::name
    /// must be NULL. The handle specified must be a globally shared KMT
    /// handle. This handle does not hold a reference to the underlying
    /// object, and thus will be invalid when all references to the
    /// synchronization object are destroyed.
    ///
    /// If ::cudaExternalSemaphoreHandleDesc::type is
    /// ::cudaExternalSemaphoreHandleTypeD3D12Fence, then exactly one of
    /// ::cudaExternalSemaphoreHandleDesc::handle::win32::handle and
    /// ::cudaExternalSemaphoreHandleDesc::handle::win32::name must not be
    /// NULL. If ::cudaExternalSemaphoreHandleDesc::handle::win32::handle
    /// is not NULL, then it must represent a valid shared NT handle that
    /// is returned by ID3DDevice::CreateSharedHandle when referring to a
    /// ID3D12Fence object. This handle holds a reference to the underlying
    /// object. If ::cudaExternalSemaphoreHandleDesc::handle::win32::name
    /// is not NULL, then it must name a valid synchronization object that
    /// refers to a valid ID3D12Fence object.
    ///
    /// \param extSem_out    - Returned handle to an external semaphore
    /// \param semHandleDesc - Semaphore import handle descriptor
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDestroyExternalSemaphore,
    /// ::cudaSignalExternalSemaphoresAsync,
    /// ::cudaWaitExternalSemaphoresAsync
    pub fn cudaImportExternalSemaphore(
        extSem_out: *mut cudaExternalSemaphore_t,
        semHandleDesc: *const cudaExternalSemaphoreHandleDesc,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Signals a set of external semaphore objects
    ///
    /// Enqueues a signal operation on a set of externally allocated
    /// semaphore object in the specified stream. The operations will be
    /// executed when all prior operations in the stream complete.
    ///
    /// The exact semantics of signaling a semaphore depends on the type of
    /// the object.
    ///
    /// If the semaphore object is any one of the following types:
    /// ::cudaExternalSemaphoreHandleTypeOpaqueFd,
    /// ::cudaExternalSemaphoreHandleTypeOpaqueWin32,
    /// ::cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt
    /// then signaling the semaphore will set it to the signaled state.
    ///
    /// If the semaphore object is of the type
    /// ::cudaExternalSemaphoreHandleTypeD3D12Fence, then the
    /// semaphore will be set to the value specified in
    /// ::cudaExternalSemaphoreSignalParams::params::fence::value.
    ///
    /// \param extSemArray - Set of external semaphores to be signaled
    /// \param paramsArray - Array of semaphore parameters
    /// \param numExtSems  - Number of semaphores to signal
    /// \param stream     - Stream to enqueue the signal operations in
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaImportExternalSemaphore,
    /// ::cudaDestroyExternalSemaphore,
    /// ::cudaWaitExternalSemaphoresAsync
    pub fn cudaSignalExternalSemaphoresAsync(
        extSemArray: *const cudaExternalSemaphore_t,
        paramsArray: *const cudaExternalSemaphoreSignalParams,
        numExtSems: ::std::os::raw::c_uint,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Waits on a set of external semaphore objects
    ///
    /// Enqueues a wait operation on a set of externally allocated
    /// semaphore object in the specified stream. The operations will be
    /// executed when all prior operations in the stream complete.
    ///
    /// The exact semantics of waiting on a semaphore depends on the type
    /// of the object.
    ///
    /// If the semaphore object is any one of the following types:
    /// ::cudaExternalSemaphoreHandleTypeOpaqueFd,
    /// ::cudaExternalSemaphoreHandleTypeOpaqueWin32,
    /// ::cudaExternalSemaphoreHandleTypeOpaqueWin32Kmt
    /// then waiting on the semaphore will wait until the semaphore reaches
    /// the signaled state. The semaphore will then be reset to the
    /// unsignaled state. Therefore for every signal operation, there can
    /// only be one wait operation.
    ///
    /// If the semaphore object is of the type
    /// ::cudaExternalSemaphoreHandleTypeD3D12Fence, then waiting on
    /// the semaphore will wait until the value of the semaphore is
    /// greater than or equal to
    /// ::cudaExternalSemaphoreWaitParams::params::fence::value.
    ///
    /// \param extSemArray - External semaphores to be waited on
    /// \param paramsArray - Array of semaphore parameters
    /// \param numExtSems  - Number of semaphores to wait on
    /// \param stream      - Stream to enqueue the wait operations in
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaImportExternalSemaphore,
    /// ::cudaDestroyExternalSemaphore,
    /// ::cudaSignalExternalSemaphoresAsync
    pub fn cudaWaitExternalSemaphoresAsync(
        extSemArray: *const cudaExternalSemaphore_t,
        paramsArray: *const cudaExternalSemaphoreWaitParams,
        numExtSems: ::std::os::raw::c_uint,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Destroys an external semaphore
    ///
    /// Destroys an external semaphore object and releases any references
    /// to the underlying resource. Any outstanding signals or waits must
    /// have completed before the semaphore is destroyed.
    ///
    /// \param extSem - External semaphore to be destroyed
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaImportExternalSemaphore,
    /// ::cudaSignalExternalSemaphoresAsync,
    /// ::cudaWaitExternalSemaphoresAsync
    pub fn cudaDestroyExternalSemaphore(extSem: cudaExternalSemaphore_t) -> cudaError_t;
}
extern "C" {
    /// \brief Launches a device function
    ///
    /// The function invokes kernel \p func on \p gridDim (\p gridDim.x  \p gridDim.y
    ///  \p gridDim.z) grid of blocks. Each block contains \p blockDim (\p blockDim.x 
    /// \p blockDim.y  \p blockDim.z) threads.
    ///
    /// If the kernel has N parameters the \p args should point to array of N pointers.
    /// Each pointer, from <tt>args[0]</tt> to <tt>args[N - 1]</tt>, point to the region
    /// of memory from which the actual parameter will be copied.
    ///
    /// For templated functions, pass the function symbol as follows:
    /// func_name<template_arg_0,...,template_arg_N>
    ///
    /// \p sharedMem sets the amount of dynamic shared memory that will be available to
    /// each thread block.
    ///
    /// \p stream specifies a stream the invocation is associated to.
    ///
    /// \param func        - Device function symbol
    /// \param gridDim     - Grid dimentions
    /// \param blockDim    - Block dimentions
    /// \param args        - Arguments
    /// \param sharedMem   - Shared memory
    /// \param stream      - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDeviceFunction,
    /// ::cudaErrorInvalidConfiguration,
    /// ::cudaErrorLaunchFailure,
    /// ::cudaErrorLaunchTimeout,
    /// ::cudaErrorLaunchOutOfResources,
    /// ::cudaErrorSharedObjectInitFailed,
    /// ::cudaErrorInvalidPtx,
    /// ::cudaErrorNoKernelImageForDevice,
    /// ::cudaErrorJitCompilerNotFound
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// \ref ::cudaLaunchKernel(const T *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C++ API)",
    /// ::cuLaunchKernel
    pub fn cudaLaunchKernel(
        func: *const ::std::os::raw::c_void,
        gridDim: dim3,
        blockDim: dim3,
        args: *mut *mut ::std::os::raw::c_void,
        sharedMem: usize,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Launches a device function where thread blocks can cooperate and synchronize as they execute
    ///
    /// The function invokes kernel \p func on \p gridDim (\p gridDim.x  \p gridDim.y
    ///  \p gridDim.z) grid of blocks. Each block contains \p blockDim (\p blockDim.x 
    /// \p blockDim.y  \p blockDim.z) threads.
    ///
    /// The device on which this kernel is invoked must have a non-zero value for
    /// the device attribute ::cudaDevAttrCooperativeLaunch.
    ///
    /// The total number of blocks launched cannot exceed the maximum number of blocks per
    /// multiprocessor as returned by ::cudaOccupancyMaxActiveBlocksPerMultiprocessor (or
    /// ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags) times the number of multiprocessors
    /// as specified by the device attribute ::cudaDevAttrMultiProcessorCount.
    ///
    /// The kernel cannot make use of CUDA dynamic parallelism.
    ///
    /// If the kernel has N parameters the \p args should point to array of N pointers.
    /// Each pointer, from <tt>args[0]</tt> to <tt>args[N - 1]</tt>, point to the region
    /// of memory from which the actual parameter will be copied.
    ///
    /// For templated functions, pass the function symbol as follows:
    /// func_name<template_arg_0,...,template_arg_N>
    ///
    /// \p sharedMem sets the amount of dynamic shared memory that will be available to
    /// each thread block.
    ///
    /// \p stream specifies a stream the invocation is associated to.
    ///
    /// \param func        - Device function symbol
    /// \param gridDim     - Grid dimentions
    /// \param blockDim    - Block dimentions
    /// \param args        - Arguments
    /// \param sharedMem   - Shared memory
    /// \param stream      - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDeviceFunction,
    /// ::cudaErrorInvalidConfiguration,
    /// ::cudaErrorLaunchFailure,
    /// ::cudaErrorLaunchTimeout,
    /// ::cudaErrorLaunchOutOfResources,
    /// ::cudaErrorCooperativeLaunchTooLarge,
    /// ::cudaErrorSharedObjectInitFailed
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// \ref ::cudaLaunchCooperativeKernel(const T *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchCooperativeKernel (C++ API)",
    /// ::cudaLaunchCooperativeKernelMultiDevice,
    /// ::cuLaunchCooperativeKernel
    pub fn cudaLaunchCooperativeKernel(
        func: *const ::std::os::raw::c_void,
        gridDim: dim3,
        blockDim: dim3,
        args: *mut *mut ::std::os::raw::c_void,
        sharedMem: usize,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Launches device functions on multiple devices where thread blocks can cooperate and synchronize as they execute
    ///
    /// Invokes kernels as specified in the \p launchParamsList array where each element
    /// of the array specifies all the parameters required to perform a single kernel launch.
    /// These kernels can cooperate and synchronize as they execute. The size of the array is
    /// specified by \p numDevices.
    ///
    /// No two kernels can be launched on the same device. All the devices targeted by this
    /// multi-device launch must be identical. All devices must have a non-zero value for the
    /// device attribute ::cudaDevAttrCooperativeMultiDeviceLaunch.
    ///
    /// The same kernel must be launched on all devices. Note that any __device__ or __constant__
    /// variables are independently instantiated on every device. It is the application's
    /// responsiblity to ensure these variables are initialized and used appropriately.
    ///
    /// The size of the grids as specified in blocks, the size of the blocks themselves and the
    /// amount of shared memory used by each thread block must also match across all launched kernels.
    ///
    /// The streams used to launch these kernels must have been created via either ::cudaStreamCreate
    /// or ::cudaStreamCreateWithPriority or ::cudaStreamCreateWithPriority. The NULL stream or
    /// ::cudaStreamLegacy or ::cudaStreamPerThread cannot be used.
    ///
    /// The total number of blocks launched per kernel cannot exceed the maximum number of blocks
    /// per multiprocessor as returned by ::cudaOccupancyMaxActiveBlocksPerMultiprocessor (or
    /// ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags) times the number of multiprocessors
    /// as specified by the device attribute ::cudaDevAttrMultiProcessorCount. Since the
    /// total number of blocks launched per device has to match across all devices, the maximum
    /// number of blocks that can be launched per device will be limited by the device with the
    /// least number of multiprocessors.
    ///
    /// The kernel cannot make use of CUDA dynamic parallelism.
    ///
    /// The ::cudaLaunchParams structure is defined as:
    /// \code
    ///struct cudaLaunchParams
    ///{
    ///void *func;
    ///dim3 gridDim;
    ///dim3 blockDim;
    ///void **args;
    ///size_t sharedMem;
    ///cudaStream_t stream;
    ///};
    /// \endcode
    /// where:
    /// - ::cudaLaunchParams::func specifies the kernel to be launched. This same functions must
    ///   be launched on all devices. For templated functions, pass the function symbol as follows:
    ///   func_name<template_arg_0,...,template_arg_N>
    /// - ::cudaLaunchParams::gridDim specifies the width, height and depth of the grid in blocks.
    ///   This must match across all kernels launched.
    /// - ::cudaLaunchParams::blockDim is the width, height and depth of each thread block. This
    ///   must match across all kernels launched.
    /// - ::cudaLaunchParams::args specifies the arguments to the kernel. If the kernel has
    ///   N parameters then ::cudaLaunchParams::args should point to array of N pointers. Each
    ///   pointer, from <tt>::cudaLaunchParams::args[0]</tt> to <tt>::cudaLaunchParams::args[N - 1]</tt>,
    ///   point to the region of memory from which the actual parameter will be copied.
    /// - ::cudaLaunchParams::sharedMem is the dynamic shared-memory size per thread block in bytes.
    ///   This must match across all kernels launched.
    /// - ::cudaLaunchParams::stream is the handle to the stream to perform the launch in. This cannot
    ///   be the NULL stream or ::cudaStreamLegacy or ::cudaStreamPerThread.
    ///
    /// By default, the kernel won't begin execution on any GPU until all prior work in all the specified
    /// streams has completed. This behavior can be overridden by specifying the flag
    /// ::cudaCooperativeLaunchMultiDeviceNoPreSync. When this flag is specified, each kernel
    /// will only wait for prior work in the stream corresponding to that GPU to complete before it begins
    /// execution.
    ///
    /// Similarly, by default, any subsequent work pushed in any of the specified streams will not begin
    /// execution until the kernels on all GPUs have completed. This behavior can be overridden by specifying
    /// the flag ::cudaCooperativeLaunchMultiDeviceNoPostSync. When this flag is specified,
    /// any subsequent work pushed in any of the specified streams will only wait for the kernel launched
    /// on the GPU corresponding to that stream to complete before it begins execution.
    ///
    /// \param launchParamsList - List of launch parameters, one per device
    /// \param numDevices       - Size of the \p launchParamsList array
    /// \param flags            - Flags to control launch behavior
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDeviceFunction,
    /// ::cudaErrorInvalidConfiguration,
    /// ::cudaErrorLaunchFailure,
    /// ::cudaErrorLaunchTimeout,
    /// ::cudaErrorLaunchOutOfResources,
    /// ::cudaErrorCooperativeLaunchTooLarge,
    /// ::cudaErrorSharedObjectInitFailed
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// \ref ::cudaLaunchCooperativeKernel(const T *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchCooperativeKernel (C++ API)",
    /// ::cudaLaunchCooperativeKernel,
    /// ::cuLaunchCooperativeKernelMultiDevice
    pub fn cudaLaunchCooperativeKernelMultiDevice(
        launchParamsList: *mut cudaLaunchParams,
        numDevices: ::std::os::raw::c_uint,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Sets the preferred cache configuration for a device function
    ///
    /// On devices where the L1 cache and shared memory use the same hardware
    /// resources, this sets through \p cacheConfig the preferred cache configuration
    /// for the function specified via \p func. This is only a preference. The
    /// runtime will use the requested configuration if possible, but it is free to
    /// choose a different configuration if required to execute \p func.
    ///
    /// \p func is a device function symbol and must be declared as a
    /// \c __global__ function. If the specified function does not exist,
    /// then ::cudaErrorInvalidDeviceFunction is returned. For templated functions,
    /// pass the function symbol as follows: func_name<template_arg_0,...,template_arg_N>
    ///
    /// This setting does nothing on devices where the size of the L1 cache and
    /// shared memory are fixed.
    ///
    /// Launching a kernel with a different preference than the most recent
    /// preference setting may insert a device-side synchronization point.
    ///
    /// The supported cache configurations are:
    /// - ::cudaFuncCachePreferNone: no preference for shared memory or L1 (default)
    /// - ::cudaFuncCachePreferShared: prefer larger shared memory and smaller L1 cache
    /// - ::cudaFuncCachePreferL1: prefer larger L1 cache and smaller shared memory
    /// - ::cudaFuncCachePreferEqual: prefer equal size L1 cache and shared memory
    ///
    /// \param func        - Device function symbol
    /// \param cacheConfig - Requested cache configuration
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDeviceFunction
    /// \notefnerr
    /// \note_string_api_deprecation2
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaConfigureCall,
    /// \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)",
    /// \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
    /// \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
    /// ::cudaSetDoubleForDevice,
    /// ::cudaSetDoubleForHost,
    /// \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)",
    /// ::cudaThreadGetCacheConfig,
    /// ::cudaThreadSetCacheConfig,
    /// ::cuFuncSetCacheConfig
    pub fn cudaFuncSetCacheConfig(
        func: *const ::std::os::raw::c_void,
        cacheConfig: cudaFuncCache,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Sets the shared memory configuration for a device function
    ///
    /// On devices with configurable shared memory banks, this function will
    /// force all subsequent launches of the specified device function to have
    /// the given shared memory bank size configuration. On any given launch of the
    /// function, the shared memory configuration of the device will be temporarily
    /// changed if needed to suit the function's preferred configuration. Changes in
    /// shared memory configuration between subsequent launches of functions,
    /// may introduce a device side synchronization point.
    ///
    /// Any per-function setting of shared memory bank size set via
    /// ::cudaFuncSetSharedMemConfig will override the device wide setting set by
    /// ::cudaDeviceSetSharedMemConfig.
    ///
    /// Changing the shared memory bank size will not increase shared memory usage
    /// or affect occupancy of kernels, but may have major effects on performance.
    /// Larger bank sizes will allow for greater potential bandwidth to shared memory,
    /// but will change what kinds of accesses to shared memory will result in bank
    /// conflicts.
    ///
    /// This function will do nothing on devices with fixed shared memory bank size.
    ///
    /// For templated functions, pass the function symbol as follows:
    /// func_name<template_arg_0,...,template_arg_N>
    ///
    /// The supported bank configurations are:
    /// - ::cudaSharedMemBankSizeDefault: use the device's shared memory configuration
    ///   when launching this function.
    /// - ::cudaSharedMemBankSizeFourByte: set shared memory bank width to be
    ///   four bytes natively when launching this function.
    /// - ::cudaSharedMemBankSizeEightByte: set shared memory bank width to be eight
    ///   bytes natively when launching this function.
    ///
    /// \param func   - Device function symbol
    /// \param config - Requested shared memory configuration
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDeviceFunction,
    /// ::cudaErrorInvalidValue,
    /// \notefnerr
    /// \note_string_api_deprecation2
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaConfigureCall,
    /// ::cudaDeviceSetSharedMemConfig,
    /// ::cudaDeviceGetSharedMemConfig,
    /// ::cudaDeviceSetCacheConfig,
    /// ::cudaDeviceGetCacheConfig,
    /// ::cudaFuncSetCacheConfig,
    /// ::cuFuncSetSharedMemConfig
    pub fn cudaFuncSetSharedMemConfig(
        func: *const ::std::os::raw::c_void,
        config: cudaSharedMemConfig,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Find out attributes for a given function
    ///
    /// This function obtains the attributes of a function specified via \p func.
    /// \p func is a device function symbol and must be declared as a
    /// \c __global__ function. The fetched attributes are placed in \p attr.
    /// If the specified function does not exist, then
    /// ::cudaErrorInvalidDeviceFunction is returned. For templated functions, pass
    /// the function symbol as follows: func_name<template_arg_0,...,template_arg_N>
    ///
    /// Note that some function attributes such as
    /// \ref ::cudaFuncAttributes::maxThreadsPerBlock "maxThreadsPerBlock"
    /// may vary based on the device that is currently being used.
    ///
    /// \param attr - Return pointer to function's attributes
    /// \param func - Device function symbol
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDeviceFunction
    /// \notefnerr
    /// \note_string_api_deprecation2
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaConfigureCall,
    /// \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
    /// \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, T*) "cudaFuncGetAttributes (C++ API)",
    /// \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
    /// ::cudaSetDoubleForDevice,
    /// ::cudaSetDoubleForHost,
    /// \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)",
    /// ::cuFuncGetAttribute
    pub fn cudaFuncGetAttributes(
        attr: *mut cudaFuncAttributes,
        func: *const ::std::os::raw::c_void,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Set attributes for a given function
    ///
    /// This function sets the attributes of a function specified via \p func.
    /// The parameter \p func must be a pointer to a function that executes
    /// on the device. The parameter specified by \p func must be declared as a \p __global__
    /// function. The enumeration defined by \p attr is set to the value defined by \p value.
    /// If the specified function does not exist, then ::cudaErrorInvalidDeviceFunction is returned.
    /// If the specified attribute cannot be written, or if the value is incorrect,
    /// then ::cudaErrorInvalidValue is returned.
    ///
    /// Valid values for \p attr are:
    /// - ::cudaFuncAttributeMaxDynamicSharedMemorySize - Maximum size of dynamic shared memory per block
    /// - ::cudaFuncAttributePreferredSharedMemoryCarveout - Preferred shared memory-L1 cache split ratio in percent of maximum shared memory
    ///
    /// \param func  - Function to get attributes of
    /// \param attr  - Attribute to set
    /// \param value - Value to set
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDeviceFunction,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \ref ::cudaLaunchKernel(const T *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C++ API)",
    /// \ref ::cudaFuncSetCacheConfig(T*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C++ API)",
    /// \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
    /// ::cudaSetDoubleForDevice,
    /// ::cudaSetDoubleForHost,
    /// \ref ::cudaSetupArgument(T, size_t) "cudaSetupArgument (C++ API)"
    pub fn cudaFuncSetAttribute(
        func: *const ::std::os::raw::c_void,
        attr: cudaFuncAttribute,
        value: ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Converts a double argument to be executed on a device
    ///
    /// \param d - Double to convert
    ///
    /// \deprecated This function is deprecated as of CUDA 7.5
    ///
    /// Converts the double value of \p d to an internal float representation if
    /// the device does not support double arithmetic. If the device does natively
    /// support doubles, then this function does nothing.
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// \ref ::cudaLaunch(const void*) "cudaLaunch (C API)",
    /// \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
    /// \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
    /// ::cudaSetDoubleForHost,
    /// \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)"
    pub fn cudaSetDoubleForDevice(d: *mut f64) -> cudaError_t;
}
extern "C" {
    /// \brief Converts a double argument after execution on a device
    ///
    /// \deprecated This function is deprecated as of CUDA 7.5
    ///
    /// Converts the double value of \p d from a potentially internal float
    /// representation if the device does not support double arithmetic. If the
    /// device does natively support doubles, then this function does nothing.
    ///
    /// \param d - Double to convert
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// \ref ::cudaLaunch(const void*) "cudaLaunch (C API)",
    /// \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
    /// \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
    /// ::cudaSetDoubleForDevice,
    /// \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)"
    pub fn cudaSetDoubleForHost(d: *mut f64) -> cudaError_t;
}
extern "C" {
    /// \brief Enqueues a host function call in a stream
    ///
    /// Enqueues a host function to run in a stream.  The function will be called
    /// after currently enqueued work and will block work added after it.
    ///
    /// The host function must not make any CUDA API calls.  Attempting to use a
    /// CUDA API may result in ::cudaErrorNotPermitted, but this is not required.
    /// The host function must not perform any synchronization that may depend on
    /// outstanding CUDA work not mandated to run earlier.  Host functions without a
    /// mandated order (such as in independent streams) execute in undefined order
    /// and may be serialized.
    ///
    /// For the purposes of Unified Memory, execution makes a number of guarantees:
    /// <ul>
    ///   <li>The stream is considered idle for the duration of the function's
    ///   execution.  Thus, for example, the function may always use memory attached
    ///   to the stream it was enqueued in.</li>
    ///   <li>The start of execution of the function has the same effect as
    ///   synchronizing an event recorded in the same stream immediately prior to
    ///   the function.  It thus synchronizes streams which have been "joined"
    ///   prior to the function.</li>
    ///   <li>Adding device work to any stream does not have the effect of making
    ///   the stream active until all preceding host functions and stream callbacks
    ///   have executed.  Thus, for
    ///   example, a function might use global attached memory even if work has
    ///   been added to another stream, if the work has been ordered behind the
    ///   function call with an event.</li>
    ///   <li>Completion of the function does not cause a stream to become
    ///   active except as described above.  The stream will remain idle
    ///   if no device work follows the function, and will remain idle across
    ///   consecutive host functions or stream callbacks without device work in
    ///   between.  Thus, for example,
    ///   stream synchronization can be done by signaling from a host function at the
    ///   end of the stream.</li>
    /// </ul>
    ///
    /// Note that, in constrast to ::cuStreamAddCallback, the function will not be
    /// called in the event of an error in the CUDA context.
    ///
    /// \param hStream  - Stream to enqueue function call in
    /// \param fn       - The function to call once preceding stream operations are complete
    /// \param userData - User-specified data to be passed to the function
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorNotSupported
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaStreamCreate,
    /// ::cudaStreamQuery,
    /// ::cudaStreamSynchronize,
    /// ::cudaStreamWaitEvent,
    /// ::cudaStreamDestroy,
    /// ::cudaMallocManaged,
    /// ::cudaStreamAttachMemAsync,
    /// ::cudaStreamAddCallback,
    /// ::cuLaunchHostFunc
    pub fn cudaLaunchHostFunc(
        stream: cudaStream_t,
        fn_: cudaHostFn_t,
        userData: *mut ::std::os::raw::c_void,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns occupancy for a device function
    ///
    /// Returns in \p *numBlocks the maximum number of active blocks per
    /// streaming multiprocessor for the device function.
    ///
    /// \param numBlocks       - Returned occupancy
    /// \param func            - Kernel function for which occupancy is calculated
    /// \param blockSize       - Block size the kernel is intended to be launched with
    /// \param dynamicSMemSize - Per-block dynamic shared memory usage intended, in bytes
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevice,
    /// ::cudaErrorInvalidDeviceFunction,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorUnknown,
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags,
    /// \ref ::cudaOccupancyMaxPotentialBlockSize(int*, int*, T, size_t, int) "cudaOccupancyMaxPotentialBlockSize (C++ API)",
    /// \ref ::cudaOccupancyMaxPotentialBlockSizeWithFlags(int*, int*, T, size_t, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeWithFlags (C++ API)",
    /// \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMem(int*, int*, T, UnaryFunction, int) "cudaOccupancyMaxPotentialBlockSizeVariableSMem (C++ API)",
    /// \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags(int*, int*, T, UnaryFunction, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags (C++ API)",
    /// ::cuOccupancyMaxActiveBlocksPerMultiprocessor
    pub fn cudaOccupancyMaxActiveBlocksPerMultiprocessor(
        numBlocks: *mut ::std::os::raw::c_int,
        func: *const ::std::os::raw::c_void,
        blockSize: ::std::os::raw::c_int,
        dynamicSMemSize: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns occupancy for a device function with the specified flags
    ///
    /// Returns in \p *numBlocks the maximum number of active blocks per
    /// streaming multiprocessor for the device function.
    ///
    /// The \p flags parameter controls how special cases are handled. Valid flags include:
    ///
    /// - ::cudaOccupancyDefault: keeps the default behavior as
    ///   ::cudaOccupancyMaxActiveBlocksPerMultiprocessor
    ///
    /// - ::cudaOccupancyDisableCachingOverride: This flag suppresses the default behavior
    ///   on platform where global caching affects occupancy. On such platforms, if caching
    ///   is enabled, but per-block SM resource usage would result in zero occupancy, the
    ///   occupancy calculator will calculate the occupancy as if caching is disabled.
    ///   Setting this flag makes the occupancy calculator to return 0 in such cases.
    ///   More information can be found about this feature in the "Unified L1/Texture Cache"
    ///   section of the Maxwell tuning guide.
    ///
    /// \param numBlocks       - Returned occupancy
    /// \param func            - Kernel function for which occupancy is calculated
    /// \param blockSize       - Block size the kernel is intended to be launched with
    /// \param dynamicSMemSize - Per-block dynamic shared memory usage intended, in bytes
    /// \param flags           - Requested behavior for the occupancy calculator
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevice,
    /// ::cudaErrorInvalidDeviceFunction,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorUnknown,
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaOccupancyMaxActiveBlocksPerMultiprocessor,
    /// \ref ::cudaOccupancyMaxPotentialBlockSize(int*, int*, T, size_t, int) "cudaOccupancyMaxPotentialBlockSize (C++ API)",
    /// \ref ::cudaOccupancyMaxPotentialBlockSizeWithFlags(int*, int*, T, size_t, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeWithFlags (C++ API)",
    /// \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMem(int*, int*, T, UnaryFunction, int) "cudaOccupancyMaxPotentialBlockSizeVariableSMem (C++ API)",
    /// \ref ::cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags(int*, int*, T, UnaryFunction, int, unsigned int) "cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags (C++ API)",
    /// ::cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags
    pub fn cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(
        numBlocks: *mut ::std::os::raw::c_int,
        func: *const ::std::os::raw::c_void,
        blockSize: ::std::os::raw::c_int,
        dynamicSMemSize: usize,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Configure a device-launch
    ///
    /// \deprecated This function is deprecated as of CUDA 7.0
    ///
    /// Specifies the grid and block dimensions for the device call to be executed
    /// similar to the execution configuration syntax. ::cudaConfigureCall() is
    /// stack based. Each call pushes data on top of an execution stack. This data
    /// contains the dimension for the grid and thread blocks, together with any
    /// arguments for the call.
    ///
    /// \param gridDim   - Grid dimensions
    /// \param blockDim  - Block dimensions
    /// \param sharedMem - Shared memory
    /// \param stream    - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidConfiguration
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
    /// \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
    /// \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
    /// \ref ::cudaLaunch(const void*) "cudaLaunch (C API)",
    /// ::cudaSetDoubleForDevice,
    /// ::cudaSetDoubleForHost,
    /// \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)",
    pub fn cudaConfigureCall(
        gridDim: dim3,
        blockDim: dim3,
        sharedMem: usize,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Configure a device launch
    ///
    /// \deprecated This function is deprecated as of CUDA 7.0
    ///
    /// Pushes \p size bytes of the argument pointed to by \p arg at \p offset
    /// bytes from the start of the parameter passing area, which starts at
    /// offset 0. The arguments are stored in the top of the execution stack.
    /// \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument()"
    /// must be preceded by a call to ::cudaConfigureCall().
    ///
    /// \param arg    - Argument to push for a kernel launch
    /// \param size   - Size of argument
    /// \param offset - Offset in argument stack to push new arg
    ///
    /// \return
    /// ::cudaSuccess
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
    /// \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
    /// \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
    /// \ref ::cudaLaunch(const void*) "cudaLaunch (C API)",
    /// ::cudaSetDoubleForDevice,
    /// ::cudaSetDoubleForHost,
    /// \ref ::cudaSetupArgument(T, size_t) "cudaSetupArgument (C++ API)",
    pub fn cudaSetupArgument(
        arg: *const ::std::os::raw::c_void,
        size: usize,
        offset: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Launches a device function
    ///
    /// \deprecated This function is deprecated as of CUDA 7.0
    ///
    /// Launches the function \p func on the device. The parameter \p func must
    /// be a device function symbol. The parameter specified by \p func must be
    /// declared as a \p __global__ function. For templated functions, pass the
    /// function symbol as follows: func_name<template_arg_0,...,template_arg_N>
    /// \ref ::cudaLaunch(const void*) "cudaLaunch()" must be preceded by a call to
    /// ::cudaConfigureCall() since it pops the data that was pushed by
    /// ::cudaConfigureCall() from the execution stack.
    ///
    /// \param func - Device function symbol
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDeviceFunction,
    /// ::cudaErrorInvalidConfiguration,
    /// ::cudaErrorLaunchFailure,
    /// ::cudaErrorLaunchTimeout,
    /// ::cudaErrorLaunchOutOfResources,
    /// ::cudaErrorSharedObjectInitFailed,
    /// ::cudaErrorInvalidPtx,
    /// ::cudaErrorNoKernelImageForDevice,
    /// ::cudaErrorJitCompilerNotFound
    /// \notefnerr
    /// \note_string_api_deprecation_50
    /// \note_init_rt
    /// \note_callback
    ///
    /// \ref ::cudaLaunchKernel(const void *func, dim3 gridDim, dim3 blockDim, void **args, size_t sharedMem, cudaStream_t stream) "cudaLaunchKernel (C API)",
    /// \ref ::cudaFuncSetCacheConfig(const void*, enum cudaFuncCache) "cudaFuncSetCacheConfig (C API)",
    /// \ref ::cudaFuncGetAttributes(struct cudaFuncAttributes*, const void*) "cudaFuncGetAttributes (C API)",
    /// \ref ::cudaLaunch(T*) "cudaLaunch (C++ API)",
    /// ::cudaSetDoubleForDevice,
    /// ::cudaSetDoubleForHost,
    /// \ref ::cudaSetupArgument(const void*, size_t, size_t) "cudaSetupArgument (C API)",
    /// ::cudaThreadGetCacheConfig,
    /// ::cudaThreadSetCacheConfig
    pub fn cudaLaunch(func: *const ::std::os::raw::c_void) -> cudaError_t;
}
extern "C" {
    /// \brief Allocates memory that will be automatically managed by the Unified Memory system
    ///
    /// Allocates \p size bytes of managed memory on the device and returns in
    /// \p *devPtr a pointer to the allocated memory. If the device doesn't support
    /// allocating managed memory, ::cudaErrorNotSupported is returned. Support
    /// for managed memory can be queried using the device attribute
    /// ::cudaDevAttrManagedMemory. The allocated memory is suitably
    /// aligned for any kind of variable. The memory is not cleared. If \p size
    /// is 0, ::cudaMallocManaged returns ::cudaErrorInvalidValue. The pointer
    /// is valid on the CPU and on all GPUs in the system that support managed memory.
    /// All accesses to this pointer must obey the Unified Memory programming model.
    ///
    /// \p flags specifies the default stream association for this allocation.
    /// \p flags must be one of ::cudaMemAttachGlobal or ::cudaMemAttachHost. The
    /// default value for \p flags is ::cudaMemAttachGlobal.
    /// If ::cudaMemAttachGlobal is specified, then this memory is accessible from
    /// any stream on any device. If ::cudaMemAttachHost is specified, then the
    /// allocation should not be accessed from devices that have a zero value for the
    /// device attribute ::cudaDevAttrConcurrentManagedAccess; an explicit call to
    /// ::cudaStreamAttachMemAsync will be required to enable access on such devices.
    ///
    /// If the association is later changed via ::cudaStreamAttachMemAsync to
    /// a single stream, the default association, as specifed during ::cudaMallocManaged,
    /// is restored when that stream is destroyed. For __managed__ variables, the
    /// default association is always ::cudaMemAttachGlobal. Note that destroying a
    /// stream is an asynchronous operation, and as a result, the change to default
    /// association won't happen until all work in the stream has completed.
    ///
    /// Memory allocated with ::cudaMallocManaged should be released with ::cudaFree.
    ///
    /// Device memory oversubscription is possible for GPUs that have a non-zero value for the
    /// device attribute ::cudaDevAttrConcurrentManagedAccess. Managed memory on
    /// such GPUs may be evicted from device memory to host memory at any time by the Unified
    /// Memory driver in order to make room for other allocations.
    ///
    /// In a multi-GPU system where all GPUs have a non-zero value for the device attribute
    /// ::cudaDevAttrConcurrentManagedAccess, managed memory may not be populated when this
    /// API returns and instead may be populated on access. In such systems, managed memory can
    /// migrate to any processor's memory at any time. The Unified Memory driver will employ heuristics to
    /// maintain data locality and prevent excessive page faults to the extent possible. The application
    /// can also guide the driver about memory usage patterns via ::cudaMemAdvise. The application
    /// can also explicitly migrate memory to a desired processor's memory via
    /// ::cudaMemPrefetchAsync.
    ///
    /// In a multi-GPU system where all of the GPUs have a zero value for the device attribute
    /// ::cudaDevAttrConcurrentManagedAccess and all the GPUs have peer-to-peer support
    /// with each other, the physical storage for managed memory is created on the GPU which is active
    /// at the time ::cudaMallocManaged is called. All other GPUs will reference the data at reduced
    /// bandwidth via peer mappings over the PCIe bus. The Unified Memory driver does not migrate
    /// memory among such GPUs.
    ///
    /// In a multi-GPU system where not all GPUs have peer-to-peer support with each other and
    /// where the value of the device attribute ::cudaDevAttrConcurrentManagedAccess
    /// is zero for at least one of those GPUs, the location chosen for physical storage of managed
    /// memory is system-dependent.
    /// - On Linux, the location chosen will be device memory as long as the current set of active
    /// contexts are on devices that either have peer-to-peer support with each other or have a
    /// non-zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess.
    /// If there is an active context on a GPU that does not have a non-zero value for that device
    /// attribute and it does not have peer-to-peer support with the other devices that have active
    /// contexts on them, then the location for physical storage will be 'zero-copy' or host memory.
    /// Note that this means that managed memory that is located in device memory is migrated to
    /// host memory if a new context is created on a GPU that doesn't have a non-zero value for
    /// the device attribute and does not support peer-to-peer with at least one of the other devices
    /// that has an active context. This in turn implies that context creation may fail if there is
    /// insufficient host memory to migrate all managed allocations.
    /// - On Windows, the physical storage is always created in 'zero-copy' or host memory.
    /// All GPUs will reference the data at reduced bandwidth over the PCIe bus. In these
    /// circumstances, use of the environment variable CUDA_VISIBLE_DEVICES is recommended to
    /// restrict CUDA to only use those GPUs that have peer-to-peer support.
    /// Alternatively, users can also set CUDA_MANAGED_FORCE_DEVICE_ALLOC to a non-zero
    /// value to force the driver to always use device memory for physical storage.
    /// When this environment variable is set to a non-zero value, all devices used in
    /// that process that support managed memory have to be peer-to-peer compatible
    /// with each other. The error ::cudaErrorInvalidDevice will be returned if a device
    /// that supports managed memory is used and it is not peer-to-peer compatible with
    /// any of the other managed memory supporting devices that were previously used in
    /// that process, even if ::cudaDeviceReset has been called on those devices. These
    /// environment variables are described in the CUDA programming guide under the
    /// "CUDA environment variables" section.
    ///
    /// \param devPtr - Pointer to allocated device memory
    /// \param size   - Requested allocation size in bytes
    /// \param flags  - Must be either ::cudaMemAttachGlobal or ::cudaMemAttachHost (defaults to ::cudaMemAttachGlobal)
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorMemoryAllocation,
    /// ::cudaErrorNotSupported,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray, ::cudaFreeArray,
    /// ::cudaMalloc3D, ::cudaMalloc3DArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaHostAlloc, ::cudaDeviceGetAttribute, ::cudaStreamAttachMemAsync,
    /// ::cuMemAllocManaged
    pub fn cudaMallocManaged(
        devPtr: *mut *mut ::std::os::raw::c_void,
        size: usize,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Allocate memory on the device
    ///
    /// Allocates \p size bytes of linear memory on the device and returns in
    /// \p *devPtr a pointer to the allocated memory. The allocated memory is
    /// suitably aligned for any kind of variable. The memory is not cleared.
    /// ::cudaMalloc() returns ::cudaErrorMemoryAllocation in case of failure.
    ///
    /// The device version of ::cudaFree cannot be used with a \p *devPtr
    /// allocated using the host API, and vice versa.
    ///
    /// \param devPtr - Pointer to allocated device memory
    /// \param size   - Requested allocation size in bytes
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray, ::cudaFreeArray,
    /// ::cudaMalloc3D, ::cudaMalloc3DArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaHostAlloc,
    /// ::cuMemAlloc
    pub fn cudaMalloc(devPtr: *mut *mut ::std::os::raw::c_void, size: usize) -> cudaError_t;
}
extern "C" {
    /// \brief Allocates page-locked memory on the host
    ///
    /// Allocates \p size bytes of host memory that is page-locked and accessible
    /// to the device. The driver tracks the virtual memory ranges allocated with
    /// this function and automatically accelerates calls to functions such as
    /// ::cudaMemcpy*(). Since the memory can be accessed directly by the device,
    /// it can be read or written with much higher bandwidth than pageable memory
    /// obtained with functions such as ::malloc(). Allocating excessive amounts of
    /// memory with ::cudaMallocHost() may degrade system performance, since it
    /// reduces the amount of memory available to the system for paging. As a
    /// result, this function is best used sparingly to allocate staging areas for
    /// data exchange between host and device.
    ///
    /// \param ptr  - Pointer to allocated host memory
    /// \param size - Requested allocation size in bytes
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaMallocArray, ::cudaMalloc3D,
    /// ::cudaMalloc3DArray, ::cudaHostAlloc, ::cudaFree, ::cudaFreeArray,
    /// \ref ::cudaMallocHost(void**, size_t, unsigned int) "cudaMallocHost (C++ API)",
    /// ::cudaFreeHost, ::cudaHostAlloc,
    /// ::cuMemAllocHost
    pub fn cudaMallocHost(ptr: *mut *mut ::std::os::raw::c_void, size: usize) -> cudaError_t;
}
extern "C" {
    /// \brief Allocates pitched memory on the device
    ///
    /// Allocates at least \p width (in bytes) * \p height bytes of linear memory
    /// on the device and returns in \p *devPtr a pointer to the allocated memory.
    /// The function may pad the allocation to ensure that corresponding pointers
    /// in any given row will continue to meet the alignment requirements for
    /// coalescing as the address is updated from row to row. The pitch returned in
    /// \p *pitch by ::cudaMallocPitch() is the width in bytes of the allocation.
    /// The intended usage of \p pitch is as a separate parameter of the allocation,
    /// used to compute addresses within the 2D array. Given the row and column of
    /// an array element of type \p T, the address is computed as:
    /// \code
    ///T* pElement = (T*)((char*)BaseAddress + Row * pitch) + Column;
    ///\endcode
    ///
    /// For allocations of 2D arrays, it is recommended that programmers consider
    /// performing pitch allocations using ::cudaMallocPitch(). Due to pitch
    /// alignment restrictions in the hardware, this is especially true if the
    /// application will be performing 2D memory copies between different regions
    /// of device memory (whether linear memory or CUDA arrays).
    ///
    /// \param devPtr - Pointer to allocated pitched device memory
    /// \param pitch  - Pitch for allocation
    /// \param width  - Requested pitched allocation width (in bytes)
    /// \param height - Requested pitched allocation height
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc, ::cudaFree, ::cudaMallocArray, ::cudaFreeArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaMalloc3D, ::cudaMalloc3DArray,
    /// ::cudaHostAlloc,
    /// ::cuMemAllocPitch
    pub fn cudaMallocPitch(
        devPtr: *mut *mut ::std::os::raw::c_void,
        pitch: *mut usize,
        width: usize,
        height: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Allocate an array on the device
    ///
    /// Allocates a CUDA array according to the ::cudaChannelFormatDesc structure
    /// \p desc and returns a handle to the new CUDA array in \p *array.
    ///
    /// The ::cudaChannelFormatDesc is defined as:
    /// \code
    ///struct cudaChannelFormatDesc {
    ///int x, y, z, w;
    ///enum cudaChannelFormatKind f;
    ///};
    ///\endcode
    /// where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
    /// ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
    ///
    /// The \p flags parameter enables different options to be specified that affect
    /// the allocation, as follows.
    /// - ::cudaArrayDefault: This flag's value is defined to be 0 and provides default array allocation
    /// - ::cudaArraySurfaceLoadStore: Allocates an array that can be read from or written to using a surface reference
    /// - ::cudaArrayTextureGather: This flag indicates that texture gather operations will be performed on the array.
    ///
    /// \p width and \p height must meet certain size requirements. See ::cudaMalloc3DArray() for more details.
    ///
    /// \param array  - Pointer to allocated array in device memory
    /// \param desc   - Requested channel format
    /// \param width  - Requested array allocation width
    /// \param height - Requested array allocation height
    /// \param flags  - Requested properties of allocated array
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaFreeArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaMalloc3D, ::cudaMalloc3DArray,
    /// ::cudaHostAlloc,
    /// ::cuArrayCreate
    pub fn cudaMallocArray(
        array: *mut cudaArray_t,
        desc: *const cudaChannelFormatDesc,
        width: usize,
        height: usize,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Frees memory on the device
    ///
    /// Frees the memory space pointed to by \p devPtr, which must have been
    /// returned by a previous call to ::cudaMalloc() or ::cudaMallocPitch().
    /// Otherwise, or if ::cudaFree(\p devPtr) has already been called before,
    /// an error is returned. If \p devPtr is 0, no operation is performed.
    /// ::cudaFree() returns ::cudaErrorInvalidDevicePointer in case of failure.
    ///
    /// The device version of ::cudaFree cannot be used with a \p *devPtr
    /// allocated using the host API, and vice versa.
    ///
    /// \param devPtr - Device pointer to memory to free
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevicePointer
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaMallocArray, ::cudaFreeArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaMalloc3D, ::cudaMalloc3DArray,
    /// ::cudaHostAlloc,
    /// ::cuMemFree
    pub fn cudaFree(devPtr: *mut ::std::os::raw::c_void) -> cudaError_t;
}
extern "C" {
    /// \brief Frees page-locked memory
    ///
    /// Frees the memory space pointed to by \p hostPtr, which must have been
    /// returned by a previous call to ::cudaMallocHost() or ::cudaHostAlloc().
    ///
    /// \param ptr - Pointer to memory to free
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray,
    /// ::cudaFreeArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaMalloc3D, ::cudaMalloc3DArray, ::cudaHostAlloc,
    /// ::cuMemFreeHost
    pub fn cudaFreeHost(ptr: *mut ::std::os::raw::c_void) -> cudaError_t;
}
extern "C" {
    /// \brief Frees an array on the device
    ///
    /// Frees the CUDA array \p array, which must have been returned by a
    /// previous call to ::cudaMallocArray(). If \p devPtr is 0,
    /// no operation is performed.
    ///
    /// \param array - Pointer to array to free
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaHostAlloc,
    /// ::cuArrayDestroy
    pub fn cudaFreeArray(array: cudaArray_t) -> cudaError_t;
}
extern "C" {
    /// \brief Frees a mipmapped array on the device
    ///
    /// Frees the CUDA mipmapped array \p mipmappedArray, which must have been
    /// returned by a previous call to ::cudaMallocMipmappedArray(). If \p devPtr
    /// is 0, no operation is performed.
    ///
    /// \param mipmappedArray - Pointer to mipmapped array to free
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc, ::cudaMallocPitch, ::cudaFree, ::cudaMallocArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaHostAlloc,
    /// ::cuMipmappedArrayDestroy
    pub fn cudaFreeMipmappedArray(mipmappedArray: cudaMipmappedArray_t) -> cudaError_t;
}
extern "C" {
    /// \brief Allocates page-locked memory on the host
    ///
    /// Allocates \p size bytes of host memory that is page-locked and accessible
    /// to the device. The driver tracks the virtual memory ranges allocated with
    /// this function and automatically accelerates calls to functions such as
    /// ::cudaMemcpy(). Since the memory can be accessed directly by the device, it
    /// can be read or written with much higher bandwidth than pageable memory
    /// obtained with functions such as ::malloc(). Allocating excessive amounts of
    /// pinned memory may degrade system performance, since it reduces the amount
    /// of memory available to the system for paging. As a result, this function is
    /// best used sparingly to allocate staging areas for data exchange between host
    /// and device.
    ///
    /// The \p flags parameter enables different options to be specified that affect
    /// the allocation, as follows.
    /// - ::cudaHostAllocDefault: This flag's value is defined to be 0 and causes
    /// ::cudaHostAlloc() to emulate ::cudaMallocHost().
    /// - ::cudaHostAllocPortable: The memory returned by this call will be
    /// considered as pinned memory by all CUDA contexts, not just the one that
    /// performed the allocation.
    /// - ::cudaHostAllocMapped: Maps the allocation into the CUDA address space.
    /// The device pointer to the memory may be obtained by calling
    /// ::cudaHostGetDevicePointer().
    /// - ::cudaHostAllocWriteCombined: Allocates the memory as write-combined (WC).
    /// WC memory can be transferred across the PCI Express bus more quickly on some
    /// system configurations, but cannot be read efficiently by most CPUs.  WC
    /// memory is a good option for buffers that will be written by the CPU and read
    /// by the device via mapped pinned memory or host->device transfers.
    ///
    /// All of these flags are orthogonal to one another: a developer may allocate
    /// memory that is portable, mapped and/or write-combined with no restrictions.
    ///
    /// In order for the ::cudaHostAllocMapped flag to have any effect, the CUDA context
    /// must support the ::cudaDeviceMapHost flag, which can be checked via
    /// ::cudaGetDeviceFlags(). The ::cudaDeviceMapHost flag is implicitly set for
    /// contexts created via the runtime API.
    ///
    /// The ::cudaHostAllocMapped flag may be specified on CUDA contexts for devices
    /// that do not support mapped pinned memory. The failure is deferred to
    /// ::cudaHostGetDevicePointer() because the memory may be mapped into other
    /// CUDA contexts via the ::cudaHostAllocPortable flag.
    ///
    /// Memory allocated by this function must be freed with ::cudaFreeHost().
    ///
    /// \param pHost - Device pointer to allocated memory
    /// \param size  - Requested allocation size in bytes
    /// \param flags - Requested properties of allocated memory
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaSetDeviceFlags,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost,
    /// ::cudaGetDeviceFlags,
    /// ::cuMemHostAlloc
    pub fn cudaHostAlloc(
        pHost: *mut *mut ::std::os::raw::c_void,
        size: usize,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Registers an existing host memory range for use by CUDA
    ///
    /// Page-locks the memory range specified by \p ptr and \p size and maps it
    /// for the device(s) as specified by \p flags. This memory range also is added
    /// to the same tracking mechanism as ::cudaHostAlloc() to automatically accelerate
    /// calls to functions such as ::cudaMemcpy(). Since the memory can be accessed
    /// directly by the device, it can be read or written with much higher bandwidth
    /// than pageable memory that has not been registered.  Page-locking excessive
    /// amounts of memory may degrade system performance, since it reduces the amount
    /// of memory available to the system for paging. As a result, this function is
    /// best used sparingly to register staging areas for data exchange between
    /// host and device.
    ///
    /// ::cudaHostRegister is supported only on I/O coherent devices that have a non-zero
    /// value for the device attribute ::cudaDevAttrHostRegisterSupported.
    ///
    /// The \p flags parameter enables different options to be specified that
    /// affect the allocation, as follows.
    ///
    /// - ::cudaHostRegisterDefault: On a system with unified virtual addressing,
    ///   the memory will be both mapped and portable.  On a system with no unified
    ///   virtual addressing, the memory will be neither mapped nor portable.
    ///
    /// - ::cudaHostRegisterPortable: The memory returned by this call will be
    ///   considered as pinned memory by all CUDA contexts, not just the one that
    ///   performed the allocation.
    ///
    /// - ::cudaHostRegisterMapped: Maps the allocation into the CUDA address
    ///   space. The device pointer to the memory may be obtained by calling
    ///   ::cudaHostGetDevicePointer().
    ///
    /// - ::cudaHostRegisterIoMemory: The passed memory pointer is treated as
    ///   pointing to some memory-mapped I/O space, e.g. belonging to a
    ///   third-party PCIe device, and it will marked as non cache-coherent and
    ///   contiguous.
    ///
    /// All of these flags are orthogonal to one another: a developer may page-lock
    /// memory that is portable or mapped with no restrictions.
    ///
    /// The CUDA context must have been created with the ::cudaMapHost flag in
    /// order for the ::cudaHostRegisterMapped flag to have any effect.
    ///
    /// The ::cudaHostRegisterMapped flag may be specified on CUDA contexts for
    /// devices that do not support mapped pinned memory. The failure is deferred
    /// to ::cudaHostGetDevicePointer() because the memory may be mapped into
    /// other CUDA contexts via the ::cudaHostRegisterPortable flag.
    ///
    /// For devices that have a non-zero value for the device attribute
    /// ::cudaDevAttrCanUseHostPointerForRegisteredMem, the memory
    /// can also be accessed from the device using the host pointer \p ptr.
    /// The device pointer returned by ::cudaHostGetDevicePointer() may or may not
    /// match the original host pointer \p ptr and depends on the devices visible to the
    /// application. If all devices visible to the application have a non-zero value for the
    /// device attribute, the device pointer returned by ::cudaHostGetDevicePointer()
    /// will match the original pointer \p ptr. If any device visible to the application
    /// has a zero value for the device attribute, the device pointer returned by
    /// ::cudaHostGetDevicePointer() will not match the original host pointer \p ptr,
    /// but it will be suitable for use on all devices provided Unified Virtual Addressing
    /// is enabled. In such systems, it is valid to access the memory using either pointer
    /// on devices that have a non-zero value for the device attribute. Note however that
    /// such devices should access the memory using only of the two pointers and not both.
    ///
    /// The memory page-locked by this function must be unregistered with ::cudaHostUnregister().
    ///
    /// \param ptr   - Host pointer to memory to page-lock
    /// \param size  - Size in bytes of the address range to page-lock in bytes
    /// \param flags - Flags for allocation request
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation,
    /// ::cudaErrorHostMemoryAlreadyRegistered,
    /// ::cudaErrorNotSupported
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaHostUnregister, ::cudaHostGetFlags, ::cudaHostGetDevicePointer,
    /// ::cuMemHostRegister
    pub fn cudaHostRegister(
        ptr: *mut ::std::os::raw::c_void,
        size: usize,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Unregisters a memory range that was registered with cudaHostRegister
    ///
    /// Unmaps the memory range whose base address is specified by \p ptr, and makes
    /// it pageable again.
    ///
    /// The base address must be the same one specified to ::cudaHostRegister().
    ///
    /// \param ptr - Host pointer to memory to unregister
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorHostMemoryNotRegistered
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaHostUnregister,
    /// ::cuMemHostUnregister
    pub fn cudaHostUnregister(ptr: *mut ::std::os::raw::c_void) -> cudaError_t;
}
extern "C" {
    /// \brief Passes back device pointer of mapped host memory allocated by
    /// cudaHostAlloc or registered by cudaHostRegister
    ///
    /// Passes back the device pointer corresponding to the mapped, pinned host
    /// buffer allocated by ::cudaHostAlloc() or registered by ::cudaHostRegister().
    ///
    /// ::cudaHostGetDevicePointer() will fail if the ::cudaDeviceMapHost flag was
    /// not specified before deferred context creation occurred, or if called on a
    /// device that does not support mapped, pinned memory.
    ///
    /// For devices that have a non-zero value for the device attribute
    /// ::cudaDevAttrCanUseHostPointerForRegisteredMem, the memory
    /// can also be accessed from the device using the host pointer \p pHost.
    /// The device pointer returned by ::cudaHostGetDevicePointer() may or may not
    /// match the original host pointer \p pHost and depends on the devices visible to the
    /// application. If all devices visible to the application have a non-zero value for the
    /// device attribute, the device pointer returned by ::cudaHostGetDevicePointer()
    /// will match the original pointer \p pHost. If any device visible to the application
    /// has a zero value for the device attribute, the device pointer returned by
    /// ::cudaHostGetDevicePointer() will not match the original host pointer \p pHost,
    /// but it will be suitable for use on all devices provided Unified Virtual Addressing
    /// is enabled. In such systems, it is valid to access the memory using either pointer
    /// on devices that have a non-zero value for the device attribute. Note however that
    /// such devices should access the memory using only of the two pointers and not both.
    ///
    /// \p flags provides for future releases.  For now, it must be set to 0.
    ///
    /// \param pDevice - Returned device pointer for mapped memory
    /// \param pHost   - Requested host pointer mapping
    /// \param flags   - Flags for extensions (must be 0 for now)
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaSetDeviceFlags, ::cudaHostAlloc,
    /// ::cuMemHostGetDevicePointer
    pub fn cudaHostGetDevicePointer(
        pDevice: *mut *mut ::std::os::raw::c_void,
        pHost: *mut ::std::os::raw::c_void,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Passes back flags used to allocate pinned host memory allocated by
    /// cudaHostAlloc
    ///
    /// ::cudaHostGetFlags() will fail if the input pointer does not
    /// reside in an address range allocated by ::cudaHostAlloc().
    ///
    /// \param pFlags - Returned flags word
    /// \param pHost - Host pointer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaHostAlloc,
    /// ::cuMemHostGetFlags
    pub fn cudaHostGetFlags(
        pFlags: *mut ::std::os::raw::c_uint,
        pHost: *mut ::std::os::raw::c_void,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Allocates logical 1D, 2D, or 3D memory objects on the device
    ///
    /// Allocates at least \p width * \p height * \p depth bytes of linear memory
    /// on the device and returns a ::cudaPitchedPtr in which \p ptr is a pointer
    /// to the allocated memory. The function may pad the allocation to ensure
    /// hardware alignment requirements are met. The pitch returned in the \p pitch
    /// field of \p pitchedDevPtr is the width in bytes of the allocation.
    ///
    /// The returned ::cudaPitchedPtr contains additional fields \p xsize and
    /// \p ysize, the logical width and height of the allocation, which are
    /// equivalent to the \p width and \p height \p extent parameters provided by
    /// the programmer during allocation.
    ///
    /// For allocations of 2D and 3D objects, it is highly recommended that
    /// programmers perform allocations using ::cudaMalloc3D() or
    /// ::cudaMallocPitch(). Due to alignment restrictions in the hardware, this is
    /// especially true if the application will be performing memory copies
    /// involving 2D or 3D objects (whether linear memory or CUDA arrays).
    ///
    /// \param pitchedDevPtr  - Pointer to allocated pitched device memory
    /// \param extent         - Requested allocation size (\p width field in bytes)
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMallocPitch, ::cudaFree, ::cudaMemcpy3D, ::cudaMemset3D,
    /// ::cudaMalloc3DArray, ::cudaMallocArray, ::cudaFreeArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaHostAlloc, ::make_cudaPitchedPtr, ::make_cudaExtent,
    /// ::cuMemAllocPitch
    pub fn cudaMalloc3D(pitchedDevPtr: *mut cudaPitchedPtr, extent: cudaExtent) -> cudaError_t;
}
extern "C" {
    /// \brief Allocate an array on the device
    ///
    /// Allocates a CUDA array according to the ::cudaChannelFormatDesc structure
    /// \p desc and returns a handle to the new CUDA array in \p *array.
    ///
    /// The ::cudaChannelFormatDesc is defined as:
    /// \code
    ///struct cudaChannelFormatDesc {
    ///int x, y, z, w;
    ///enum cudaChannelFormatKind f;
    ///};
    ///\endcode
    /// where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
    /// ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
    ///
    /// ::cudaMalloc3DArray() can allocate the following:
    ///
    /// - A 1D array is allocated if the height and depth extents are both zero.
    /// - A 2D array is allocated if only the depth extent is zero.
    /// - A 3D array is allocated if all three extents are non-zero.
    /// - A 1D layered CUDA array is allocated if only the height extent is zero and
    /// the cudaArrayLayered flag is set. Each layer is a 1D array. The number of layers is
    /// determined by the depth extent.
    /// - A 2D layered CUDA array is allocated if all three extents are non-zero and
    /// the cudaArrayLayered flag is set. Each layer is a 2D array. The number of layers is
    /// determined by the depth extent.
    /// - A cubemap CUDA array is allocated if all three extents are non-zero and the
    /// cudaArrayCubemap flag is set. Width must be equal to height, and depth must be six. A cubemap is
    /// a special type of 2D layered CUDA array, where the six layers represent the six faces of a cube.
    /// The order of the six layers in memory is the same as that listed in ::cudaGraphicsCubeFace.
    /// - A cubemap layered CUDA array is allocated if all three extents are non-zero, and both,
    /// cudaArrayCubemap and cudaArrayLayered flags are set. Width must be equal to height, and depth must be
    /// a multiple of six. A cubemap layered CUDA array is a special type of 2D layered CUDA array that consists
    /// of a collection of cubemaps. The first six layers represent the first cubemap, the next six layers form
    /// the second cubemap, and so on.
    ///
    ///
    /// The \p flags parameter enables different options to be specified that affect
    /// the allocation, as follows.
    /// - ::cudaArrayDefault: This flag's value is defined to be 0 and provides default array allocation
    /// - ::cudaArrayLayered: Allocates a layered CUDA array, with the depth extent indicating the number of layers
    /// - ::cudaArrayCubemap: Allocates a cubemap CUDA array. Width must be equal to height, and depth must be six.
    ///   If the cudaArrayLayered flag is also set, depth must be a multiple of six.
    /// - ::cudaArraySurfaceLoadStore: Allocates a CUDA array that could be read from or written to using a surface
    ///   reference.
    /// - ::cudaArrayTextureGather: This flag indicates that texture gather operations will be performed on the CUDA
    ///   array. Texture gather can only be performed on 2D CUDA arrays.
    ///
    /// The width, height and depth extents must meet certain size requirements as listed in the following table.
    /// All values are specified in elements.
    ///
    /// Note that 2D CUDA arrays have different size requirements if the ::cudaArrayTextureGather flag is set. In that
    /// case, the valid range for (width, height, depth) is ((1,maxTexture2DGather[0]), (1,maxTexture2DGather[1]), 0).
    ///
    /// \xmlonly
    /// <table outputclass="xmlonly">
    /// <tgroup cols="3" colsep="1" rowsep="1">
    /// <colspec colname="c1" colwidth="1.0*"/>
    /// <colspec colname="c2" colwidth="3.0*"/>
    /// <colspec colname="c3" colwidth="3.0*"/>
    /// <thead>
    /// <row>
    /// <entry>CUDA array type</entry>
    /// <entry>Valid extents that must always be met {(width range in elements),
    /// (height range), (depth range)}</entry>
    /// <entry>Valid extents with cudaArraySurfaceLoadStore set {(width range in
    /// elements), (height range), (depth range)}</entry>
    /// </row>
    /// </thead>
    /// <tbody>
    /// <row>
    /// <entry>1D</entry>
    /// <entry>{ (1,maxTexture1D), 0, 0 }</entry>
    /// <entry>{ (1,maxSurface1D), 0, 0 }</entry>
    /// </row>
    /// <row>
    /// <entry>2D</entry>
    /// <entry>{ (1,maxTexture2D[0]), (1,maxTexture2D[1]), 0 }</entry>
    /// <entry>{ (1,maxSurface2D[0]), (1,maxSurface2D[1]), 0 }</entry>
    /// </row>
    /// <row>
    /// <entry>3D</entry>
    /// <entry>{ (1,maxTexture3D[0]), (1,maxTexture3D[1]), (1,maxTexture3D[2]) }
    /// OR { (1,maxTexture3DAlt[0]), (1,maxTexture3DAlt[1]),
    /// (1,maxTexture3DAlt[2]) }</entry>
    /// <entry>{ (1,maxSurface3D[0]), (1,maxSurface3D[1]), (1,maxSurface3D[2]) }</entry>
    /// </row>
    /// <row>
    /// <entry>1D Layered</entry>
    /// <entry>{ (1,maxTexture1DLayered[0]), 0, (1,maxTexture1DLayered[1]) }</entry>
    /// <entry>{ (1,maxSurface1DLayered[0]), 0, (1,maxSurface1DLayered[1]) }</entry>
    /// </row>
    /// <row>
    /// <entry>2D Layered</entry>
    /// <entry>{ (1,maxTexture2DLayered[0]), (1,maxTexture2DLayered[1]),
    /// (1,maxTexture2DLayered[2]) }</entry>
    /// <entry>{ (1,maxSurface2DLayered[0]), (1,maxSurface2DLayered[1]),
    /// (1,maxSurface2DLayered[2]) }</entry>
    /// </row>
    /// <row>
    /// <entry>Cubemap</entry>
    /// <entry>{ (1,maxTextureCubemap), (1,maxTextureCubemap), 6 }</entry>
    /// <entry>{ (1,maxSurfaceCubemap), (1,maxSurfaceCubemap), 6 }</entry>
    /// </row>
    /// <row>
    /// <entry>Cubemap Layered</entry>
    /// <entry>{ (1,maxTextureCubemapLayered[0]), (1,maxTextureCubemapLayered[0]),
    /// (1,maxTextureCubemapLayered[1]) }</entry>
    /// <entry>{ (1,maxSurfaceCubemapLayered[0]), (1,maxSurfaceCubemapLayered[0]),
    /// (1,maxSurfaceCubemapLayered[1]) }</entry>
    /// </row>
    /// </tbody>
    /// </tgroup>
    /// </table>
    /// \endxmlonly
    ///
    /// \param array  - Pointer to allocated array in device memory
    /// \param desc   - Requested channel format
    /// \param extent - Requested allocation size (\p width field in elements)
    /// \param flags  - Flags for extensions
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc3D, ::cudaMalloc, ::cudaMallocPitch, ::cudaFree,
    /// ::cudaFreeArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaHostAlloc,
    /// ::make_cudaExtent,
    /// ::cuArray3DCreate
    pub fn cudaMalloc3DArray(
        array: *mut cudaArray_t,
        desc: *const cudaChannelFormatDesc,
        extent: cudaExtent,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Allocate a mipmapped array on the device
    ///
    /// Allocates a CUDA mipmapped array according to the ::cudaChannelFormatDesc structure
    /// \p desc and returns a handle to the new CUDA mipmapped array in \p *mipmappedArray.
    /// \p numLevels specifies the number of mipmap levels to be allocated. This value is
    /// clamped to the range [1, 1 + floor(log2(max(width, height, depth)))].
    ///
    /// The ::cudaChannelFormatDesc is defined as:
    /// \code
    ///struct cudaChannelFormatDesc {
    ///int x, y, z, w;
    ///enum cudaChannelFormatKind f;
    ///};
    ///\endcode
    /// where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
    /// ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
    ///
    /// ::cudaMallocMipmappedArray() can allocate the following:
    ///
    /// - A 1D mipmapped array is allocated if the height and depth extents are both zero.
    /// - A 2D mipmapped array is allocated if only the depth extent is zero.
    /// - A 3D mipmapped array is allocated if all three extents are non-zero.
    /// - A 1D layered CUDA mipmapped array is allocated if only the height extent is zero and
    /// the cudaArrayLayered flag is set. Each layer is a 1D mipmapped array. The number of layers is
    /// determined by the depth extent.
    /// - A 2D layered CUDA mipmapped array is allocated if all three extents are non-zero and
    /// the cudaArrayLayered flag is set. Each layer is a 2D mipmapped array. The number of layers is
    /// determined by the depth extent.
    /// - A cubemap CUDA mipmapped array is allocated if all three extents are non-zero and the
    /// cudaArrayCubemap flag is set. Width must be equal to height, and depth must be six.
    /// The order of the six layers in memory is the same as that listed in ::cudaGraphicsCubeFace.
    /// - A cubemap layered CUDA mipmapped array is allocated if all three extents are non-zero, and both,
    /// cudaArrayCubemap and cudaArrayLayered flags are set. Width must be equal to height, and depth must be
    /// a multiple of six. A cubemap layered CUDA mipmapped array is a special type of 2D layered CUDA mipmapped
    /// array that consists of a collection of cubemap mipmapped arrays. The first six layers represent the
    /// first cubemap mipmapped array, the next six layers form the second cubemap mipmapped array, and so on.
    ///
    ///
    /// The \p flags parameter enables different options to be specified that affect
    /// the allocation, as follows.
    /// - ::cudaArrayDefault: This flag's value is defined to be 0 and provides default mipmapped array allocation
    /// - ::cudaArrayLayered: Allocates a layered CUDA mipmapped array, with the depth extent indicating the number of layers
    /// - ::cudaArrayCubemap: Allocates a cubemap CUDA mipmapped array. Width must be equal to height, and depth must be six.
    ///   If the cudaArrayLayered flag is also set, depth must be a multiple of six.
    /// - ::cudaArraySurfaceLoadStore: This flag indicates that individual mipmap levels of the CUDA mipmapped array
    ///   will be read from or written to using a surface reference.
    /// - ::cudaArrayTextureGather: This flag indicates that texture gather operations will be performed on the CUDA
    ///   array. Texture gather can only be performed on 2D CUDA mipmapped arrays, and the gather operations are
    ///   performed only on the most detailed mipmap level.
    ///
    /// The width, height and depth extents must meet certain size requirements as listed in the following table.
    /// All values are specified in elements.
    ///
    /// \xmlonly
    /// <table outputclass="xmlonly">
    /// <tgroup cols="3" colsep="1" rowsep="1">
    /// <colspec colname="c1" colwidth="1.0*"/>
    /// <colspec colname="c2" colwidth="3.0*"/>
    /// <colspec colname="c3" colwidth="3.0*"/>
    /// <thead>
    /// <row>
    /// <entry>CUDA array type</entry>
    /// <entry>Valid extents that must always be met {(width range in elements),
    /// (height range), (depth range)}</entry>
    /// <entry>Valid extents with cudaArraySurfaceLoadStore set {(width range in
    /// elements), (height range), (depth range)}</entry>
    /// </row>
    /// </thead>
    /// <tbody>
    /// <row>
    /// <entry>1D</entry>
    /// <entry>{ (1,maxTexture1DMipmap), 0, 0 }</entry>
    /// <entry>{ (1,maxSurface1D), 0, 0 }</entry>
    /// </row>
    /// <row>
    /// <entry>2D</entry>
    /// <entry>{ (1,maxTexture2DMipmap[0]), (1,maxTexture2DMipmap[1]), 0 }</entry>
    /// <entry>{ (1,maxSurface2D[0]), (1,maxSurface2D[1]), 0 }</entry>
    /// </row>
    /// <row>
    /// <entry>3D</entry>
    /// <entry>{ (1,maxTexture3D[0]), (1,maxTexture3D[1]), (1,maxTexture3D[2]) }
    /// OR { (1,maxTexture3DAlt[0]), (1,maxTexture3DAlt[1]),
    /// (1,maxTexture3DAlt[2]) }</entry>
    /// <entry>{ (1,maxSurface3D[0]), (1,maxSurface3D[1]), (1,maxSurface3D[2]) }</entry>
    /// </row>
    /// <row>
    /// <entry>1D Layered</entry>
    /// <entry>{ (1,maxTexture1DLayered[0]), 0, (1,maxTexture1DLayered[1]) }</entry>
    /// <entry>{ (1,maxSurface1DLayered[0]), 0, (1,maxSurface1DLayered[1]) }</entry>
    /// </row>
    /// <row>
    /// <entry>2D Layered</entry>
    /// <entry>{ (1,maxTexture2DLayered[0]), (1,maxTexture2DLayered[1]),
    /// (1,maxTexture2DLayered[2]) }</entry>
    /// <entry>{ (1,maxSurface2DLayered[0]), (1,maxSurface2DLayered[1]),
    /// (1,maxSurface2DLayered[2]) }</entry>
    /// </row>
    /// <row>
    /// <entry>Cubemap</entry>
    /// <entry>{ (1,maxTextureCubemap), (1,maxTextureCubemap), 6 }</entry>
    /// <entry>{ (1,maxSurfaceCubemap), (1,maxSurfaceCubemap), 6 }</entry>
    /// </row>
    /// <row>
    /// <entry>Cubemap Layered</entry>
    /// <entry>{ (1,maxTextureCubemapLayered[0]), (1,maxTextureCubemapLayered[0]),
    /// (1,maxTextureCubemapLayered[1]) }</entry>
    /// <entry>{ (1,maxSurfaceCubemapLayered[0]), (1,maxSurfaceCubemapLayered[0]),
    /// (1,maxSurfaceCubemapLayered[1]) }</entry>
    /// </row>
    /// </tbody>
    /// </tgroup>
    /// </table>
    /// \endxmlonly
    ///
    /// \param mipmappedArray  - Pointer to allocated mipmapped array in device memory
    /// \param desc            - Requested channel format
    /// \param extent          - Requested allocation size (\p width field in elements)
    /// \param numLevels       - Number of mipmap levels to allocate
    /// \param flags           - Flags for extensions
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc3D, ::cudaMalloc, ::cudaMallocPitch, ::cudaFree,
    /// ::cudaFreeArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaHostAlloc,
    /// ::make_cudaExtent,
    /// ::cuMipmappedArrayCreate
    pub fn cudaMallocMipmappedArray(
        mipmappedArray: *mut cudaMipmappedArray_t,
        desc: *const cudaChannelFormatDesc,
        extent: cudaExtent,
        numLevels: ::std::os::raw::c_uint,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Gets a mipmap level of a CUDA mipmapped array
    ///
    /// Returns in \p *levelArray a CUDA array that represents a single mipmap level
    /// of the CUDA mipmapped array \p mipmappedArray.
    ///
    /// If \p level is greater than the maximum number of levels in this mipmapped array,
    /// ::cudaErrorInvalidValue is returned.
    ///
    /// \param levelArray     - Returned mipmap level CUDA array
    /// \param mipmappedArray - CUDA mipmapped array
    /// \param level          - Mipmap level
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc3D, ::cudaMalloc, ::cudaMallocPitch, ::cudaFree,
    /// ::cudaFreeArray,
    /// \ref ::cudaMallocHost(void**, size_t) "cudaMallocHost (C API)",
    /// ::cudaFreeHost, ::cudaHostAlloc,
    /// ::make_cudaExtent,
    /// ::cuMipmappedArrayGetLevel
    pub fn cudaGetMipmappedArrayLevel(
        levelArray: *mut cudaArray_t,
        mipmappedArray: cudaMipmappedArray_const_t,
        level: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between 3D objects
    ///
    ///\code
    ///struct cudaExtent {
    ///size_t width;
    ///size_t height;
    ///size_t depth;
    ///};
    ///struct cudaExtent make_cudaExtent(size_t w, size_t h, size_t d);
    ///
    ///struct cudaPos {
    ///size_t x;
    ///size_t y;
    ///size_t z;
    ///};
    ///struct cudaPos make_cudaPos(size_t x, size_t y, size_t z);
    ///
    ///struct cudaMemcpy3DParms {
    ///cudaArray_t           srcArray;
    ///struct cudaPos        srcPos;
    ///struct cudaPitchedPtr srcPtr;
    ///cudaArray_t           dstArray;
    ///struct cudaPos        dstPos;
    ///struct cudaPitchedPtr dstPtr;
    ///struct cudaExtent     extent;
    ///enum cudaMemcpyKind   kind;
    ///};
    ///\endcode
    ///
    /// ::cudaMemcpy3D() copies data betwen two 3D objects. The source and
    /// destination objects may be in either host memory, device memory, or a CUDA
    /// array. The source, destination, extent, and kind of copy performed is
    /// specified by the ::cudaMemcpy3DParms struct which should be initialized to
    /// zero before use:
    ///\code
    ///cudaMemcpy3DParms myParms = {0};
    ///\endcode
    ///
    /// The struct passed to ::cudaMemcpy3D() must specify one of \p srcArray or
    /// \p srcPtr and one of \p dstArray or \p dstPtr. Passing more than one
    /// non-zero source or destination will cause ::cudaMemcpy3D() to return an
    /// error.
    ///
    /// The \p srcPos and \p dstPos fields are optional offsets into the source and
    /// destination objects and are defined in units of each object's elements. The
    /// element for a host or device pointer is assumed to be <b>unsigned char</b>.
    ///
    /// The \p extent field defines the dimensions of the transferred area in
    /// elements. If a CUDA array is participating in the copy, the extent is
    /// defined in terms of that array's elements. If no CUDA array is
    /// participating in the copy then the extents are defined in elements of
    /// <b>unsigned char</b>.
    ///
    /// The \p kind field defines the direction of the copy. It must be one of
    /// ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    /// For ::cudaMemcpyHostToHost or ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost
    /// passed as kind and cudaArray type passed as source or destination, if the kind
    /// implies cudaArray type to be present on the host, ::cudaMemcpy3D() will
    /// disregard that implication and silently correct the kind based on the fact that
    /// cudaArray type can only be present on the device.
    ///
    /// If the source and destination are both arrays, ::cudaMemcpy3D() will return
    /// an error if they do not have the same element size.
    ///
    /// The source and destination object may not overlap. If overlapping source
    /// and destination objects are specified, undefined behavior will result.
    ///
    /// The source object must lie entirely within the region defined by \p srcPos
    /// and \p extent. The destination object must lie entirely within the region
    /// defined by \p dstPos and \p extent.
    ///
    /// ::cudaMemcpy3D() returns an error if the pitch of \p srcPtr or \p dstPtr
    /// exceeds the maximum allowed. The pitch of a ::cudaPitchedPtr allocated
    /// with ::cudaMalloc3D() will always be valid.
    ///
    /// \param p - 3D memory copy parameters
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidPitchValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_sync
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc3D, ::cudaMalloc3DArray, ::cudaMemset3D, ::cudaMemcpy3DAsync,
    /// ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::make_cudaExtent, ::make_cudaPos,
    /// ::cuMemcpy3D
    pub fn cudaMemcpy3D(p: *const cudaMemcpy3DParms) -> cudaError_t;
}
extern "C" {
    /// \brief Copies memory between devices
    ///
    /// Perform a 3D memory copy according to the parameters specified in
    /// \p p.  See the definition of the ::cudaMemcpy3DPeerParms structure
    /// for documentation of its parameters.
    ///
    /// Note that this function is synchronous with respect to the host only if
    /// the source or destination of the transfer is host memory.  Note also
    /// that this copy is serialized with respect to all pending and future
    /// asynchronous work in to the current device, the copy's source device,
    /// and the copy's destination device (use ::cudaMemcpy3DPeerAsync to avoid
    /// this synchronization).
    ///
    /// \param p - Parameters for the memory copy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_sync
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync, ::cudaMemcpyPeerAsync,
    /// ::cudaMemcpy3DPeerAsync,
    /// ::cuMemcpy3DPeer
    pub fn cudaMemcpy3DPeer(p: *const cudaMemcpy3DPeerParms) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between 3D objects
    ///
    ///\code
    ///struct cudaExtent {
    ///size_t width;
    ///size_t height;
    ///size_t depth;
    ///};
    ///struct cudaExtent make_cudaExtent(size_t w, size_t h, size_t d);
    ///
    ///struct cudaPos {
    ///size_t x;
    ///size_t y;
    ///size_t z;
    ///};
    ///struct cudaPos make_cudaPos(size_t x, size_t y, size_t z);
    ///
    ///struct cudaMemcpy3DParms {
    ///cudaArray_t           srcArray;
    ///struct cudaPos        srcPos;
    ///struct cudaPitchedPtr srcPtr;
    ///cudaArray_t           dstArray;
    ///struct cudaPos        dstPos;
    ///struct cudaPitchedPtr dstPtr;
    ///struct cudaExtent     extent;
    ///enum cudaMemcpyKind   kind;
    ///};
    ///\endcode
    ///
    /// ::cudaMemcpy3DAsync() copies data betwen two 3D objects. The source and
    /// destination objects may be in either host memory, device memory, or a CUDA
    /// array. The source, destination, extent, and kind of copy performed is
    /// specified by the ::cudaMemcpy3DParms struct which should be initialized to
    /// zero before use:
    ///\code
    ///cudaMemcpy3DParms myParms = {0};
    ///\endcode
    ///
    /// The struct passed to ::cudaMemcpy3DAsync() must specify one of \p srcArray
    /// or \p srcPtr and one of \p dstArray or \p dstPtr. Passing more than one
    /// non-zero source or destination will cause ::cudaMemcpy3DAsync() to return an
    /// error.
    ///
    /// The \p srcPos and \p dstPos fields are optional offsets into the source and
    /// destination objects and are defined in units of each object's elements. The
    /// element for a host or device pointer is assumed to be <b>unsigned char</b>.
    /// For CUDA arrays, positions must be in the range [0, 2048) for any
    /// dimension.
    ///
    /// The \p extent field defines the dimensions of the transferred area in
    /// elements. If a CUDA array is participating in the copy, the extent is
    /// defined in terms of that array's elements. If no CUDA array is
    /// participating in the copy then the extents are defined in elements of
    /// <b>unsigned char</b>.
    ///
    /// The \p kind field defines the direction of the copy. It must be one of
    /// ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    /// For ::cudaMemcpyHostToHost or ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost
    /// passed as kind and cudaArray type passed as source or destination, if the kind
    /// implies cudaArray type to be present on the host, ::cudaMemcpy3DAsync() will
    /// disregard that implication and silently correct the kind based on the fact that
    /// cudaArray type can only be present on the device.
    ///
    /// If the source and destination are both arrays, ::cudaMemcpy3DAsync() will
    /// return an error if they do not have the same element size.
    ///
    /// The source and destination object may not overlap. If overlapping source
    /// and destination objects are specified, undefined behavior will result.
    ///
    /// The source object must lie entirely within the region defined by \p srcPos
    /// and \p extent. The destination object must lie entirely within the region
    /// defined by \p dstPos and \p extent.
    ///
    /// ::cudaMemcpy3DAsync() returns an error if the pitch of \p srcPtr or
    /// \p dstPtr exceeds the maximum allowed. The pitch of a
    /// ::cudaPitchedPtr allocated with ::cudaMalloc3D() will always be valid.
    ///
    /// ::cudaMemcpy3DAsync() is asynchronous with respect to the host, so
    /// the call may return before the copy is complete. The copy can optionally
    /// be associated to a stream by passing a non-zero \p stream argument. If
    /// \p kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream
    /// is non-zero, the copy may overlap with operations in other streams.
    ///
    /// The device version of this function only handles device to device copies and
    /// cannot be given local or shared pointers.
    ///
    /// \param p      - 3D memory copy parameters
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidPitchValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMalloc3D, ::cudaMalloc3DArray, ::cudaMemset3D, ::cudaMemcpy3D,
    /// ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::make_cudaExtent, ::make_cudaPos,
    /// ::cuMemcpy3DAsync
    pub fn cudaMemcpy3DAsync(p: *const cudaMemcpy3DParms, stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /// \brief Copies memory between devices asynchronously.
    ///
    /// Perform a 3D memory copy according to the parameters specified in
    /// \p p.  See the definition of the ::cudaMemcpy3DPeerParms structure
    /// for documentation of its parameters.
    ///
    /// \param p      - Parameters for the memory copy
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync, ::cudaMemcpyPeerAsync,
    /// ::cudaMemcpy3DPeerAsync,
    /// ::cuMemcpy3DPeerAsync
    pub fn cudaMemcpy3DPeerAsync(
        p: *const cudaMemcpy3DPeerParms,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Gets free and total device memory
    ///
    /// Returns in \p *free and \p *total respectively, the free and total amount of
    /// memory available for allocation by the device in bytes.
    ///
    /// \param free  - Returned free memory in bytes
    /// \param total - Returned total memory in bytes
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorLaunchFailure
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cuMemGetInfo
    pub fn cudaMemGetInfo(free: *mut usize, total: *mut usize) -> cudaError_t;
}
extern "C" {
    /// \brief Gets info about the specified cudaArray
    ///
    /// Returns in \p *desc, \p *extent and \p *flags respectively, the type, shape
    /// and flags of \p array.
    ///
    /// Any of \p *desc, \p *extent and \p *flags may be specified as NULL.
    ///
    /// \param desc   - Returned array type
    /// \param extent - Returned array shape. 2D arrays will have depth of zero
    /// \param flags  - Returned array flags
    /// \param array  - The ::cudaArray to get info for
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cuArrayGetDescriptor,
    /// ::cuArray3DGetDescriptor
    pub fn cudaArrayGetInfo(
        desc: *mut cudaChannelFormatDesc,
        extent: *mut cudaExtent,
        flags: *mut ::std::os::raw::c_uint,
        array: cudaArray_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies \p count bytes from the memory area pointed to by \p src to the
    /// memory area pointed to by \p dst, where \p kind specifies the direction
    /// of the copy, and must be one of ::cudaMemcpyHostToHost,
    /// ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing. Calling
    /// ::cudaMemcpy() with dst and src pointers that do not match the direction of
    /// the copy results in an undefined behavior.
    ///
    /// \param dst   - Destination memory address
    /// \param src   - Source memory address
    /// \param count - Size in bytes to copy
    /// \param kind  - Type of transfer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \note_sync
    ///
    /// \sa ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpyDtoH,
    /// ::cuMemcpyHtoD,
    /// ::cuMemcpyDtoD,
    /// ::cuMemcpy
    pub fn cudaMemcpy(
        dst: *mut ::std::os::raw::c_void,
        src: *const ::std::os::raw::c_void,
        count: usize,
        kind: cudaMemcpyKind,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies memory between two devices
    ///
    /// Copies memory from one device to memory on another device.  \p dst is the
    /// base device pointer of the destination memory and \p dstDevice is the
    /// destination device.  \p src is the base device pointer of the source memory
    /// and \p srcDevice is the source device.  \p count specifies the number of bytes
    /// to copy.
    ///
    /// Note that this function is asynchronous with respect to the host, but
    /// serialized with respect all pending and future asynchronous work in to the
    /// current device, \p srcDevice, and \p dstDevice (use ::cudaMemcpyPeerAsync
    /// to avoid this synchronization).
    ///
    /// \param dst       - Destination device pointer
    /// \param dstDevice - Destination device
    /// \param src       - Source device pointer
    /// \param srcDevice - Source device
    /// \param count     - Size of memory copy in bytes
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_sync
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpyAsync, ::cudaMemcpyPeerAsync,
    /// ::cudaMemcpy3DPeerAsync,
    /// ::cuMemcpyPeer
    pub fn cudaMemcpyPeer(
        dst: *mut ::std::os::raw::c_void,
        dstDevice: ::std::os::raw::c_int,
        src: *const ::std::os::raw::c_void,
        srcDevice: ::std::os::raw::c_int,
        count: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies \p count bytes from the memory area pointed to by \p src to the
    /// CUDA array \p dst starting at the upper left corner
    /// (\p wOffset, \p hOffset), where \p kind specifies the direction
    /// of the copy, and must be one of ::cudaMemcpyHostToHost,
    /// ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    ///
    /// \param dst     - Destination memory address
    /// \param wOffset - Destination starting X offset
    /// \param hOffset - Destination starting Y offset
    /// \param src     - Source memory address
    /// \param count   - Size in bytes to copy
    /// \param kind    - Type of transfer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_sync
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpyHtoA,
    /// ::cuMemcpyDtoA
    pub fn cudaMemcpyToArray(
        dst: cudaArray_t,
        wOffset: usize,
        hOffset: usize,
        src: *const ::std::os::raw::c_void,
        count: usize,
        kind: cudaMemcpyKind,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies \p count bytes from the CUDA array \p src starting at the upper
    /// left corner (\p wOffset, hOffset) to the memory area pointed to by \p dst,
    /// where \p kind specifies the direction of the copy, and must be one of
    /// ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    ///
    /// \param dst     - Destination memory address
    /// \param src     - Source memory address
    /// \param wOffset - Source starting X offset
    /// \param hOffset - Source starting Y offset
    /// \param count   - Size in bytes to copy
    /// \param kind    - Type of transfer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_sync
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpyAtoH,
    /// ::cuMemcpyAtoD
    pub fn cudaMemcpyFromArray(
        dst: *mut ::std::os::raw::c_void,
        src: cudaArray_const_t,
        wOffset: usize,
        hOffset: usize,
        count: usize,
        kind: cudaMemcpyKind,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies \p count bytes from the CUDA array \p src starting at the upper
    /// left corner (\p wOffsetSrc, \p hOffsetSrc) to the CUDA array \p dst
    /// starting at the upper left corner (\p wOffsetDst, \p hOffsetDst) where
    /// \p kind specifies the direction of the copy, and must be one of
    /// ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    ///
    /// \param dst        - Destination memory address
    /// \param wOffsetDst - Destination starting X offset
    /// \param hOffsetDst - Destination starting Y offset
    /// \param src        - Source memory address
    /// \param wOffsetSrc - Source starting X offset
    /// \param hOffsetSrc - Source starting Y offset
    /// \param count      - Size in bytes to copy
    /// \param kind       - Type of transfer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpyAtoA
    pub fn cudaMemcpyArrayToArray(
        dst: cudaArray_t,
        wOffsetDst: usize,
        hOffsetDst: usize,
        src: cudaArray_const_t,
        wOffsetSrc: usize,
        hOffsetSrc: usize,
        count: usize,
        kind: cudaMemcpyKind,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies a matrix (\p height rows of \p width bytes each) from the memory
    /// area pointed to by \p src to the memory area pointed to by \p dst, where
    /// \p kind specifies the direction of the copy, and must be one of
    /// ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing. \p dpitch and
    /// \p spitch are the widths in memory in bytes of the 2D arrays pointed to by
    /// \p dst and \p src, including any padding added to the end of each row. The
    /// memory areas may not overlap. \p width must not exceed either \p dpitch or
    /// \p spitch. Calling ::cudaMemcpy2D() with \p dst and \p src pointers that do
    /// not match the direction of the copy results in an undefined behavior.
    /// ::cudaMemcpy2D() returns an error if \p dpitch or \p spitch exceeds
    /// the maximum allowed.
    ///
    /// \param dst    - Destination memory address
    /// \param dpitch - Pitch of destination memory
    /// \param src    - Source memory address
    /// \param spitch - Pitch of source memory
    /// \param width  - Width of matrix transfer (columns in bytes)
    /// \param height - Height of matrix transfer (rows)
    /// \param kind   - Type of transfer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidPitchValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpy2D,
    /// ::cuMemcpy2DUnaligned
    pub fn cudaMemcpy2D(
        dst: *mut ::std::os::raw::c_void,
        dpitch: usize,
        src: *const ::std::os::raw::c_void,
        spitch: usize,
        width: usize,
        height: usize,
        kind: cudaMemcpyKind,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies a matrix (\p height rows of \p width bytes each) from the memory
    /// area pointed to by \p src to the CUDA array \p dst starting at the
    /// upper left corner (\p wOffset, \p hOffset) where \p kind specifies the
    /// direction of the copy, and must be one of ::cudaMemcpyHostToHost,
    /// ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    /// \p spitch is the width in memory in bytes of the 2D array pointed to by
    /// \p src, including any padding added to the end of each row. \p wOffset +
    /// \p width must not exceed the width of the CUDA array \p dst. \p width must
    /// not exceed \p spitch. ::cudaMemcpy2DToArray() returns an error if \p spitch
    /// exceeds the maximum allowed.
    ///
    /// \param dst     - Destination memory address
    /// \param wOffset - Destination starting X offset
    /// \param hOffset - Destination starting Y offset
    /// \param src     - Source memory address
    /// \param spitch  - Pitch of source memory
    /// \param width   - Width of matrix transfer (columns in bytes)
    /// \param height  - Height of matrix transfer (rows)
    /// \param kind    - Type of transfer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidPitchValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_sync
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpy2D,
    /// ::cuMemcpy2DUnaligned
    pub fn cudaMemcpy2DToArray(
        dst: cudaArray_t,
        wOffset: usize,
        hOffset: usize,
        src: *const ::std::os::raw::c_void,
        spitch: usize,
        width: usize,
        height: usize,
        kind: cudaMemcpyKind,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies a matrix (\p height rows of \p width bytes each) from the CUDA
    /// array \p srcArray starting at the upper left corner
    /// (\p wOffset, \p hOffset) to the memory area pointed to by \p dst, where
    /// \p kind specifies the direction of the copy, and must be one of
    /// ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing. \p dpitch is the
    /// width in memory in bytes of the 2D array pointed to by \p dst, including any
    /// padding added to the end of each row. \p wOffset + \p width must not exceed
    /// the width of the CUDA array \p src. \p width must not exceed \p dpitch.
    /// ::cudaMemcpy2DFromArray() returns an error if \p dpitch exceeds the maximum
    /// allowed.
    ///
    /// \param dst     - Destination memory address
    /// \param dpitch  - Pitch of destination memory
    /// \param src     - Source memory address
    /// \param wOffset - Source starting X offset
    /// \param hOffset - Source starting Y offset
    /// \param width   - Width of matrix transfer (columns in bytes)
    /// \param height  - Height of matrix transfer (rows)
    /// \param kind    - Type of transfer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidPitchValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_sync
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpy2D,
    /// ::cuMemcpy2DUnaligned
    pub fn cudaMemcpy2DFromArray(
        dst: *mut ::std::os::raw::c_void,
        dpitch: usize,
        src: cudaArray_const_t,
        wOffset: usize,
        hOffset: usize,
        width: usize,
        height: usize,
        kind: cudaMemcpyKind,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies a matrix (\p height rows of \p width bytes each) from the CUDA
    /// array \p srcArray starting at the upper left corner
    /// (\p wOffsetSrc, \p hOffsetSrc) to the CUDA array \p dst starting at
    /// the upper left corner (\p wOffsetDst, \p hOffsetDst), where \p kind
    /// specifies the direction of the copy, and must be one of
    /// ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    /// \p wOffsetDst + \p width must not exceed the width of the CUDA array \p dst.
    /// \p wOffsetSrc + \p width must not exceed the width of the CUDA array \p src.
    ///
    /// \param dst        - Destination memory address
    /// \param wOffsetDst - Destination starting X offset
    /// \param hOffsetDst - Destination starting Y offset
    /// \param src        - Source memory address
    /// \param wOffsetSrc - Source starting X offset
    /// \param hOffsetSrc - Source starting Y offset
    /// \param width      - Width of matrix transfer (columns in bytes)
    /// \param height     - Height of matrix transfer (rows)
    /// \param kind       - Type of transfer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_sync
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpy2D,
    /// ::cuMemcpy2DUnaligned
    pub fn cudaMemcpy2DArrayToArray(
        dst: cudaArray_t,
        wOffsetDst: usize,
        hOffsetDst: usize,
        src: cudaArray_const_t,
        wOffsetSrc: usize,
        hOffsetSrc: usize,
        width: usize,
        height: usize,
        kind: cudaMemcpyKind,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data to the given symbol on the device
    ///
    /// Copies \p count bytes from the memory area pointed to by \p src
    /// to the memory area pointed to by \p offset bytes from the start of symbol
    /// \p symbol. The memory areas may not overlap. \p symbol is a variable that
    /// resides in global or constant memory space. \p kind can be either
    /// ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
    /// Passing ::cudaMemcpyDefault is recommended, in which case the type of
    /// transfer is inferred from the pointer values. However, ::cudaMemcpyDefault
    /// is only allowed on systems that support unified virtual addressing.
    ///
    /// \param symbol - Device symbol address
    /// \param src    - Source memory address
    /// \param count  - Size in bytes to copy
    /// \param offset - Offset from start of symbol in bytes
    /// \param kind   - Type of transfer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidSymbol,
    /// ::cudaErrorInvalidMemcpyDirection,
    /// ::cudaErrorNoKernelImageForDevice
    /// \notefnerr
    /// \note_sync
    /// \note_string_api_deprecation
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpy,
    /// ::cuMemcpyHtoD,
    /// ::cuMemcpyDtoD
    pub fn cudaMemcpyToSymbol(
        symbol: *const ::std::os::raw::c_void,
        src: *const ::std::os::raw::c_void,
        count: usize,
        offset: usize,
        kind: cudaMemcpyKind,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data from the given symbol on the device
    ///
    /// Copies \p count bytes from the memory area pointed to by \p offset bytes
    /// from the start of symbol \p symbol to the memory area pointed to by \p dst.
    /// The memory areas may not overlap. \p symbol is a variable that
    /// resides in global or constant memory space. \p kind can be either
    /// ::cudaMemcpyDeviceToHost, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
    /// Passing ::cudaMemcpyDefault is recommended, in which case the type of
    /// transfer is inferred from the pointer values. However, ::cudaMemcpyDefault
    /// is only allowed on systems that support unified virtual addressing.
    ///
    /// \param dst    - Destination memory address
    /// \param symbol - Device symbol address
    /// \param count  - Size in bytes to copy
    /// \param offset - Offset from start of symbol in bytes
    /// \param kind   - Type of transfer
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidSymbol,
    /// ::cudaErrorInvalidMemcpyDirection,
    /// ::cudaErrorNoKernelImageForDevice
    /// \notefnerr
    /// \note_sync
    /// \note_string_api_deprecation
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpy,
    /// ::cuMemcpyDtoH,
    /// ::cuMemcpyDtoD
    pub fn cudaMemcpyFromSymbol(
        dst: *mut ::std::os::raw::c_void,
        symbol: *const ::std::os::raw::c_void,
        count: usize,
        offset: usize,
        kind: cudaMemcpyKind,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies \p count bytes from the memory area pointed to by \p src to the
    /// memory area pointed to by \p dst, where \p kind specifies the
    /// direction of the copy, and must be one of ::cudaMemcpyHostToHost,
    /// ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    ///
    /// The memory areas may not overlap. Calling ::cudaMemcpyAsync() with \p dst and
    /// \p src pointers that do not match the direction of the copy results in an
    /// undefined behavior.
    ///
    /// ::cudaMemcpyAsync() is asynchronous with respect to the host, so the call
    /// may return before the copy is complete. The copy can optionally be
    /// associated to a stream by passing a non-zero \p stream argument. If \p kind
    /// is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and the \p stream is
    /// non-zero, the copy may overlap with operations in other streams.
    ///
    /// The device version of this function only handles device to device copies and
    /// cannot be given local or shared pointers.
    ///
    /// \param dst    - Destination memory address
    /// \param src    - Source memory address
    /// \param count  - Size in bytes to copy
    /// \param kind   - Type of transfer
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync
    /// ::cuMemcpyAsync,
    /// ::cuMemcpyDtoHAsync,
    /// ::cuMemcpyHtoDAsync,
    /// ::cuMemcpyDtoDAsync
    pub fn cudaMemcpyAsync(
        dst: *mut ::std::os::raw::c_void,
        src: *const ::std::os::raw::c_void,
        count: usize,
        kind: cudaMemcpyKind,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies memory between two devices asynchronously.
    ///
    /// Copies memory from one device to memory on another device.  \p dst is the
    /// base device pointer of the destination memory and \p dstDevice is the
    /// destination device.  \p src is the base device pointer of the source memory
    /// and \p srcDevice is the source device.  \p count specifies the number of bytes
    /// to copy.
    ///
    /// Note that this function is asynchronous with respect to the host and all work
    /// on other devices.
    ///
    /// \param dst       - Destination device pointer
    /// \param dstDevice - Destination device
    /// \param src       - Source device pointer
    /// \param srcDevice - Source device
    /// \param count     - Size of memory copy in bytes
    /// \param stream    - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync,
    /// ::cudaMemcpy3DPeerAsync,
    /// ::cuMemcpyPeerAsync
    pub fn cudaMemcpyPeerAsync(
        dst: *mut ::std::os::raw::c_void,
        dstDevice: ::std::os::raw::c_int,
        src: *const ::std::os::raw::c_void,
        srcDevice: ::std::os::raw::c_int,
        count: usize,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies \p count bytes from the memory area pointed to by \p src to the
    /// CUDA array \p dst starting at the upper left corner
    /// (\p wOffset, \p hOffset), where \p kind specifies the
    /// direction of the copy, and must be one of ::cudaMemcpyHostToHost,
    /// ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    ///
    /// ::cudaMemcpyToArrayAsync() is asynchronous with respect to the host, so
    /// the call may return before the copy is complete. The copy can optionally
    /// be associated to a stream by passing a non-zero \p stream argument. If \p
    /// kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream
    /// is non-zero, the copy may overlap with operations in other streams.
    ///
    /// \param dst     - Destination memory address
    /// \param wOffset - Destination starting X offset
    /// \param hOffset - Destination starting Y offset
    /// \param src     - Source memory address
    /// \param count   - Size in bytes to copy
    /// \param kind    - Type of transfer
    /// \param stream  - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpyHtoAAsync,
    /// ::cuMemcpy2DAsync
    pub fn cudaMemcpyToArrayAsync(
        dst: cudaArray_t,
        wOffset: usize,
        hOffset: usize,
        src: *const ::std::os::raw::c_void,
        count: usize,
        kind: cudaMemcpyKind,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies \p count bytes from the CUDA array \p src starting at the upper
    /// left corner (\p wOffset, hOffset) to the memory area pointed to by \p dst,
    /// where \p kind specifies the direction of the copy, and must be one of
    /// ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    ///
    /// ::cudaMemcpyFromArrayAsync() is asynchronous with respect to the host, so
    /// the call may return before the copy is complete. The copy can optionally
    /// be associated to a stream by passing a non-zero \p stream argument. If \p
    /// kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream
    /// is non-zero, the copy may overlap with operations in other streams.
    ///
    /// \param dst     - Destination memory address
    /// \param src     - Source memory address
    /// \param wOffset - Source starting X offset
    /// \param hOffset - Source starting Y offset
    /// \param count   - Size in bytes to copy
    /// \param kind    - Type of transfer
    /// \param stream  - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpyAtoHAsync,
    /// ::cuMemcpy2DAsync
    pub fn cudaMemcpyFromArrayAsync(
        dst: *mut ::std::os::raw::c_void,
        src: cudaArray_const_t,
        wOffset: usize,
        hOffset: usize,
        count: usize,
        kind: cudaMemcpyKind,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies a matrix (\p height rows of \p width bytes each) from the memory
    /// area pointed to by \p src to the memory area pointed to by \p dst, where
    /// \p kind specifies the direction of the copy, and must be one of
    /// ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    /// \p dpitch and \p spitch are the widths in memory in bytes of the 2D arrays
    /// pointed to by \p dst and \p src, including any padding added to the end of
    /// each row. The memory areas may not overlap. \p width must not exceed either
    /// \p dpitch or \p spitch.
    ///
    /// Calling ::cudaMemcpy2DAsync() with \p dst and \p src pointers that do not
    /// match the direction of the copy results in an undefined behavior.
    /// ::cudaMemcpy2DAsync() returns an error if \p dpitch or \p spitch is greater
    /// than the maximum allowed.
    ///
    /// ::cudaMemcpy2DAsync() is asynchronous with respect to the host, so
    /// the call may return before the copy is complete. The copy can optionally
    /// be associated to a stream by passing a non-zero \p stream argument. If
    /// \p kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and
    /// \p stream is non-zero, the copy may overlap with operations in other
    /// streams.
    ///
    /// The device version of this function only handles device to device copies and
    /// cannot be given local or shared pointers.
    ///
    /// \param dst    - Destination memory address
    /// \param dpitch - Pitch of destination memory
    /// \param src    - Source memory address
    /// \param spitch - Pitch of source memory
    /// \param width  - Width of matrix transfer (columns in bytes)
    /// \param height - Height of matrix transfer (rows)
    /// \param kind   - Type of transfer
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidPitchValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpy2DAsync
    pub fn cudaMemcpy2DAsync(
        dst: *mut ::std::os::raw::c_void,
        dpitch: usize,
        src: *const ::std::os::raw::c_void,
        spitch: usize,
        width: usize,
        height: usize,
        kind: cudaMemcpyKind,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies a matrix (\p height rows of \p width bytes each) from the memory
    /// area pointed to by \p src to the CUDA array \p dst starting at the
    /// upper left corner (\p wOffset, \p hOffset) where \p kind specifies the
    /// direction of the copy, and must be one of ::cudaMemcpyHostToHost,
    /// ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    /// \p spitch is the width in memory in bytes of the 2D array pointed to by
    /// \p src, including any padding added to the end of each row. \p wOffset +
    /// \p width must not exceed the width of the CUDA array \p dst. \p width must
    /// not exceed \p spitch. ::cudaMemcpy2DToArrayAsync() returns an error if
    /// \p spitch exceeds the maximum allowed.
    ///
    /// ::cudaMemcpy2DToArrayAsync() is asynchronous with respect to the host, so
    /// the call may return before the copy is complete. The copy can optionally
    /// be associated to a stream by passing a non-zero \p stream argument. If
    /// \p kind is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and
    /// \p stream is non-zero, the copy may overlap with operations in other
    /// streams.
    ///
    /// \param dst     - Destination memory address
    /// \param wOffset - Destination starting X offset
    /// \param hOffset - Destination starting Y offset
    /// \param src     - Source memory address
    /// \param spitch  - Pitch of source memory
    /// \param width   - Width of matrix transfer (columns in bytes)
    /// \param height  - Height of matrix transfer (rows)
    /// \param kind    - Type of transfer
    /// \param stream  - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidPitchValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpy2DAsync
    pub fn cudaMemcpy2DToArrayAsync(
        dst: cudaArray_t,
        wOffset: usize,
        hOffset: usize,
        src: *const ::std::os::raw::c_void,
        spitch: usize,
        width: usize,
        height: usize,
        kind: cudaMemcpyKind,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data between host and device
    ///
    /// Copies a matrix (\p height rows of \p width bytes each) from the CUDA
    /// array \p srcArray starting at the upper left corner
    /// (\p wOffset, \p hOffset) to the memory area pointed to by \p dst, where
    /// \p kind specifies the direction of the copy, and must be one of
    /// ::cudaMemcpyHostToHost, ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToHost,
    /// ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault. Passing
    /// ::cudaMemcpyDefault is recommended, in which case the type of transfer is
    /// inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    /// \p dpitch is the width in memory in bytes of the 2D
    /// array pointed to by \p dst, including any padding added to the end of each
    /// row. \p wOffset + \p width must not exceed the width of the CUDA array
    /// \p src. \p width must not exceed \p dpitch. ::cudaMemcpy2DFromArrayAsync()
    /// returns an error if \p dpitch exceeds the maximum allowed.
    ///
    /// ::cudaMemcpy2DFromArrayAsync() is asynchronous with respect to the host, so
    /// the call may return before the copy is complete. The copy can optionally be
    /// associated to a stream by passing a non-zero \p stream argument. If \p kind
    /// is ::cudaMemcpyHostToDevice or ::cudaMemcpyDeviceToHost and \p stream is
    /// non-zero, the copy may overlap with operations in other streams.
    ///
    /// \param dst     - Destination memory address
    /// \param dpitch  - Pitch of destination memory
    /// \param src     - Source memory address
    /// \param wOffset - Source starting X offset
    /// \param hOffset - Source starting Y offset
    /// \param width   - Width of matrix transfer (columns in bytes)
    /// \param height  - Height of matrix transfer (rows)
    /// \param kind    - Type of transfer
    /// \param stream  - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidPitchValue,
    /// ::cudaErrorInvalidMemcpyDirection
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync, ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpy2DAsync
    pub fn cudaMemcpy2DFromArrayAsync(
        dst: *mut ::std::os::raw::c_void,
        dpitch: usize,
        src: cudaArray_const_t,
        wOffset: usize,
        hOffset: usize,
        width: usize,
        height: usize,
        kind: cudaMemcpyKind,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data to the given symbol on the device
    ///
    /// Copies \p count bytes from the memory area pointed to by \p src
    /// to the memory area pointed to by \p offset bytes from the start of symbol
    /// \p symbol. The memory areas may not overlap. \p symbol is a variable that
    /// resides in global or constant memory space. \p kind can be either
    /// ::cudaMemcpyHostToDevice, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
    /// Passing ::cudaMemcpyDefault is recommended, in which case the type of transfer
    /// is inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    ///
    /// ::cudaMemcpyToSymbolAsync() is asynchronous with respect to the host, so
    /// the call may return before the copy is complete. The copy can optionally
    /// be associated to a stream by passing a non-zero \p stream argument. If
    /// \p kind is ::cudaMemcpyHostToDevice and \p stream is non-zero, the copy
    /// may overlap with operations in other streams.
    ///
    /// \param symbol - Device symbol address
    /// \param src    - Source memory address
    /// \param count  - Size in bytes to copy
    /// \param offset - Offset from start of symbol in bytes
    /// \param kind   - Type of transfer
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidSymbol,
    /// ::cudaErrorInvalidMemcpyDirection,
    /// ::cudaErrorNoKernelImageForDevice
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_string_api_deprecation
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyFromSymbolAsync,
    /// ::cuMemcpyAsync,
    /// ::cuMemcpyHtoDAsync,
    /// ::cuMemcpyDtoDAsync
    pub fn cudaMemcpyToSymbolAsync(
        symbol: *const ::std::os::raw::c_void,
        src: *const ::std::os::raw::c_void,
        count: usize,
        offset: usize,
        kind: cudaMemcpyKind,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Copies data from the given symbol on the device
    ///
    /// Copies \p count bytes from the memory area pointed to by \p offset bytes
    /// from the start of symbol \p symbol to the memory area pointed to by \p dst.
    /// The memory areas may not overlap. \p symbol is a variable that resides in
    /// global or constant memory space. \p kind can be either
    /// ::cudaMemcpyDeviceToHost, ::cudaMemcpyDeviceToDevice, or ::cudaMemcpyDefault.
    /// Passing ::cudaMemcpyDefault is recommended, in which case the type of transfer
    /// is inferred from the pointer values. However, ::cudaMemcpyDefault is only
    /// allowed on systems that support unified virtual addressing.
    ///
    /// ::cudaMemcpyFromSymbolAsync() is asynchronous with respect to the host, so
    /// the call may return before the copy is complete. The copy can optionally be
    /// associated to a stream by passing a non-zero \p stream argument. If \p kind
    /// is ::cudaMemcpyDeviceToHost and \p stream is non-zero, the copy may overlap
    /// with operations in other streams.
    ///
    /// \param dst    - Destination memory address
    /// \param symbol - Device symbol address
    /// \param count  - Size in bytes to copy
    /// \param offset - Offset from start of symbol in bytes
    /// \param kind   - Type of transfer
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidSymbol,
    /// ::cudaErrorInvalidMemcpyDirection,
    /// ::cudaErrorNoKernelImageForDevice
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_string_api_deprecation
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpy2D, ::cudaMemcpyToArray,
    /// ::cudaMemcpy2DToArray, ::cudaMemcpyFromArray, ::cudaMemcpy2DFromArray,
    /// ::cudaMemcpyArrayToArray, ::cudaMemcpy2DArrayToArray, ::cudaMemcpyToSymbol,
    /// ::cudaMemcpyFromSymbol, ::cudaMemcpyAsync, ::cudaMemcpy2DAsync,
    /// ::cudaMemcpyToArrayAsync, ::cudaMemcpy2DToArrayAsync,
    /// ::cudaMemcpyFromArrayAsync, ::cudaMemcpy2DFromArrayAsync,
    /// ::cudaMemcpyToSymbolAsync,
    /// ::cuMemcpyAsync,
    /// ::cuMemcpyDtoHAsync,
    /// ::cuMemcpyDtoDAsync
    pub fn cudaMemcpyFromSymbolAsync(
        dst: *mut ::std::os::raw::c_void,
        symbol: *const ::std::os::raw::c_void,
        count: usize,
        offset: usize,
        kind: cudaMemcpyKind,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Initializes or sets device memory to a value
    ///
    /// Fills the first \p count bytes of the memory area pointed to by \p devPtr
    /// with the constant byte value \p value.
    ///
    /// Note that this function is asynchronous with respect to the host unless
    /// \p devPtr refers to pinned host memory.
    ///
    /// \param devPtr - Pointer to device memory
    /// \param value  - Value to set for each byte of specified memory
    /// \param count  - Size in bytes to set
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// \notefnerr
    /// \note_memset
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cuMemsetD8,
    /// ::cuMemsetD16,
    /// ::cuMemsetD32
    pub fn cudaMemset(
        devPtr: *mut ::std::os::raw::c_void,
        value: ::std::os::raw::c_int,
        count: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Initializes or sets device memory to a value
    ///
    /// Sets to the specified value \p value a matrix (\p height rows of \p width
    /// bytes each) pointed to by \p dstPtr. \p pitch is the width in bytes of the
    /// 2D array pointed to by \p dstPtr, including any padding added to the end
    /// of each row. This function performs fastest when the pitch is one that has
    /// been passed back by ::cudaMallocPitch().
    ///
    /// Note that this function is asynchronous with respect to the host unless
    /// \p devPtr refers to pinned host memory.
    ///
    /// \param devPtr - Pointer to 2D device memory
    /// \param pitch  - Pitch in bytes of 2D device memory
    /// \param value  - Value to set for each byte of specified memory
    /// \param width  - Width of matrix set (columns in bytes)
    /// \param height - Height of matrix set (rows)
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// \notefnerr
    /// \note_memset
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemset, ::cudaMemset3D, ::cudaMemsetAsync,
    /// ::cudaMemset2DAsync, ::cudaMemset3DAsync,
    /// ::cuMemsetD2D8,
    /// ::cuMemsetD2D16,
    /// ::cuMemsetD2D32
    pub fn cudaMemset2D(
        devPtr: *mut ::std::os::raw::c_void,
        pitch: usize,
        value: ::std::os::raw::c_int,
        width: usize,
        height: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Initializes or sets device memory to a value
    ///
    /// Initializes each element of a 3D array to the specified value \p value.
    /// The object to initialize is defined by \p pitchedDevPtr. The \p pitch field
    /// of \p pitchedDevPtr is the width in memory in bytes of the 3D array pointed
    /// to by \p pitchedDevPtr, including any padding added to the end of each row.
    /// The \p xsize field specifies the logical width of each row in bytes, while
    /// the \p ysize field specifies the height of each 2D slice in rows.
    ///
    /// The extents of the initialized region are specified as a \p width in bytes,
    /// a \p height in rows, and a \p depth in slices.
    ///
    /// Extents with \p width greater than or equal to the \p xsize of
    /// \p pitchedDevPtr may perform significantly faster than extents narrower
    /// than the \p xsize. Secondarily, extents with \p height equal to the
    /// \p ysize of \p pitchedDevPtr will perform faster than when the \p height is
    /// shorter than the \p ysize.
    ///
    /// This function performs fastest when the \p pitchedDevPtr has been allocated
    /// by ::cudaMalloc3D().
    ///
    /// Note that this function is asynchronous with respect to the host unless
    /// \p pitchedDevPtr refers to pinned host memory.
    ///
    /// \param pitchedDevPtr - Pointer to pitched device memory
    /// \param value         - Value to set for each byte of specified memory
    /// \param extent        - Size parameters for where to set device memory (\p width field in bytes)
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// \notefnerr
    /// \note_memset
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemset, ::cudaMemset2D,
    /// ::cudaMemsetAsync, ::cudaMemset2DAsync, ::cudaMemset3DAsync,
    /// ::cudaMalloc3D, ::make_cudaPitchedPtr,
    /// ::make_cudaExtent
    pub fn cudaMemset3D(
        pitchedDevPtr: cudaPitchedPtr,
        value: ::std::os::raw::c_int,
        extent: cudaExtent,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Initializes or sets device memory to a value
    ///
    /// Fills the first \p count bytes of the memory area pointed to by \p devPtr
    /// with the constant byte value \p value.
    ///
    /// ::cudaMemsetAsync() is asynchronous with respect to the host, so
    /// the call may return before the memset is complete. The operation can optionally
    /// be associated to a stream by passing a non-zero \p stream argument.
    /// If \p stream is non-zero, the operation may overlap with operations in other streams.
    ///
    /// The device version of this function only handles device to device copies and
    /// cannot be given local or shared pointers.
    ///
    /// \param devPtr - Pointer to device memory
    /// \param value  - Value to set for each byte of specified memory
    /// \param count  - Size in bytes to set
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// \notefnerr
    /// \note_memset
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemset, ::cudaMemset2D, ::cudaMemset3D,
    /// ::cudaMemset2DAsync, ::cudaMemset3DAsync,
    /// ::cuMemsetD8Async,
    /// ::cuMemsetD16Async,
    /// ::cuMemsetD32Async
    pub fn cudaMemsetAsync(
        devPtr: *mut ::std::os::raw::c_void,
        value: ::std::os::raw::c_int,
        count: usize,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Initializes or sets device memory to a value
    ///
    /// Sets to the specified value \p value a matrix (\p height rows of \p width
    /// bytes each) pointed to by \p dstPtr. \p pitch is the width in bytes of the
    /// 2D array pointed to by \p dstPtr, including any padding added to the end
    /// of each row. This function performs fastest when the pitch is one that has
    /// been passed back by ::cudaMallocPitch().
    ///
    /// ::cudaMemset2DAsync() is asynchronous with respect to the host, so
    /// the call may return before the memset is complete. The operation can optionally
    /// be associated to a stream by passing a non-zero \p stream argument.
    /// If \p stream is non-zero, the operation may overlap with operations in other streams.
    ///
    /// The device version of this function only handles device to device copies and
    /// cannot be given local or shared pointers.
    ///
    /// \param devPtr - Pointer to 2D device memory
    /// \param pitch  - Pitch in bytes of 2D device memory
    /// \param value  - Value to set for each byte of specified memory
    /// \param width  - Width of matrix set (columns in bytes)
    /// \param height - Height of matrix set (rows)
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// \notefnerr
    /// \note_memset
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemset, ::cudaMemset2D, ::cudaMemset3D,
    /// ::cudaMemsetAsync, ::cudaMemset3DAsync,
    /// ::cuMemsetD2D8Async,
    /// ::cuMemsetD2D16Async,
    /// ::cuMemsetD2D32Async
    pub fn cudaMemset2DAsync(
        devPtr: *mut ::std::os::raw::c_void,
        pitch: usize,
        value: ::std::os::raw::c_int,
        width: usize,
        height: usize,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Initializes or sets device memory to a value
    ///
    /// Initializes each element of a 3D array to the specified value \p value.
    /// The object to initialize is defined by \p pitchedDevPtr. The \p pitch field
    /// of \p pitchedDevPtr is the width in memory in bytes of the 3D array pointed
    /// to by \p pitchedDevPtr, including any padding added to the end of each row.
    /// The \p xsize field specifies the logical width of each row in bytes, while
    /// the \p ysize field specifies the height of each 2D slice in rows.
    ///
    /// The extents of the initialized region are specified as a \p width in bytes,
    /// a \p height in rows, and a \p depth in slices.
    ///
    /// Extents with \p width greater than or equal to the \p xsize of
    /// \p pitchedDevPtr may perform significantly faster than extents narrower
    /// than the \p xsize. Secondarily, extents with \p height equal to the
    /// \p ysize of \p pitchedDevPtr will perform faster than when the \p height is
    /// shorter than the \p ysize.
    ///
    /// This function performs fastest when the \p pitchedDevPtr has been allocated
    /// by ::cudaMalloc3D().
    ///
    /// ::cudaMemset3DAsync() is asynchronous with respect to the host, so
    /// the call may return before the memset is complete. The operation can optionally
    /// be associated to a stream by passing a non-zero \p stream argument.
    /// If \p stream is non-zero, the operation may overlap with operations in other streams.
    ///
    /// The device version of this function only handles device to device copies and
    /// cannot be given local or shared pointers.
    ///
    /// \param pitchedDevPtr - Pointer to pitched device memory
    /// \param value         - Value to set for each byte of specified memory
    /// \param extent        - Size parameters for where to set device memory (\p width field in bytes)
    /// \param stream - Stream identifier
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// \notefnerr
    /// \note_memset
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemset, ::cudaMemset2D, ::cudaMemset3D,
    /// ::cudaMemsetAsync, ::cudaMemset2DAsync,
    /// ::cudaMalloc3D, ::make_cudaPitchedPtr,
    /// ::make_cudaExtent
    pub fn cudaMemset3DAsync(
        pitchedDevPtr: cudaPitchedPtr,
        value: ::std::os::raw::c_int,
        extent: cudaExtent,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Finds the address associated with a CUDA symbol
    ///
    /// Returns in \p *devPtr the address of symbol \p symbol on the device.
    /// \p symbol is a variable that resides in global or constant memory space.
    /// If \p symbol cannot be found, or if \p symbol is not declared in the
    /// global or constant memory space, \p *devPtr is unchanged and the error
    /// ::cudaErrorInvalidSymbol is returned.
    ///
    /// \param devPtr - Return device pointer associated with symbol
    /// \param symbol - Device symbol address
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidSymbol,
    /// ::cudaErrorNoKernelImageForDevice
    /// \notefnerr
    /// \note_string_api_deprecation
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// \ref ::cudaGetSymbolAddress(void**, const T&) "cudaGetSymbolAddress (C++ API)",
    /// \ref ::cudaGetSymbolSize(size_t*, const void*) "cudaGetSymbolSize (C API)",
    /// ::cuModuleGetGlobal
    pub fn cudaGetSymbolAddress(
        devPtr: *mut *mut ::std::os::raw::c_void,
        symbol: *const ::std::os::raw::c_void,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Finds the size of the object associated with a CUDA symbol
    ///
    /// Returns in \p *size the size of symbol \p symbol. \p symbol is a variable that
    /// resides in global or constant memory space. If \p symbol cannot be found, or
    /// if \p symbol is not declared in global or constant memory space, \p *size is
    /// unchanged and the error ::cudaErrorInvalidSymbol is returned.
    ///
    /// \param size   - Size of object associated with symbol
    /// \param symbol - Device symbol address
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidSymbol,
    /// ::cudaErrorNoKernelImageForDevice
    /// \notefnerr
    /// \note_string_api_deprecation
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// \ref ::cudaGetSymbolAddress(void**, const void*) "cudaGetSymbolAddress (C API)",
    /// \ref ::cudaGetSymbolSize(size_t*, const T&) "cudaGetSymbolSize (C++ API)",
    /// ::cuModuleGetGlobal
    pub fn cudaGetSymbolSize(
        size: *mut usize,
        symbol: *const ::std::os::raw::c_void,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Prefetches memory to the specified destination device
    ///
    /// Prefetches memory to the specified destination device.  \p devPtr is the
    /// base device pointer of the memory to be prefetched and \p dstDevice is the
    /// destination device. \p count specifies the number of bytes to copy. \p stream
    /// is the stream in which the operation is enqueued. The memory range must refer
    /// to managed memory allocated via ::cudaMallocManaged or declared via __managed__ variables.
    ///
    /// Passing in cudaCpuDeviceId for \p dstDevice will prefetch the data to host memory. If
    /// \p dstDevice is a GPU, then the device attribute ::cudaDevAttrConcurrentManagedAccess
    /// must be non-zero. Additionally, \p stream must be associated with a device that has a
    /// non-zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess.
    ///
    /// The start address and end address of the memory range will be rounded down and rounded up
    /// respectively to be aligned to CPU page size before the prefetch operation is enqueued
    /// in the stream.
    ///
    /// If no physical memory has been allocated for this region, then this memory region
    /// will be populated and mapped on the destination device. If there's insufficient
    /// memory to prefetch the desired region, the Unified Memory driver may evict pages from other
    /// ::cudaMallocManaged allocations to host memory in order to make room. Device memory
    /// allocated using ::cudaMalloc or ::cudaMallocArray will not be evicted.
    ///
    /// By default, any mappings to the previous location of the migrated pages are removed and
    /// mappings for the new location are only setup on \p dstDevice. The exact behavior however
    /// also depends on the settings applied to this memory range via ::cudaMemAdvise as described
    /// below:
    ///
    /// If ::cudaMemAdviseSetReadMostly was set on any subset of this memory range,
    /// then that subset will create a read-only copy of the pages on \p dstDevice.
    ///
    /// If ::cudaMemAdviseSetPreferredLocation was called on any subset of this memory
    /// range, then the pages will be migrated to \p dstDevice even if \p dstDevice is not the
    /// preferred location of any pages in the memory range.
    ///
    /// If ::cudaMemAdviseSetAccessedBy was called on any subset of this memory range,
    /// then mappings to those pages from all the appropriate processors are updated to
    /// refer to the new location if establishing such a mapping is possible. Otherwise,
    /// those mappings are cleared.
    ///
    /// Note that this API is not required for functionality and only serves to improve performance
    /// by allowing the application to migrate data to a suitable location before it is accessed.
    /// Memory accesses to this range are always coherent and are allowed even when the data is
    /// actively being migrated.
    ///
    /// Note that this function is asynchronous with respect to the host and all work
    /// on other devices.
    ///
    /// \param devPtr    - Pointer to be prefetched
    /// \param count     - Size in bytes
    /// \param dstDevice - Destination device to prefetch to
    /// \param stream    - Stream to enqueue prefetch operation
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync,
    /// ::cudaMemcpy3DPeerAsync, ::cudaMemAdvise,
    /// ::cuMemPrefetchAsync
    pub fn cudaMemPrefetchAsync(
        devPtr: *const ::std::os::raw::c_void,
        count: usize,
        dstDevice: ::std::os::raw::c_int,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Advise about the usage of a given memory range
    ///
    /// Advise the Unified Memory subsystem about the usage pattern for the memory range
    /// starting at \p devPtr with a size of \p count bytes. The start address and end address of the memory
    /// range will be rounded down and rounded up respectively to be aligned to CPU page size before the
    /// advice is applied. The memory range must refer to managed memory allocated via ::cudaMallocManaged
    /// or declared via __managed__ variables. The memory range could also refer to system-allocated pageable
    /// memory provided it represents a valid, host-accessible region of memory and all additional constraints
    /// imposed by \p advice as outlined below are also satisfied. Specifying an invalid system-allocated pageable
    /// memory range results in an error being returned.
    ///
    /// The \p advice parameter can take the following values:
    /// - ::cudaMemAdviseSetReadMostly: This implies that the data is mostly going to be read
    /// from and only occasionally written to. Any read accesses from any processor to this region will create a
    /// read-only copy of at least the accessed pages in that processor's memory. Additionally, if ::cudaMemPrefetchAsync
    /// is called on this region, it will create a read-only copy of the data on the destination processor.
    /// If any processor writes to this region, all copies of the corresponding page will be invalidated
    /// except for the one where the write occurred. The \p device argument is ignored for this advice.
    /// Note that for a page to be read-duplicated, the accessing processor must either be the CPU or a GPU
    /// that has a non-zero value for the device attribute ::cudaDevAttrConcurrentManagedAccess.
    /// Also, if a context is created on a device that does not have the device attribute
    /// ::cudaDevAttrConcurrentManagedAccess set, then read-duplication will not occur until
    /// all such contexts are destroyed.
    /// If the memory region refers to valid system-allocated pageable memory, then the accessing device must
    /// have a non-zero value for the device attribute ::cudaDevAttrPageableMemoryAccess for a read-only
    /// copy to be created on that device. Note however that if the accessing device also has a non-zero value for the
    /// device attribute ::cudaDevAttrPageableMemoryAccessUsesHostPageTables, then setting this advice
    /// will not create a read-only copy when that device accesses this memory region.
    ///
    /// - ::cudaMemAdviceUnsetReadMostly: Undoes the effect of ::cudaMemAdviceReadMostly and also prevents the
    /// Unified Memory driver from attempting heuristic read-duplication on the memory range. Any read-duplicated
    /// copies of the data will be collapsed into a single copy. The location for the collapsed
    /// copy will be the preferred location if the page has a preferred location and one of the read-duplicated
    /// copies was resident at that location. Otherwise, the location chosen is arbitrary.
    ///
    /// - ::cudaMemAdviseSetPreferredLocation: This advice sets the preferred location for the
    /// data to be the memory belonging to \p device. Passing in cudaCpuDeviceId for \p device sets the
    /// preferred location as host memory. If \p device is a GPU, then it must have a non-zero value for the
    /// device attribute ::cudaDevAttrConcurrentManagedAccess. Setting the preferred location
    /// does not cause data to migrate to that location immediately. Instead, it guides the migration policy
    /// when a fault occurs on that memory region. If the data is already in its preferred location and the
    /// faulting processor can establish a mapping without requiring the data to be migrated, then
    /// data migration will be avoided. On the other hand, if the data is not in its preferred location
    /// or if a direct mapping cannot be established, then it will be migrated to the processor accessing
    /// it. It is important to note that setting the preferred location does not prevent data prefetching
    /// done using ::cudaMemPrefetchAsync.
    /// Having a preferred location can override the page thrash detection and resolution logic in the Unified
    /// Memory driver. Normally, if a page is detected to be constantly thrashing between for example host and device
    /// memory, the page may eventually be pinned to host memory by the Unified Memory driver. But
    /// if the preferred location is set as device memory, then the page will continue to thrash indefinitely.
    /// If ::cudaMemAdviseSetReadMostly is also set on this memory region or any subset of it, then the
    /// policies associated with that advice will override the policies of this advice, unless read accesses from
    /// \p device will not result in a read-only copy being created on that device as outlined in description for
    /// the advice ::cudaMemAdviseSetReadMostly.
    /// If the memory region refers to valid system-allocated pageable memory, then \p device must have a non-zero
    /// value for the device attribute ::cudaDevAttrPageableMemoryAccess. Additionally, if \p device has
    /// a non-zero value for the device attribute ::cudaDevAttrPageableMemoryAccessUsesHostPageTables,
    /// then this call has no effect. Note however that this behavior may change in the future.
    ///
    /// - ::cudaMemAdviseUnsetPreferredLocation: Undoes the effect of ::cudaMemAdviseSetPreferredLocation
    /// and changes the preferred location to none.
    ///
    /// - ::cudaMemAdviseSetAccessedBy: This advice implies that the data will be accessed by \p device.
    /// Passing in ::cudaCpuDeviceId for \p device will set the advice for the CPU. If \p device is a GPU, then
    /// the device attribute ::cudaDevAttrConcurrentManagedAccess must be non-zero.
    /// This advice does not cause data migration and has no impact on the location of the data per se. Instead,
    /// it causes the data to always be mapped in the specified processor's page tables, as long as the
    /// location of the data permits a mapping to be established. If the data gets migrated for any reason,
    /// the mappings are updated accordingly.
    /// This advice is recommended in scenarios where data locality is not important, but avoiding faults is.
    /// Consider for example a system containing multiple GPUs with peer-to-peer access enabled, where the
    /// data located on one GPU is occasionally accessed by peer GPUs. In such scenarios, migrating data
    /// over to the other GPUs is not as important because the accesses are infrequent and the overhead of
    /// migration may be too high. But preventing faults can still help improve performance, and so having
    /// a mapping set up in advance is useful. Note that on CPU access of this data, the data may be migrated
    /// to host memory because the CPU typically cannot access device memory directly. Any GPU that had the
    /// ::cudaMemAdviceSetAccessedBy flag set for this data will now have its mapping updated to point to the
    /// page in host memory.
    /// If ::cudaMemAdviseSetReadMostly is also set on this memory region or any subset of it, then the
    /// policies associated with that advice will override the policies of this advice. Additionally, if the
    /// preferred location of this memory region or any subset of it is also \p device, then the policies
    /// associated with ::cudaMemAdviseSetPreferredLocation will override the policies of this advice.
    /// If the memory region refers to valid system-allocated pageable memory, then \p device must have a non-zero
    /// value for the device attribute ::cudaDevAttrPageableMemoryAccess. Additionally, if \p device has
    /// a non-zero value for the device attribute ::cudaDevAttrPageableMemoryAccessUsesHostPageTables,
    /// then this call has no effect.
    ///
    /// - ::cudaMemAdviseUnsetAccessedBy: Undoes the effect of ::cudaMemAdviseSetAccessedBy. Any mappings to
    /// the data from \p device may be removed at any time causing accesses to result in non-fatal page faults.
    /// If the memory region refers to valid system-allocated pageable memory, then \p device must have a non-zero
    /// value for the device attribute ::cudaDevAttrPageableMemoryAccess. Additionally, if \p device has
    /// a non-zero value for the device attribute ::cudaDevAttrPageableMemoryAccessUsesHostPageTables,
    /// then this call has no effect.
    ///
    /// \param devPtr - Pointer to memory to set the advice for
    /// \param count  - Size in bytes of the memory range
    /// \param advice - Advice to be applied for the specified memory range
    /// \param device - Device to apply the advice for
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemcpy, ::cudaMemcpyPeer, ::cudaMemcpyAsync,
    /// ::cudaMemcpy3DPeerAsync, ::cudaMemPrefetchAsync,
    /// ::cuMemAdvise
    pub fn cudaMemAdvise(
        devPtr: *const ::std::os::raw::c_void,
        count: usize,
        advice: cudaMemoryAdvise,
        device: ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Query an attribute of a given memory range
    ///
    /// Query an attribute about the memory range starting at \p devPtr with a size of \p count bytes. The
    /// memory range must refer to managed memory allocated via ::cudaMallocManaged or declared via
    /// __managed__ variables.
    ///
    /// The \p attribute parameter can take the following values:
    /// - ::cudaMemRangeAttributeReadMostly: If this attribute is specified, \p data will be interpreted
    /// as a 32-bit integer, and \p dataSize must be 4. The result returned will be 1 if all pages in the given
    /// memory range have read-duplication enabled, or 0 otherwise.
    /// - ::cudaMemRangeAttributePreferredLocation: If this attribute is specified, \p data will be
    /// interpreted as a 32-bit integer, and \p dataSize must be 4. The result returned will be a GPU device
    /// id if all pages in the memory range have that GPU as their preferred location, or it will be cudaCpuDeviceId
    /// if all pages in the memory range have the CPU as their preferred location, or it will be cudaInvalidDeviceId
    /// if either all the pages don't have the same preferred location or some of the pages don't have a
    /// preferred location at all. Note that the actual location of the pages in the memory range at the time of
    /// the query may be different from the preferred location.
    /// - ::cudaMemRangeAttributeAccessedBy: If this attribute is specified, \p data will be interpreted
    /// as an array of 32-bit integers, and \p dataSize must be a non-zero multiple of 4. The result returned
    /// will be a list of device ids that had ::cudaMemAdviceSetAccessedBy set for that entire memory range.
    /// If any device does not have that advice set for the entire memory range, that device will not be included.
    /// If \p data is larger than the number of devices that have that advice set for that memory range,
    /// cudaInvalidDeviceId will be returned in all the extra space provided. For ex., if \p dataSize is 12
    /// (i.e. \p data has 3 elements) and only device 0 has the advice set, then the result returned will be
    /// { 0, cudaInvalidDeviceId, cudaInvalidDeviceId }. If \p data is smaller than the number of devices that have
    /// that advice set, then only as many devices will be returned as can fit in the array. There is no
    /// guarantee on which specific devices will be returned, however.
    /// - ::cudaMemRangeAttributeLastPrefetchLocation: If this attribute is specified, \p data will be
    /// interpreted as a 32-bit integer, and \p dataSize must be 4. The result returned will be the last location
    /// to which all pages in the memory range were prefetched explicitly via ::cudaMemPrefetchAsync. This will either be
    /// a GPU id or cudaCpuDeviceId depending on whether the last location for prefetch was a GPU or the CPU
    /// respectively. If any page in the memory range was never explicitly prefetched or if all pages were not
    /// prefetched to the same location, cudaInvalidDeviceId will be returned. Note that this simply returns the
    /// last location that the applicaton requested to prefetch the memory range to. It gives no indication as to
    /// whether the prefetch operation to that location has completed or even begun.
    ///
    /// \param data      - A pointers to a memory location where the result
    ///                    of each attribute query will be written to.
    /// \param dataSize  - Array containing the size of data
    /// \param attribute - The attribute to query
    /// \param devPtr    - Start of the range to query
    /// \param count     - Size of the range to query
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_async
    /// \note_null_stream
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemRangeGetAttributes, ::cudaMemPrefetchAsync,
    /// ::cudaMemAdvise,
    /// ::cuMemRangeGetAttribute
    pub fn cudaMemRangeGetAttribute(
        data: *mut ::std::os::raw::c_void,
        dataSize: usize,
        attribute: cudaMemRangeAttribute,
        devPtr: *const ::std::os::raw::c_void,
        count: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Query attributes of a given memory range.
    ///
    /// Query attributes of the memory range starting at \p devPtr with a size of \p count bytes. The
    /// memory range must refer to managed memory allocated via ::cudaMallocManaged or declared via
    /// __managed__ variables. The \p attributes array will be interpreted to have \p numAttributes
    /// entries. The \p dataSizes array will also be interpreted to have \p numAttributes entries.
    /// The results of the query will be stored in \p data.
    ///
    /// The list of supported attributes are given below. Please refer to ::cudaMemRangeGetAttribute for
    /// attribute descriptions and restrictions.
    ///
    /// - ::cudaMemRangeAttributeReadMostly
    /// - ::cudaMemRangeAttributePreferredLocation
    /// - ::cudaMemRangeAttributeAccessedBy
    /// - ::cudaMemRangeAttributeLastPrefetchLocation
    ///
    /// \param data          - A two-dimensional array containing pointers to memory
    ///                        locations where the result of each attribute query will be written to.
    /// \param dataSizes     - Array containing the sizes of each result
    /// \param attributes    - An array of attributes to query
    ///                        (numAttributes and the number of attributes in this array should match)
    /// \param numAttributes - Number of attributes to query
    /// \param devPtr        - Start of the range to query
    /// \param count         - Size of the range to query
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaMemRangeGetAttribute, ::cudaMemAdvise
    /// ::cudaMemPrefetchAsync,
    /// ::cuMemRangeGetAttributes
    pub fn cudaMemRangeGetAttributes(
        data: *mut *mut ::std::os::raw::c_void,
        dataSizes: *mut usize,
        attributes: *mut cudaMemRangeAttribute,
        numAttributes: usize,
        devPtr: *const ::std::os::raw::c_void,
        count: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns attributes about a specified pointer
    ///
    /// Returns in \p *attributes the attributes of the pointer \p ptr.
    /// If pointer was not allocated in, mapped by or registered with context
    /// supporting unified addressing ::cudaErrorInvalidValue is returned.
    ///
    /// \note In CUDA 11.0 forward passing host pointer will return ::cudaMemoryTypeUnregistered
    /// in ::cudaPointerAttributes::type and call will return ::cudaSuccess.
    ///
    /// The ::cudaPointerAttributes structure is defined as:
    /// \code
    ///struct cudaPointerAttributes {
    ///enum cudaMemoryType memoryType;
    ///enum cudaMemoryType type;
    ///int device;
    ///void *devicePointer;
    ///void *hostPointer;
    ///int isManaged;
    ///}
    ///\endcode
    /// In this structure, the individual fields mean
    ///
    /// - \ref ::cudaPointerAttributes::memoryType identifies the
    ///   location of the memory associated with pointer \p ptr.  It can be
    ///   ::cudaMemoryTypeHost for host memory or ::cudaMemoryTypeDevice for device
    ///   and managed memory. It has been deprecated in favour of ::cudaPointerAttributes::type.
    ///
    /// - \ref ::cudaPointerAttributes::type identifies type of memory. It can be
    ///    ::cudaMemoryTypeUnregistered for unregistered host memory,
    ///    ::cudaMemoryTypeHost for registered host memory, ::cudaMemoryTypeDevice for device
    ///    memory or  ::cudaMemoryTypeManaged for managed memory.
    ///
    /// - \ref ::cudaPointerAttributes::device "device" is the device against which
    ///   \p ptr was allocated.  If \p ptr has memory type ::cudaMemoryTypeDevice
    ///   then this identifies the device on which the memory referred to by \p ptr
    ///   physically resides.  If \p ptr has memory type ::cudaMemoryTypeHost then this
    ///   identifies the device which was current when the allocation was made
    ///   (and if that device is deinitialized then this allocation will vanish
    ///   with that device's state).
    ///
    /// - \ref ::cudaPointerAttributes::devicePointer "devicePointer" is
    ///   the device pointer alias through which the memory referred to by \p ptr
    ///   may be accessed on the current device.
    ///   If the memory referred to by \p ptr cannot be accessed directly by the
    ///   current device then this is NULL.
    ///
    /// - \ref ::cudaPointerAttributes::hostPointer "hostPointer" is
    ///   the host pointer alias through which the memory referred to by \p ptr
    ///   may be accessed on the host.
    ///   If the memory referred to by \p ptr cannot be accessed directly by the
    ///   host then this is NULL.
    ///
    /// - \ref ::cudaPointerAttributes::isManaged "isManaged" indicates if
    ///   the pointer \p ptr points to managed memory or not. It has been deprecated
    ///   in favour of ::cudaPointerAttributes::type.
    ///
    /// \param attributes - Attributes for the specified pointer
    /// \param ptr        - Pointer to get attributes for
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevice,
    /// ::cudaErrorInvalidValue
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaGetDeviceCount, ::cudaGetDevice, ::cudaSetDevice,
    /// ::cudaChooseDevice,
    /// ::cuPointerGetAttributes
    pub fn cudaPointerGetAttributes(
        attributes: *mut cudaPointerAttributes,
        ptr: *const ::std::os::raw::c_void,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Queries if a device may directly access a peer device's memory.
    ///
    /// Returns in \p *canAccessPeer a value of 1 if device \p device is capable of
    /// directly accessing memory from \p peerDevice and 0 otherwise.  If direct
    /// access of \p peerDevice from \p device is possible, then access may be
    /// enabled by calling ::cudaDeviceEnablePeerAccess().
    ///
    /// \param canAccessPeer - Returned access capability
    /// \param device        - Device from which allocations on \p peerDevice are to
    ///                        be directly accessed.
    /// \param peerDevice    - Device on which the allocations to be directly accessed
    ///                        by \p device reside.
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceEnablePeerAccess,
    /// ::cudaDeviceDisablePeerAccess,
    /// ::cuDeviceCanAccessPeer
    pub fn cudaDeviceCanAccessPeer(
        canAccessPeer: *mut ::std::os::raw::c_int,
        device: ::std::os::raw::c_int,
        peerDevice: ::std::os::raw::c_int,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Enables direct access to memory allocations on a peer device.
    ///
    /// On success, all allocations from \p peerDevice will immediately be accessible by
    /// the current device.  They will remain accessible until access is explicitly
    /// disabled using ::cudaDeviceDisablePeerAccess() or either device is reset using
    /// ::cudaDeviceReset().
    ///
    /// Note that access granted by this call is unidirectional and that in order to access
    /// memory on the current device from \p peerDevice, a separate symmetric call
    /// to ::cudaDeviceEnablePeerAccess() is required.
    ///
    /// Each device can support a system-wide maximum of eight peer connections.
    ///
    /// Peer access is not supported in 32 bit applications.
    ///
    /// Returns ::cudaErrorInvalidDevice if ::cudaDeviceCanAccessPeer() indicates
    /// that the current device cannot directly access memory from \p peerDevice.
    ///
    /// Returns ::cudaErrorPeerAccessAlreadyEnabled if direct access of
    /// \p peerDevice from the current device has already been enabled.
    ///
    /// Returns ::cudaErrorInvalidValue if \p flags is not 0.
    ///
    /// \param peerDevice  - Peer device to enable direct access to from the current device
    /// \param flags       - Reserved for future use and must be set to 0
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidDevice,
    /// ::cudaErrorPeerAccessAlreadyEnabled,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceCanAccessPeer,
    /// ::cudaDeviceDisablePeerAccess,
    /// ::cuCtxEnablePeerAccess
    pub fn cudaDeviceEnablePeerAccess(
        peerDevice: ::std::os::raw::c_int,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Disables direct access to memory allocations on a peer device.
    ///
    /// Returns ::cudaErrorPeerAccessNotEnabled if direct access to memory on
    /// \p peerDevice has not yet been enabled from the current device.
    ///
    /// \param peerDevice - Peer device to disable direct access to
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorPeerAccessNotEnabled,
    /// ::cudaErrorInvalidDevice
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa ::cudaDeviceCanAccessPeer,
    /// ::cudaDeviceEnablePeerAccess,
    /// ::cuCtxDisablePeerAccess
    pub fn cudaDeviceDisablePeerAccess(peerDevice: ::std::os::raw::c_int) -> cudaError_t;
}
extern "C" {
    /// \brief Unregisters a graphics resource for access by CUDA
    ///
    /// Unregisters the graphics resource \p resource so it is not accessible by
    /// CUDA unless registered again.
    ///
    /// If \p resource is invalid then ::cudaErrorInvalidResourceHandle is
    /// returned.
    ///
    /// \param resource - Resource to unregister
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorUnknown
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphicsD3D9RegisterResource,
    /// ::cudaGraphicsD3D10RegisterResource,
    /// ::cudaGraphicsD3D11RegisterResource,
    /// ::cudaGraphicsGLRegisterBuffer,
    /// ::cudaGraphicsGLRegisterImage,
    /// ::cuGraphicsUnregisterResource
    pub fn cudaGraphicsUnregisterResource(resource: cudaGraphicsResource_t) -> cudaError_t;
}
extern "C" {
    /// \brief Set usage flags for mapping a graphics resource
    ///
    /// Set \p flags for mapping the graphics resource \p resource.
    ///
    /// Changes to \p flags will take effect the next time \p resource is mapped.
    /// The \p flags argument may be any of the following:
    /// - ::cudaGraphicsMapFlagsNone: Specifies no hints about how \p resource will
    ///     be used. It is therefore assumed that CUDA may read from or write to \p resource.
    /// - ::cudaGraphicsMapFlagsReadOnly: Specifies that CUDA will not write to \p resource.
    /// - ::cudaGraphicsMapFlagsWriteDiscard: Specifies CUDA will not read from \p resource and will
    ///   write over the entire contents of \p resource, so none of the data
    ///   previously stored in \p resource will be preserved.
    ///
    /// If \p resource is presently mapped for access by CUDA then ::cudaErrorUnknown is returned.
    /// If \p flags is not one of the above values then ::cudaErrorInvalidValue is returned.
    ///
    /// \param resource - Registered resource to set flags for
    /// \param flags    - Parameters for resource mapping
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorUnknown,
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphicsMapResources,
    /// ::cuGraphicsResourceSetMapFlags
    pub fn cudaGraphicsResourceSetMapFlags(
        resource: cudaGraphicsResource_t,
        flags: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Map graphics resources for access by CUDA
    ///
    /// Maps the \p count graphics resources in \p resources for access by CUDA.
    ///
    /// The resources in \p resources may be accessed by CUDA until they
    /// are unmapped. The graphics API from which \p resources were registered
    /// should not access any resources while they are mapped by CUDA. If an
    /// application does so, the results are undefined.
    ///
    /// This function provides the synchronization guarantee that any graphics calls
    /// issued before ::cudaGraphicsMapResources() will complete before any subsequent CUDA
    /// work issued in \p stream begins.
    ///
    /// If \p resources contains any duplicate entries then ::cudaErrorInvalidResourceHandle
    /// is returned. If any of \p resources are presently mapped for access by
    /// CUDA then ::cudaErrorUnknown is returned.
    ///
    /// \param count     - Number of resources to map
    /// \param resources - Resources to map for CUDA
    /// \param stream    - Stream for synchronization
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorUnknown
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphicsResourceGetMappedPointer,
    /// ::cudaGraphicsSubResourceGetMappedArray,
    /// ::cudaGraphicsUnmapResources,
    /// ::cuGraphicsMapResources
    pub fn cudaGraphicsMapResources(
        count: ::std::os::raw::c_int,
        resources: *mut cudaGraphicsResource_t,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Unmap graphics resources.
    ///
    /// Unmaps the \p count graphics resources in \p resources.
    ///
    /// Once unmapped, the resources in \p resources may not be accessed by CUDA
    /// until they are mapped again.
    ///
    /// This function provides the synchronization guarantee that any CUDA work issued
    /// in \p stream before ::cudaGraphicsUnmapResources() will complete before any
    /// subsequently issued graphics work begins.
    ///
    /// If \p resources contains any duplicate entries then ::cudaErrorInvalidResourceHandle
    /// is returned. If any of \p resources are not presently mapped for access by
    /// CUDA then ::cudaErrorUnknown is returned.
    ///
    /// \param count     - Number of resources to unmap
    /// \param resources - Resources to unmap
    /// \param stream    - Stream for synchronization
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorUnknown
    /// \note_null_stream
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphicsMapResources,
    /// ::cuGraphicsUnmapResources
    pub fn cudaGraphicsUnmapResources(
        count: ::std::os::raw::c_int,
        resources: *mut cudaGraphicsResource_t,
        stream: cudaStream_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Get an device pointer through which to access a mapped graphics resource.
    ///
    /// Returns in \p *devPtr a pointer through which the mapped graphics resource
    /// \p resource may be accessed.
    /// Returns in \p *size the size of the memory in bytes which may be accessed from that pointer.
    /// The value set in \p devPtr may change every time that \p resource is mapped.
    ///
    /// If \p resource is not a buffer then it cannot be accessed via a pointer and
    /// ::cudaErrorUnknown is returned.
    /// If \p resource is not mapped then ::cudaErrorUnknown is returned.
    /// *
    /// \param devPtr     - Returned pointer through which \p resource may be accessed
    /// \param size       - Returned size of the buffer accessible starting at \p *devPtr
    /// \param resource   - Mapped resource to access
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorUnknown
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphicsMapResources,
    /// ::cudaGraphicsSubResourceGetMappedArray,
    /// ::cuGraphicsResourceGetMappedPointer
    pub fn cudaGraphicsResourceGetMappedPointer(
        devPtr: *mut *mut ::std::os::raw::c_void,
        size: *mut usize,
        resource: cudaGraphicsResource_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Get an array through which to access a subresource of a mapped graphics resource.
    ///
    /// Returns in \p *array an array through which the subresource of the mapped
    /// graphics resource \p resource which corresponds to array index \p arrayIndex
    /// and mipmap level \p mipLevel may be accessed.  The value set in \p array may
    /// change every time that \p resource is mapped.
    ///
    /// If \p resource is not a texture then it cannot be accessed via an array and
    /// ::cudaErrorUnknown is returned.
    /// If \p arrayIndex is not a valid array index for \p resource then
    /// ::cudaErrorInvalidValue is returned.
    /// If \p mipLevel is not a valid mipmap level for \p resource then
    /// ::cudaErrorInvalidValue is returned.
    /// If \p resource is not mapped then ::cudaErrorUnknown is returned.
    ///
    /// \param array       - Returned array through which a subresource of \p resource may be accessed
    /// \param resource    - Mapped resource to access
    /// \param arrayIndex  - Array index for array textures or cubemap face
    ///                      index as defined by ::cudaGraphicsCubeFace for
    ///                      cubemap textures for the subresource to access
    /// \param mipLevel    - Mipmap level for the subresource to access
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorUnknown
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphicsResourceGetMappedPointer,
    /// ::cuGraphicsSubResourceGetMappedArray
    pub fn cudaGraphicsSubResourceGetMappedArray(
        array: *mut cudaArray_t,
        resource: cudaGraphicsResource_t,
        arrayIndex: ::std::os::raw::c_uint,
        mipLevel: ::std::os::raw::c_uint,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Get a mipmapped array through which to access a mapped graphics resource.
    ///
    /// Returns in \p *mipmappedArray a mipmapped array through which the mapped
    /// graphics resource \p resource may be accessed. The value set in \p mipmappedArray may
    /// change every time that \p resource is mapped.
    ///
    /// If \p resource is not a texture then it cannot be accessed via an array and
    /// ::cudaErrorUnknown is returned.
    /// If \p resource is not mapped then ::cudaErrorUnknown is returned.
    ///
    /// \param mipmappedArray - Returned mipmapped array through which \p resource may be accessed
    /// \param resource       - Mapped resource to access
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorUnknown
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphicsResourceGetMappedPointer,
    /// ::cuGraphicsResourceGetMappedMipmappedArray
    pub fn cudaGraphicsResourceGetMappedMipmappedArray(
        mipmappedArray: *mut cudaMipmappedArray_t,
        resource: cudaGraphicsResource_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Get the channel descriptor of an array
    ///
    /// Returns in \p *desc the channel descriptor of the CUDA array \p array.
    ///
    /// \param desc  - Channel format
    /// \param array - Memory array on device
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
    /// ::cudaGetTextureReference,
    /// \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
    /// \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
    /// \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
    /// \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
    /// \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)"
    pub fn cudaGetChannelDesc(
        desc: *mut cudaChannelFormatDesc,
        array: cudaArray_const_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a channel descriptor using the specified format
    ///
    /// Returns a channel descriptor with format \p f and number of bits of each
    /// component \p x, \p y, \p z, and \p w.  The ::cudaChannelFormatDesc is
    /// defined as:
    /// \code
    ///struct cudaChannelFormatDesc {
    ///int x, y, z, w;
    ///enum cudaChannelFormatKind f;
    ///};
    /// \endcode
    ///
    /// where ::cudaChannelFormatKind is one of ::cudaChannelFormatKindSigned,
    /// ::cudaChannelFormatKindUnsigned, or ::cudaChannelFormatKindFloat.
    ///
    /// \param x - X component
    /// \param y - Y component
    /// \param z - Z component
    /// \param w - W component
    /// \param f - Channel format
    ///
    /// \return
    /// Channel descriptor with format \p f
    ///
    /// \sa \ref ::cudaCreateChannelDesc(void) "cudaCreateChannelDesc (C++ API)",
    /// ::cudaGetChannelDesc, ::cudaGetTextureReference,
    /// \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
    /// \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
    /// \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
    /// \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
    /// \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)",
    /// ::cuTexRefSetFormat
    pub fn cudaCreateChannelDesc(
        x: ::std::os::raw::c_int,
        y: ::std::os::raw::c_int,
        z: ::std::os::raw::c_int,
        w: ::std::os::raw::c_int,
        f: cudaChannelFormatKind,
    ) -> cudaChannelFormatDesc;
}
extern "C" {
    /// \brief Binds a memory area to a texture
    ///
    /// Binds \p size bytes of the memory area pointed to by \p devPtr to the
    /// texture reference \p texref. \p desc describes how the memory is interpreted
    /// when fetching values from the texture. Any memory previously bound to
    /// \p texref is unbound.
    ///
    /// Since the hardware enforces an alignment requirement on texture base
    /// addresses,
    /// \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture()"
    /// returns in \p *offset a byte offset that
    /// must be applied to texture fetches in order to read from the desired memory.
    /// This offset must be divided by the texel size and passed to kernels that
    /// read from the texture so they can be applied to the ::tex1Dfetch() function.
    /// If the device memory pointer was returned from ::cudaMalloc(), the offset is
    /// guaranteed to be 0 and NULL may be passed as the \p offset parameter.
    ///
    /// The total number of elements (or texels) in the linear address range
    /// cannot exceed ::cudaDeviceProp::maxTexture1DLinear[0].
    /// The number of elements is computed as (\p size / elementSize),
    /// where elementSize is determined from \p desc.
    ///
    /// \param offset - Offset in bytes
    /// \param texref - Texture to bind
    /// \param devPtr - Memory area on device
    /// \param desc   - Channel format
    /// \param size   - Size of the memory area pointed to by devPtr
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidTexture
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
    /// ::cudaGetChannelDesc, ::cudaGetTextureReference,
    /// \ref ::cudaBindTexture(size_t*, const struct texture< T, dim, readMode>&, const void*, const struct cudaChannelFormatDesc&, size_t) "cudaBindTexture (C++ API)",
    /// \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
    /// \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
    /// \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
    /// \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)",
    /// ::cuTexRefSetAddress,
    /// ::cuTexRefSetAddressMode,
    /// ::cuTexRefSetFormat,
    /// ::cuTexRefSetFlags,
    /// ::cuTexRefSetBorderColor
    pub fn cudaBindTexture(
        offset: *mut usize,
        texref: *const textureReference,
        devPtr: *const ::std::os::raw::c_void,
        desc: *const cudaChannelFormatDesc,
        size: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Binds a 2D memory area to a texture
    ///
    /// Binds the 2D memory area pointed to by \p devPtr to the
    /// texture reference \p texref. The size of the area is constrained by
    /// \p width in texel units, \p height in texel units, and \p pitch in byte
    /// units. \p desc describes how the memory is interpreted when fetching values
    /// from the texture. Any memory previously bound to \p texref is unbound.
    ///
    /// Since the hardware enforces an alignment requirement on texture base
    /// addresses, ::cudaBindTexture2D() returns in \p *offset a byte offset that
    /// must be applied to texture fetches in order to read from the desired memory.
    /// This offset must be divided by the texel size and passed to kernels that
    /// read from the texture so they can be applied to the ::tex2D() function.
    /// If the device memory pointer was returned from ::cudaMalloc(), the offset is
    /// guaranteed to be 0 and NULL may be passed as the \p offset parameter.
    ///
    /// \p width and \p height, which are specified in elements (or texels), cannot
    /// exceed ::cudaDeviceProp::maxTexture2DLinear[0] and ::cudaDeviceProp::maxTexture2DLinear[1]
    /// respectively. \p pitch, which is specified in bytes, cannot exceed
    /// ::cudaDeviceProp::maxTexture2DLinear[2].
    ///
    /// The driver returns ::cudaErrorInvalidValue if \p pitch is not a multiple of
    /// ::cudaDeviceProp::texturePitchAlignment.
    ///
    /// \param offset - Offset in bytes
    /// \param texref - Texture reference to bind
    /// \param devPtr - 2D memory area on device
    /// \param desc   - Channel format
    /// \param width  - Width in texel units
    /// \param height - Height in texel units
    /// \param pitch  - Pitch in bytes
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidTexture
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
    /// ::cudaGetChannelDesc, ::cudaGetTextureReference,
    /// \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
    /// \ref ::cudaBindTexture2D(size_t*, const struct texture< T, dim, readMode>&, const void*, const struct cudaChannelFormatDesc&, size_t, size_t, size_t) "cudaBindTexture2D (C++ API)",
    /// \ref ::cudaBindTexture2D(size_t*, const struct texture<T, dim, readMode>&, const void*, size_t, size_t, size_t) "cudaBindTexture2D (C++ API, inherited channel descriptor)",
    /// \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
    /// \ref ::cudaUnbindTexture(const struct textureReference*) "cudaBindTextureToArray (C API)",
    /// \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)",
    /// ::cuTexRefSetAddress2D,
    /// ::cuTexRefSetFormat,
    /// ::cuTexRefSetFlags,
    /// ::cuTexRefSetAddressMode,
    /// ::cuTexRefSetBorderColor
    pub fn cudaBindTexture2D(
        offset: *mut usize,
        texref: *const textureReference,
        devPtr: *const ::std::os::raw::c_void,
        desc: *const cudaChannelFormatDesc,
        width: usize,
        height: usize,
        pitch: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Binds an array to a texture
    ///
    /// Binds the CUDA array \p array to the texture reference \p texref.
    /// \p desc describes how the memory is interpreted when fetching values from
    /// the texture. Any CUDA array previously bound to \p texref is unbound.
    ///
    /// \param texref - Texture to bind
    /// \param array  - Memory array on device
    /// \param desc   - Channel format
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidTexture
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
    /// ::cudaGetChannelDesc, ::cudaGetTextureReference,
    /// \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
    /// \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
    /// \ref ::cudaBindTextureToArray(const struct texture< T, dim, readMode>&, cudaArray_const_t, const struct cudaChannelFormatDesc&) "cudaBindTextureToArray (C++ API)",
    /// \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
    /// \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)",
    /// ::cuTexRefSetArray,
    /// ::cuTexRefSetFormat,
    /// ::cuTexRefSetFlags,
    /// ::cuTexRefSetAddressMode,
    /// ::cuTexRefSetFilterMode,
    /// ::cuTexRefSetBorderColor,
    /// ::cuTexRefSetMaxAnisotropy
    pub fn cudaBindTextureToArray(
        texref: *const textureReference,
        array: cudaArray_const_t,
        desc: *const cudaChannelFormatDesc,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Binds a mipmapped array to a texture
    ///
    /// Binds the CUDA mipmapped array \p mipmappedArray to the texture reference \p texref.
    /// \p desc describes how the memory is interpreted when fetching values from
    /// the texture. Any CUDA mipmapped array previously bound to \p texref is unbound.
    ///
    /// \param texref         - Texture to bind
    /// \param mipmappedArray - Memory mipmapped array on device
    /// \param desc           - Channel format
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidTexture
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
    /// ::cudaGetChannelDesc, ::cudaGetTextureReference,
    /// \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
    /// \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
    /// \ref ::cudaBindTextureToArray(const struct texture< T, dim, readMode>&, cudaArray_const_t, const struct cudaChannelFormatDesc&) "cudaBindTextureToArray (C++ API)",
    /// \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
    /// \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)",
    /// ::cuTexRefSetMipmappedArray,
    /// ::cuTexRefSetMipmapFilterMode
    /// ::cuTexRefSetMipmapLevelClamp,
    /// ::cuTexRefSetMipmapLevelBias,
    /// ::cuTexRefSetFormat,
    /// ::cuTexRefSetFlags,
    /// ::cuTexRefSetAddressMode,
    /// ::cuTexRefSetBorderColor,
    /// ::cuTexRefSetMaxAnisotropy
    pub fn cudaBindTextureToMipmappedArray(
        texref: *const textureReference,
        mipmappedArray: cudaMipmappedArray_const_t,
        desc: *const cudaChannelFormatDesc,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Unbinds a texture
    ///
    /// Unbinds the texture bound to \p texref. If \p texref is not currently bound, no operation is performed.
    ///
    /// \param texref - Texture to unbind
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidTexture
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
    /// ::cudaGetChannelDesc, ::cudaGetTextureReference,
    /// \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
    /// \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
    /// \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
    /// \ref ::cudaUnbindTexture(const struct texture< T, dim, readMode>&) "cudaUnbindTexture (C++ API)",
    /// \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)"
    pub fn cudaUnbindTexture(texref: *const textureReference) -> cudaError_t;
}
extern "C" {
    /// \brief Get the alignment offset of a texture
    ///
    /// Returns in \p *offset the offset that was returned when texture reference
    /// \p texref was bound.
    ///
    /// \param offset - Offset of texture reference in bytes
    /// \param texref - Texture to get offset of
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidTexture,
    /// ::cudaErrorInvalidTextureBinding
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
    /// ::cudaGetChannelDesc, ::cudaGetTextureReference,
    /// \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
    /// \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
    /// \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
    /// \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
    /// \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct texture< T, dim, readMode>&) "cudaGetTextureAlignmentOffset (C++ API)"
    pub fn cudaGetTextureAlignmentOffset(
        offset: *mut usize,
        texref: *const textureReference,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Get the texture reference associated with a symbol
    ///
    /// Returns in \p *texref the structure associated to the texture reference
    /// defined by symbol \p symbol.
    ///
    /// \param texref - Texture reference associated with symbol
    /// \param symbol - Texture to get reference for
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidTexture
    /// \notefnerr
    /// \note_string_api_deprecation_50
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind) "cudaCreateChannelDesc (C API)",
    /// ::cudaGetChannelDesc,
    /// \ref ::cudaGetTextureAlignmentOffset(size_t*, const struct textureReference*) "cudaGetTextureAlignmentOffset (C API)",
    /// \ref ::cudaBindTexture(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t) "cudaBindTexture (C API)",
    /// \ref ::cudaBindTexture2D(size_t*, const struct textureReference*, const void*, const struct cudaChannelFormatDesc*, size_t, size_t, size_t) "cudaBindTexture2D (C API)",
    /// \ref ::cudaBindTextureToArray(const struct textureReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindTextureToArray (C API)",
    /// \ref ::cudaUnbindTexture(const struct textureReference*) "cudaUnbindTexture (C API)",
    /// ::cuModuleGetTexRef
    pub fn cudaGetTextureReference(
        texref: *mut *const textureReference,
        symbol: *const ::std::os::raw::c_void,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Binds an array to a surface
    ///
    /// Binds the CUDA array \p array to the surface reference \p surfref.
    /// \p desc describes how the memory is interpreted when fetching values from
    /// the surface. Any CUDA array previously bound to \p surfref is unbound.
    ///
    /// \param surfref - Surface to bind
    /// \param array  - Memory array on device
    /// \param desc   - Channel format
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidSurface
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa \ref ::cudaBindSurfaceToArray(const struct surface< T, dim>&, cudaArray_const_t, const struct cudaChannelFormatDesc&) "cudaBindSurfaceToArray (C++ API)",
    /// \ref ::cudaBindSurfaceToArray(const struct surface< T, dim>&, cudaArray_const_t) "cudaBindSurfaceToArray (C++ API, inherited channel descriptor)",
    /// ::cudaGetSurfaceReference,
    /// ::cuSurfRefSetArray
    pub fn cudaBindSurfaceToArray(
        surfref: *const surfaceReference,
        array: cudaArray_const_t,
        desc: *const cudaChannelFormatDesc,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Get the surface reference associated with a symbol
    ///
    /// Returns in \p *surfref the structure associated to the surface reference
    /// defined by symbol \p symbol.
    ///
    /// \param surfref - Surface reference associated with symbol
    /// \param symbol - Surface to get reference for
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidSurface
    /// \notefnerr
    /// \note_string_api_deprecation_50
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// \ref ::cudaBindSurfaceToArray(const struct surfaceReference*, cudaArray_const_t, const struct cudaChannelFormatDesc*) "cudaBindSurfaceToArray (C API)",
    /// ::cuModuleGetSurfRef
    pub fn cudaGetSurfaceReference(
        surfref: *mut *const surfaceReference,
        symbol: *const ::std::os::raw::c_void,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Creates a texture object
    ///
    /// Creates a texture object and returns it in \p pTexObject. \p pResDesc describes
    /// the data to texture from. \p pTexDesc describes how the data should be sampled.
    /// \p pResViewDesc is an optional argument that specifies an alternate format for
    /// the data described by \p pResDesc, and also describes the subresource region
    /// to restrict access to when texturing. \p pResViewDesc can only be specified if
    /// the type of resource is a CUDA array or a CUDA mipmapped array.
    ///
    /// Texture objects are only supported on devices of compute capability 3.0 or higher.
    /// Additionally, a texture object is an opaque value, and, as such, should only be
    /// accessed through CUDA API calls.
    ///
    /// The ::cudaResourceDesc structure is defined as:
    /// \code
    ///struct cudaResourceDesc {
    ///enum cudaResourceType resType;
    ///
    ///union {
    ///struct {
    ///cudaArray_t array;
    ///} array;
    ///struct {
    ///cudaMipmappedArray_t mipmap;
    ///} mipmap;
    ///struct {
    ///void *devPtr;
    ///struct cudaChannelFormatDesc desc;
    ///size_t sizeInBytes;
    ///} linear;
    ///struct {
    ///void *devPtr;
    ///struct cudaChannelFormatDesc desc;
    ///size_t width;
    ///size_t height;
    ///size_t pitchInBytes;
    ///} pitch2D;
    ///} res;
    ///};
    /// \endcode
    /// where:
    /// - ::cudaResourceDesc::resType specifies the type of resource to texture from.
    /// CUresourceType is defined as:
    /// \code
    ///enum cudaResourceType {
    ///cudaResourceTypeArray          = 0x00,
    ///cudaResourceTypeMipmappedArray = 0x01,
    ///cudaResourceTypeLinear         = 0x02,
    ///cudaResourceTypePitch2D        = 0x03
    ///};
    /// \endcode
    ///
    /// \par
    /// If ::cudaResourceDesc::resType is set to ::cudaResourceTypeArray, ::cudaResourceDesc::res::array::array
    /// must be set to a valid CUDA array handle.
    ///
    /// \par
    /// If ::cudaResourceDesc::resType is set to ::cudaResourceTypeMipmappedArray, ::cudaResourceDesc::res::mipmap::mipmap
    /// must be set to a valid CUDA mipmapped array handle and ::cudaTextureDesc::normalizedCoords must be set to true.
    ///
    /// \par
    /// If ::cudaResourceDesc::resType is set to ::cudaResourceTypeLinear, ::cudaResourceDesc::res::linear::devPtr
    /// must be set to a valid device pointer, that is aligned to ::cudaDeviceProp::textureAlignment.
    /// ::cudaResourceDesc::res::linear::desc describes the format and the number of components per array element. ::cudaResourceDesc::res::linear::sizeInBytes
    /// specifies the size of the array in bytes. The total number of elements in the linear address range cannot exceed
    /// ::cudaDeviceProp::maxTexture1DLinear. The number of elements is computed as (sizeInBytes / sizeof(desc)).
    ///
    /// \par
    /// If ::cudaResourceDesc::resType is set to ::cudaResourceTypePitch2D, ::cudaResourceDesc::res::pitch2D::devPtr
    /// must be set to a valid device pointer, that is aligned to ::cudaDeviceProp::textureAlignment.
    /// ::cudaResourceDesc::res::pitch2D::desc describes the format and the number of components per array element. ::cudaResourceDesc::res::pitch2D::width
    /// and ::cudaResourceDesc::res::pitch2D::height specify the width and height of the array in elements, and cannot exceed
    /// ::cudaDeviceProp::maxTexture2DLinear[0] and ::cudaDeviceProp::maxTexture2DLinear[1] respectively.
    /// ::cudaResourceDesc::res::pitch2D::pitchInBytes specifies the pitch between two rows in bytes and has to be aligned to
    /// ::cudaDeviceProp::texturePitchAlignment. Pitch cannot exceed ::cudaDeviceProp::maxTexture2DLinear[2].
    ///
    ///
    /// The ::cudaTextureDesc struct is defined as
    /// \code
    ///struct cudaTextureDesc {
    ///enum cudaTextureAddressMode addressMode[3];
    ///enum cudaTextureFilterMode  filterMode;
    ///enum cudaTextureReadMode    readMode;
    ///int                         sRGB;
    ///float                       borderColor[4];
    ///int                         normalizedCoords;
    ///unsigned int                maxAnisotropy;
    ///enum cudaTextureFilterMode  mipmapFilterMode;
    ///float                       mipmapLevelBias;
    ///float                       minMipmapLevelClamp;
    ///float                       maxMipmapLevelClamp;
    ///};
    /// \endcode
    /// where
    /// - ::cudaTextureDesc::addressMode specifies the addressing mode for each dimension of the texture data. ::cudaTextureAddressMode is defined as:
    ///   \code
    ///enum cudaTextureAddressMode {
    ///cudaAddressModeWrap   = 0,
    ///cudaAddressModeClamp  = 1,
    ///cudaAddressModeMirror = 2,
    ///cudaAddressModeBorder = 3
    ///};
    ///   \endcode
    ///   This is ignored if ::cudaResourceDesc::resType is ::cudaResourceTypeLinear. Also, if ::cudaTextureDesc::normalizedCoords
    ///   is set to zero, ::cudaAddressModeWrap and ::cudaAddressModeMirror won't be supported and will be switched to ::cudaAddressModeClamp.
    ///
    /// - ::cudaTextureDesc::filterMode specifies the filtering mode to be used when fetching from the texture. ::cudaTextureFilterMode is defined as:
    ///   \code
    ///enum cudaTextureFilterMode {
    ///cudaFilterModePoint  = 0,
    ///cudaFilterModeLinear = 1
    ///};
    ///   \endcode
    ///   This is ignored if ::cudaResourceDesc::resType is ::cudaResourceTypeLinear.
    ///
    /// - ::cudaTextureDesc::readMode specifies whether integer data should be converted to floating point or not. ::cudaTextureReadMode is defined as:
    ///   \code
    ///enum cudaTextureReadMode {
    ///cudaReadModeElementType     = 0,
    ///cudaReadModeNormalizedFloat = 1
    ///};
    ///   \endcode
    ///   Note that this applies only to 8-bit and 16-bit integer formats. 32-bit integer format would not be promoted, regardless of
    ///   whether or not this ::cudaTextureDesc::readMode is set ::cudaReadModeNormalizedFloat is specified.
    ///
    /// - ::cudaTextureDesc::sRGB specifies whether sRGB to linear conversion should be performed during texture fetch.
    ///
    /// - ::cudaTextureDesc::borderColor specifies the float values of color. where:
    ///   ::cudaTextureDesc::borderColor[0] contains value of 'R',
    ///   ::cudaTextureDesc::borderColor[1] contains value of 'G',
    ///   ::cudaTextureDesc::borderColor[2] contains value of 'B',
    ///   ::cudaTextureDesc::borderColor[3] contains value of 'A'
    ///   Note that application using integer border color values will need to <reinterpret_cast> these values to float.
    ///   The values are set only when the addressing mode specified by ::cudaTextureDesc::addressMode is cudaAddressModeBorder.
    ///
    /// - ::cudaTextureDesc::normalizedCoords specifies whether the texture coordinates will be normalized or not.
    ///
    /// - ::cudaTextureDesc::maxAnisotropy specifies the maximum anistropy ratio to be used when doing anisotropic filtering. This value will be
    ///   clamped to the range [1,16].
    ///
    /// - ::cudaTextureDesc::mipmapFilterMode specifies the filter mode when the calculated mipmap level lies between two defined mipmap levels.
    ///
    /// - ::cudaTextureDesc::mipmapLevelBias specifies the offset to be applied to the calculated mipmap level.
    ///
    /// - ::cudaTextureDesc::minMipmapLevelClamp specifies the lower end of the mipmap level range to clamp access to.
    ///
    /// - ::cudaTextureDesc::maxMipmapLevelClamp specifies the upper end of the mipmap level range to clamp access to.
    ///
    ///
    /// The ::cudaResourceViewDesc struct is defined as
    /// \code
    ///struct cudaResourceViewDesc {
    ///enum cudaResourceViewFormat format;
    ///size_t                      width;
    ///size_t                      height;
    ///size_t                      depth;
    ///unsigned int                firstMipmapLevel;
    ///unsigned int                lastMipmapLevel;
    ///unsigned int                firstLayer;
    ///unsigned int                lastLayer;
    ///};
    /// \endcode
    /// where:
    /// - ::cudaResourceViewDesc::format specifies how the data contained in the CUDA array or CUDA mipmapped array should
    ///   be interpreted. Note that this can incur a change in size of the texture data. If the resource view format is a block
    ///   compressed format, then the underlying CUDA array or CUDA mipmapped array has to have a 32-bit unsigned integer format
    ///   with 2 or 4 channels, depending on the block compressed format. For ex., BC1 and BC4 require the underlying CUDA array to have
    ///   a 32-bit unsigned int with 2 channels. The other BC formats require the underlying resource to have the same 32-bit unsigned int
    ///   format but with 4 channels.
    ///
    /// - ::cudaResourceViewDesc::width specifies the new width of the texture data. If the resource view format is a block
    ///   compressed format, this value has to be 4 times the original width of the resource. For non block compressed formats,
    ///   this value has to be equal to that of the original resource.
    ///
    /// - ::cudaResourceViewDesc::height specifies the new height of the texture data. If the resource view format is a block
    ///   compressed format, this value has to be 4 times the original height of the resource. For non block compressed formats,
    ///   this value has to be equal to that of the original resource.
    ///
    /// - ::cudaResourceViewDesc::depth specifies the new depth of the texture data. This value has to be equal to that of the
    ///   original resource.
    ///
    /// - ::cudaResourceViewDesc::firstMipmapLevel specifies the most detailed mipmap level. This will be the new mipmap level zero.
    ///   For non-mipmapped resources, this value has to be zero.::cudaTextureDesc::minMipmapLevelClamp and ::cudaTextureDesc::maxMipmapLevelClamp
    ///   will be relative to this value. For ex., if the firstMipmapLevel is set to 2, and a minMipmapLevelClamp of 1.2 is specified,
    ///   then the actual minimum mipmap level clamp will be 3.2.
    ///
    /// - ::cudaResourceViewDesc::lastMipmapLevel specifies the least detailed mipmap level. For non-mipmapped resources, this value
    ///   has to be zero.
    ///
    /// - ::cudaResourceViewDesc::firstLayer specifies the first layer index for layered textures. This will be the new layer zero.
    ///   For non-layered resources, this value has to be zero.
    ///
    /// - ::cudaResourceViewDesc::lastLayer specifies the last layer index for layered textures. For non-layered resources,
    ///   this value has to be zero.
    ///
    ///
    /// \param pTexObject   - Texture object to create
    /// \param pResDesc     - Resource descriptor
    /// \param pTexDesc     - Texture descriptor
    /// \param pResViewDesc - Resource view descriptor
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaDestroyTextureObject,
    /// ::cuTexObjectCreate
    pub fn cudaCreateTextureObject(
        pTexObject: *mut cudaTextureObject_t,
        pResDesc: *const cudaResourceDesc,
        pTexDesc: *const cudaTextureDesc,
        pResViewDesc: *const cudaResourceViewDesc,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Destroys a texture object
    ///
    /// Destroys the texture object specified by \p texObject.
    ///
    /// \param texObject - Texture object to destroy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaCreateTextureObject,
    /// ::cuTexObjectDestroy
    pub fn cudaDestroyTextureObject(texObject: cudaTextureObject_t) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a texture object's resource descriptor
    ///
    /// Returns the resource descriptor for the texture object specified by \p texObject.
    ///
    /// \param pResDesc  - Resource descriptor
    /// \param texObject - Texture object
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaCreateTextureObject,
    /// ::cuTexObjectGetResourceDesc
    pub fn cudaGetTextureObjectResourceDesc(
        pResDesc: *mut cudaResourceDesc,
        texObject: cudaTextureObject_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a texture object's texture descriptor
    ///
    /// Returns the texture descriptor for the texture object specified by \p texObject.
    ///
    /// \param pTexDesc  - Texture descriptor
    /// \param texObject - Texture object
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaCreateTextureObject,
    /// ::cuTexObjectGetTextureDesc
    pub fn cudaGetTextureObjectTextureDesc(
        pTexDesc: *mut cudaTextureDesc,
        texObject: cudaTextureObject_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a texture object's resource view descriptor
    ///
    /// Returns the resource view descriptor for the texture object specified by \p texObject.
    /// If no resource view was specified, ::cudaErrorInvalidValue is returned.
    ///
    /// \param pResViewDesc - Resource view descriptor
    /// \param texObject    - Texture object
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaCreateTextureObject,
    /// ::cuTexObjectGetResourceViewDesc
    pub fn cudaGetTextureObjectResourceViewDesc(
        pResViewDesc: *mut cudaResourceViewDesc,
        texObject: cudaTextureObject_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Creates a surface object
    ///
    /// Creates a surface object and returns it in \p pSurfObject. \p pResDesc describes
    /// the data to perform surface load/stores on. ::cudaResourceDesc::resType must be
    /// ::cudaResourceTypeArray and  ::cudaResourceDesc::res::array::array
    /// must be set to a valid CUDA array handle.
    ///
    /// Surface objects are only supported on devices of compute capability 3.0 or higher.
    /// Additionally, a surface object is an opaque value, and, as such, should only be
    /// accessed through CUDA API calls.
    ///
    /// \param pSurfObject - Surface object to create
    /// \param pResDesc    - Resource descriptor
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaDestroySurfaceObject,
    /// ::cuSurfObjectCreate
    pub fn cudaCreateSurfaceObject(
        pSurfObject: *mut cudaSurfaceObject_t,
        pResDesc: *const cudaResourceDesc,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Destroys a surface object
    ///
    /// Destroys the surface object specified by \p surfObject.
    ///
    /// \param surfObject - Surface object to destroy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaCreateSurfaceObject,
    /// ::cuSurfObjectDestroy
    pub fn cudaDestroySurfaceObject(surfObject: cudaSurfaceObject_t) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a surface object's resource descriptor
    /// Returns the resource descriptor for the surface object specified by \p surfObject.
    ///
    /// \param pResDesc   - Resource descriptor
    /// \param surfObject - Surface object
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaCreateSurfaceObject,
    /// ::cuSurfObjectGetResourceDesc
    pub fn cudaGetSurfaceObjectResourceDesc(
        pResDesc: *mut cudaResourceDesc,
        surfObject: cudaSurfaceObject_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns the latest version of CUDA supported by the driver
    ///
    /// Returns in \p *driverVersion the latest version of CUDA supported by
    /// the driver. The version is returned as (1000 &times; major + 10 &times; minor).
    /// For example, CUDA 9.2 would be represented by 9020. If no driver is installed,
    /// then 0 is returned as the driver version.
    ///
    /// This function automatically returns ::cudaErrorInvalidValue
    /// if \p driverVersion is NULL.
    ///
    /// \param driverVersion - Returns the CUDA driver version.
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaRuntimeGetVersion,
    /// ::cuDriverGetVersion
    pub fn cudaDriverGetVersion(driverVersion: *mut ::std::os::raw::c_int) -> cudaError_t;
}
extern "C" {
    /// \brief Returns the CUDA Runtime version
    ///
    /// Returns in \p *runtimeVersion the version number of the current CUDA
    /// Runtime instance. The version is returned as
    /// (1000 &times; major + 10 &times; minor). For example,
    /// CUDA 9.2 would be represented by 9020.
    ///
    /// This function automatically returns ::cudaErrorInvalidValue if
    /// the \p runtimeVersion argument is NULL.
    ///
    /// \param runtimeVersion - Returns the CUDA Runtime version.
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaDriverGetVersion,
    /// ::cuDriverGetVersion
    pub fn cudaRuntimeGetVersion(runtimeVersion: *mut ::std::os::raw::c_int) -> cudaError_t;
}
extern "C" {
    /// \brief Creates a graph
    ///
    /// Creates an empty graph, which is returned via \p pGraph.
    ///
    /// \param pGraph - Returns newly created graph
    /// \param flags   - Graph creation flags, must be 0
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphAddChildGraphNode,
    /// ::cudaGraphAddEmptyNode,
    /// ::cudaGraphAddKernelNode,
    /// ::cudaGraphAddHostNode,
    /// ::cudaGraphAddMemcpyNode,
    /// ::cudaGraphAddMemsetNode,
    /// ::cudaGraphInstantiate,
    /// ::cudaGraphDestroy,
    /// ::cudaGraphGetNodes,
    /// ::cudaGraphGetRootNodes,
    /// ::cudaGraphGetEdges,
    /// ::cudaGraphClone
    pub fn cudaGraphCreate(pGraph: *mut cudaGraph_t, flags: ::std::os::raw::c_uint) -> cudaError_t;
}
extern "C" {
    /// \brief Creates a kernel execution node and adds it to a graph
    ///
    /// Creates a new kernel execution node and adds it to \p graph with \p numDependencies
    /// dependencies specified via \p pDependencies and arguments specified in \p pNodeParams.
    /// It is possible for \p numDependencies to be 0, in which case the node will be placed
    /// at the root of the graph. \p pDependencies may not have any duplicate entries.
    /// A handle to the new node will be returned in \p pGraphNode.
    ///
    /// The cudaKernelNodeParams structure is defined as:
    ///
    /// \code
    ///  struct cudaKernelNodeParams
    ///  {
    ///      void* func;
    ///      dim3 gridDim;
    ///      dim3 blockDim;
    ///      unsigned int sharedMemBytes;
    ///      void **kernelParams;
    ///      void **extra;
    ///  };
    /// \endcode
    ///
    /// When the graph is launched, the node will invoke kernel \p func on a (\p gridDim.x x
    /// \p gridDim.y x \p gridDim.z) grid of blocks. Each block contains
    /// (\p blockDim.x x \p blockDim.y x \p blockDim.z) threads.
    ///
    /// \p sharedMem sets the amount of dynamic shared memory that will be
    /// available to each thread block.
    ///
    /// Kernel parameters to \p func can be specified in one of two ways:
    ///
    /// 1) Kernel parameters can be specified via \p kernelParams. If the kernel has N
    /// parameters, then \p kernelParams needs to be an array of N pointers. Each pointer,
    /// from \p kernelParams[0] to \p kernelParams[N-1], points to the region of memory from which the actual
    /// parameter will be copied. The number of kernel parameters and their offsets and sizes do not need
    /// to be specified as that information is retrieved directly from the kernel's image.
    ///
    /// 2) Kernel parameters can also be packaged by the application into a single buffer that is passed in
    /// via \p extra. This places the burden on the application of knowing each kernel
    /// parameter's size and alignment/padding within the buffer. The \p extra parameter exists
    /// to allow this function to take additional less commonly used arguments. \p extra specifies
    /// a list of names of extra settings and their corresponding values. Each extra setting name is
    /// immediately followed by the corresponding value. The list must be terminated with either NULL or
    /// CU_LAUNCH_PARAM_END.
    ///
    /// - ::CU_LAUNCH_PARAM_END, which indicates the end of the \p extra
    ///   array;
    /// - ::CU_LAUNCH_PARAM_BUFFER_POINTER, which specifies that the next
    ///   value in \p extra will be a pointer to a buffer
    ///   containing all the kernel parameters for launching kernel
    ///   \p func;
    /// - ::CU_LAUNCH_PARAM_BUFFER_SIZE, which specifies that the next
    ///   value in \p extra will be a pointer to a size_t
    ///   containing the size of the buffer specified with
    ///   ::CU_LAUNCH_PARAM_BUFFER_POINTER;
    ///
    /// The error ::cudaErrorInvalidValue will be returned if kernel parameters are specified with both
    /// \p kernelParams and \p extra (i.e. both \p kernelParams and
    /// \p extra are non-NULL).
    ///
    /// The \p kernelParams or \p extra array, as well as the argument values it points to,
    /// are copied during this call.
    ///
    /// \note Kernels launched using graphs must not use texture and surface references. Reading or
    ///       writing through any texture or surface reference is undefined behavior.
    ///       This restriction does not apply to texture and surface objects.
    ///
    /// \param pGraphNode     - Returns newly created node
    /// \param graph          - Graph to which to add the node
    /// \param pDependencies    - Dependencies of the node
    /// \param numDependencies - Number of dependencies
    /// \param pNodeParams      - Parameters for the GPU execution node
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDeviceFunction
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaLaunchKernel,
    /// ::cudaGraphKernelNodeGetParams,
    /// ::cudaGraphKernelNodeSetParams,
    /// ::cudaGraphCreate,
    /// ::cudaGraphDestroyNode,
    /// ::cudaGraphAddChildGraphNode,
    /// ::cudaGraphAddEmptyNode,
    /// ::cudaGraphAddHostNode,
    /// ::cudaGraphAddMemcpyNode,
    /// ::cudaGraphAddMemsetNode
    pub fn cudaGraphAddKernelNode(
        pGraphNode: *mut cudaGraphNode_t,
        graph: cudaGraph_t,
        pDependencies: *mut cudaGraphNode_t,
        numDependencies: usize,
        pNodeParams: *const cudaKernelNodeParams,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a kernel node's parameters
    ///
    /// Returns the parameters of kernel node \p node in \p pNodeParams.
    /// The \p kernelParams or \p extra array returned in \p pNodeParams,
    /// as well as the argument values it points to, are owned by the node.
    /// This memory remains valid until the node is destroyed or its
    /// parameters are modified, and should not be modified
    /// directly. Use ::cudaGraphKernelNodeSetParams to update the
    /// parameters of this node.
    ///
    /// The params will contain either \p kernelParams or \p extra,
    /// according to which of these was most recently set on the node.
    ///
    /// \param node        - Node to get the parameters for
    /// \param pNodeParams - Pointer to return the parameters
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDeviceFunction
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaLaunchKernel,
    /// ::cudaGraphAddKernelNode,
    /// ::cudaGraphKernelNodeSetParams
    pub fn cudaGraphKernelNodeGetParams(
        node: cudaGraphNode_t,
        pNodeParams: *mut cudaKernelNodeParams,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Sets a kernel node's parameters
    ///
    /// Sets the parameters of kernel node \p node to \p pNodeParams.
    ///
    /// \param node        - Node to set the parameters for
    /// \param pNodeParams - Parameters to copy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidResourceHandle,
    /// ::cudaErrorMemoryAllocation
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaLaunchKernel,
    /// ::cudaGraphAddKernelNode,
    /// ::cudaGraphKernelNodeGetParams
    pub fn cudaGraphKernelNodeSetParams(
        node: cudaGraphNode_t,
        pNodeParams: *const cudaKernelNodeParams,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Creates a memcpy node and adds it to a graph
    ///
    /// Creates a new memcpy node and adds it to \p graph with \p numDependencies
    /// dependencies specified via \p pDependencies.
    /// It is possible for \p numDependencies to be 0, in which case the node will be placed
    /// at the root of the graph. \p pDependencies may not have any duplicate entries.
    /// A handle to the new node will be returned in \p pGraphNode.
    ///
    /// When the graph is launched, the node will perform the memcpy described by \p pCopyParams.
    /// See ::cudaMemcpy3D() for a description of the structure and its restrictions.
    ///
    /// Memcpy nodes have some additional restrictions with regards to managed memory, if the
    /// system contains at least one device which has a zero value for the device attribute
    /// ::cudaDevAttrConcurrentManagedAccess.
    ///
    /// \param pGraphNode     - Returns newly created node
    /// \param graph          - Graph to which to add the node
    /// \param pDependencies    - Dependencies of the node
    /// \param numDependencies - Number of dependencies
    /// \param pCopyParams      - Parameters for the memory copy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaMemcpy3D,
    /// ::cudaGraphMemcpyNodeGetParams,
    /// ::cudaGraphMemcpyNodeSetParams,
    /// ::cudaGraphCreate,
    /// ::cudaGraphDestroyNode,
    /// ::cudaGraphAddChildGraphNode,
    /// ::cudaGraphAddEmptyNode,
    /// ::cudaGraphAddKernelNode,
    /// ::cudaGraphAddHostNode,
    /// ::cudaGraphAddMemsetNode
    pub fn cudaGraphAddMemcpyNode(
        pGraphNode: *mut cudaGraphNode_t,
        graph: cudaGraph_t,
        pDependencies: *mut cudaGraphNode_t,
        numDependencies: usize,
        pCopyParams: *const cudaMemcpy3DParms,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a memcpy node's parameters
    ///
    /// Returns the parameters of memcpy node \p node in \p pNodeParams.
    ///
    /// \param node        - Node to get the parameters for
    /// \param pNodeParams - Pointer to return the parameters
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaMemcpy3D,
    /// ::cudaGraphAddMemcpyNode,
    /// ::cudaGraphMemcpyNodeSetParams
    pub fn cudaGraphMemcpyNodeGetParams(
        node: cudaGraphNode_t,
        pNodeParams: *mut cudaMemcpy3DParms,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Sets a memcpy node's parameters
    ///
    /// Sets the parameters of memcpy node \p node to \p pNodeParams.
    ///
    /// \param node        - Node to set the parameters for
    /// \param pNodeParams - Parameters to copy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaMemcpy3D,
    /// ::cudaGraphAddMemcpyNode,
    /// ::cudaGraphMemcpyNodeGetParams
    pub fn cudaGraphMemcpyNodeSetParams(
        node: cudaGraphNode_t,
        pNodeParams: *const cudaMemcpy3DParms,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Creates a memset node and adds it to a graph
    ///
    /// Creates a new memset node and adds it to \p graph with \p numDependencies
    /// dependencies specified via \p pDependencies.
    /// It is possible for \p numDependencies to be 0, in which case the node will be placed
    /// at the root of the graph. \p pDependencies may not have any duplicate entries.
    /// A handle to the new node will be returned in \p pGraphNode.
    ///
    /// The element size must be 1, 2, or 4 bytes.
    /// When the graph is launched, the node will perform the memset described by \p pMemsetParams.
    ///
    /// \param pGraphNode     - Returns newly created node
    /// \param graph          - Graph to which to add the node
    /// \param pDependencies    - Dependencies of the node
    /// \param numDependencies - Number of dependencies
    /// \param pMemsetParams    - Parameters for the memory set
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorInvalidDevice
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaMemset2D,
    /// ::cudaGraphMemsetNodeGetParams,
    /// ::cudaGraphMemsetNodeSetParams,
    /// ::cudaGraphCreate,
    /// ::cudaGraphDestroyNode,
    /// ::cudaGraphAddChildGraphNode,
    /// ::cudaGraphAddEmptyNode,
    /// ::cudaGraphAddKernelNode,
    /// ::cudaGraphAddHostNode,
    /// ::cudaGraphAddMemcpyNode
    pub fn cudaGraphAddMemsetNode(
        pGraphNode: *mut cudaGraphNode_t,
        graph: cudaGraph_t,
        pDependencies: *mut cudaGraphNode_t,
        numDependencies: usize,
        pMemsetParams: *const cudaMemsetParams,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a memset node's parameters
    ///
    /// Returns the parameters of memset node \p node in \p pNodeParams.
    ///
    /// \param node        - Node to get the parameters for
    /// \param pNodeParams - Pointer to return the parameters
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaMemset2D,
    /// ::cudaGraphAddMemsetNode,
    /// ::cudaGraphMemsetNodeSetParams
    pub fn cudaGraphMemsetNodeGetParams(
        node: cudaGraphNode_t,
        pNodeParams: *mut cudaMemsetParams,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Sets a memset node's parameters
    ///
    /// Sets the parameters of memset node \p node to \p pNodeParams.
    ///
    /// \param node        - Node to set the parameters for
    /// \param pNodeParams - Parameters to copy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaMemset2D,
    /// ::cudaGraphAddMemsetNode,
    /// ::cudaGraphMemsetNodeGetParams
    pub fn cudaGraphMemsetNodeSetParams(
        node: cudaGraphNode_t,
        pNodeParams: *const cudaMemsetParams,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Creates a host execution node and adds it to a graph
    ///
    /// Creates a new CPU execution node and adds it to \p graph with \p numDependencies
    /// dependencies specified via \p pDependencies and arguments specified in \p pNodeParams.
    /// It is possible for \p numDependencies to be 0, in which case the node will be placed
    /// at the root of the graph. \p pDependencies may not have any duplicate entries.
    /// A handle to the new node will be returned in \p pGraphNode.
    ///
    /// When the graph is launched, the node will invoke the specified CPU function.
    ///
    /// \param pGraphNode     - Returns newly created node
    /// \param graph          - Graph to which to add the node
    /// \param pDependencies    - Dependencies of the node
    /// \param numDependencies - Number of dependencies
    /// \param pNodeParams      - Parameters for the host node
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaLaunchHostFunc,
    /// ::cudaGraphHostNodeGetParams,
    /// ::cudaGraphHostNodeSetParams,
    /// ::cudaGraphCreate,
    /// ::cudaGraphDestroyNode,
    /// ::cudaGraphAddChildGraphNode,
    /// ::cudaGraphAddEmptyNode,
    /// ::cudaGraphAddKernelNode,
    /// ::cudaGraphAddMemcpyNode,
    /// ::cudaGraphAddMemsetNode
    pub fn cudaGraphAddHostNode(
        pGraphNode: *mut cudaGraphNode_t,
        graph: cudaGraph_t,
        pDependencies: *mut cudaGraphNode_t,
        numDependencies: usize,
        pNodeParams: *const cudaHostNodeParams,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a host node's parameters
    ///
    /// Returns the parameters of host node \p node in \p pNodeParams.
    ///
    /// \param node        - Node to get the parameters for
    /// \param pNodeParams - Pointer to return the parameters
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaLaunchHostFunc,
    /// ::cudaGraphAddHostNode,
    /// ::cudaGraphHostNodeSetParams
    pub fn cudaGraphHostNodeGetParams(
        node: cudaGraphNode_t,
        pNodeParams: *mut cudaHostNodeParams,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Sets a host node's parameters
    ///
    /// Sets the parameters of host node \p node to \p nodeParams.
    ///
    /// \param node        - Node to set the parameters for
    /// \param pNodeParams - Parameters to copy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaLaunchHostFunc,
    /// ::cudaGraphAddHostNode,
    /// ::cudaGraphHostNodeGetParams
    pub fn cudaGraphHostNodeSetParams(
        node: cudaGraphNode_t,
        pNodeParams: *const cudaHostNodeParams,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Creates a child graph node and adds it to a graph
    ///
    /// Creates a new node which executes an embedded graph, and adds it to \p graph with
    /// \p numDependencies dependencies specified via \p pDependencies.
    /// It is possible for \p numDependencies to be 0, in which case the node will be placed
    /// at the root of the graph. \p pDependencies may not have any duplicate entries.
    /// A handle to the new node will be returned in \p pGraphNode.
    ///
    /// The node executes an embedded child graph. The child graph is cloned in this call.
    ///
    /// \param pGraphNode     - Returns newly created node
    /// \param graph          - Graph to which to add the node
    /// \param pDependencies    - Dependencies of the node
    /// \param numDependencies - Number of dependencies
    /// \param childGraph      - The graph to clone into this node
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphChildGraphNodeGetGraph,
    /// ::cudaGraphCreate,
    /// ::cudaGraphDestroyNode,
    /// ::cudaGraphAddEmptyNode,
    /// ::cudaGraphAddKernelNode,
    /// ::cudaGraphAddHostNode,
    /// ::cudaGraphAddMemcpyNode,
    /// ::cudaGraphAddMemsetNode,
    /// ::cudaGraphClone
    pub fn cudaGraphAddChildGraphNode(
        pGraphNode: *mut cudaGraphNode_t,
        graph: cudaGraph_t,
        pDependencies: *mut cudaGraphNode_t,
        numDependencies: usize,
        childGraph: cudaGraph_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Gets a handle to the embedded graph of a child graph node
    ///
    /// Gets a handle to the embedded graph in a child graph node. This call
    /// does not clone the graph. Changes to the graph will be reflected in
    /// the node, and the node retains ownership of the graph.
    ///
    /// \param node   - Node to get the embedded graph for
    /// \param pGraph - Location to store a handle to the graph
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphAddChildGraphNode,
    /// ::cudaGraphNodeFindInClone
    pub fn cudaGraphChildGraphNodeGetGraph(
        node: cudaGraphNode_t,
        pGraph: *mut cudaGraph_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Creates an empty node and adds it to a graph
    ///
    /// Creates a new node which performs no operation, and adds it to \p graph with
    /// \p numDependencies dependencies specified via \p pDependencies.
    /// It is possible for \p numDependencies to be 0, in which case the node will be placed
    /// at the root of the graph. \p pDependencies may not have any duplicate entries.
    /// A handle to the new node will be returned in \p pGraphNode.
    ///
    /// An empty node performs no operation during execution, but can be used for
    /// transitive ordering. For example, a phased execution graph with 2 groups of n
    /// nodes with a barrier between them can be represented using an empty node and
    /// 2*n dependency edges, rather than no empty node and n^2 dependency edges.
    ///
    /// \param pGraphNode     - Returns newly created node
    /// \param graph          - Graph to which to add the node
    /// \param pDependencies    - Dependencies of the node
    /// \param numDependencies - Number of dependencies
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphCreate,
    /// ::cudaGraphDestroyNode,
    /// ::cudaGraphAddChildGraphNode,
    /// ::cudaGraphAddKernelNode,
    /// ::cudaGraphAddHostNode,
    /// ::cudaGraphAddMemcpyNode,
    /// ::cudaGraphAddMemsetNode
    pub fn cudaGraphAddEmptyNode(
        pGraphNode: *mut cudaGraphNode_t,
        graph: cudaGraph_t,
        pDependencies: *mut cudaGraphNode_t,
        numDependencies: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Clones a graph
    ///
    /// This function creates a copy of \p originalGraph and returns it in \p pGraphClone.
    /// All parameters are copied into the cloned graph. The original graph may be modified
    /// after this call without affecting the clone.
    ///
    /// Child graph nodes in the original graph are recursively copied into the clone.
    ///
    /// \param pGraphClone  - Returns newly created cloned graph
    /// \param originalGraph - Graph to clone
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue,
    /// ::cudaErrorMemoryAllocation
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphCreate,
    /// ::cudaGraphNodeFindInClone
    pub fn cudaGraphClone(pGraphClone: *mut cudaGraph_t, originalGraph: cudaGraph_t)
        -> cudaError_t;
}
extern "C" {
    /// \brief Finds a cloned version of a node
    ///
    /// This function returns the node in \p clonedGraph corresponding to \p originalNode
    /// in the original graph.
    ///
    /// \p clonedGraph must have been cloned from \p originalGraph via ::cudaGraphClone.
    /// \p originalNode must have been in \p originalGraph at the time of the call to
    /// ::cudaGraphClone, and the corresponding cloned node in \p clonedGraph must not have
    /// been removed. The cloned node is then returned via \p pClonedNode.
    ///
    /// \param pNode  - Returns handle to the cloned node
    /// \param originalNode - Handle to the original node
    /// \param clonedGraph - Cloned graph to query
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphClone
    pub fn cudaGraphNodeFindInClone(
        pNode: *mut cudaGraphNode_t,
        originalNode: cudaGraphNode_t,
        clonedGraph: cudaGraph_t,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a node's type
    ///
    /// Returns the node type of \p node in \p pType.
    ///
    /// \param node - Node to query
    /// \param pType  - Pointer to return the node type
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphGetNodes,
    /// ::cudaGraphGetRootNodes,
    /// ::cudaGraphChildGraphNodeGetGraph,
    /// ::cudaGraphKernelNodeGetParams,
    /// ::cudaGraphKernelNodeSetParams,
    /// ::cudaGraphHostNodeGetParams,
    /// ::cudaGraphHostNodeSetParams,
    /// ::cudaGraphMemcpyNodeGetParams,
    /// ::cudaGraphMemcpyNodeSetParams,
    /// ::cudaGraphMemsetNodeGetParams,
    /// ::cudaGraphMemsetNodeSetParams
    pub fn cudaGraphNodeGetType(
        node: cudaGraphNode_t,
        pType: *mut cudaGraphNodeType,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a graph's nodes
    ///
    /// Returns a list of \p graph's nodes. \p nodes may be NULL, in which case this
    /// function will return the number of nodes in \p numNodes. Otherwise,
    /// \p numNodes entries will be filled in. If \p numNodes is higher than the actual
    /// number of nodes, the remaining entries in \p nodes will be set to NULL, and the
    /// number of nodes actually obtained will be returned in \p numNodes.
    ///
    /// \param graph    - Graph to query
    /// \param nodes    - Pointer to return the nodes
    /// \param numNodes - See description
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphCreate,
    /// ::cudaGraphGetRootNodes,
    /// ::cudaGraphGetEdges,
    /// ::cudaGraphNodeGetType,
    /// ::cudaGraphNodeGetDependencies,
    /// ::cudaGraphNodeGetDependentNodes
    pub fn cudaGraphGetNodes(
        graph: cudaGraph_t,
        nodes: *mut cudaGraphNode_t,
        numNodes: *mut usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a graph's root nodes
    ///
    /// Returns a list of \p graph's root nodes. \p pRootNodes may be NULL, in which case this
    /// function will return the number of root nodes in \p pNumRootNodes. Otherwise,
    /// \p pNumRootNodes entries will be filled in. If \p pNumRootNodes is higher than the actual
    /// number of root nodes, the remaining entries in \p pRootNodes will be set to NULL, and the
    /// number of nodes actually obtained will be returned in \p pNumRootNodes.
    ///
    /// \param graph       - Graph to query
    /// \param pRootNodes    - Pointer to return the root nodes
    /// \param pNumRootNodes - See description
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphCreate,
    /// ::cudaGraphGetNodes,
    /// ::cudaGraphGetEdges,
    /// ::cudaGraphNodeGetType,
    /// ::cudaGraphNodeGetDependencies,
    /// ::cudaGraphNodeGetDependentNodes
    pub fn cudaGraphGetRootNodes(
        graph: cudaGraph_t,
        pRootNodes: *mut cudaGraphNode_t,
        pNumRootNodes: *mut usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a graph's dependency edges
    ///
    /// Returns a list of \p graph's dependency edges. Edges are returned via corresponding
    /// indices in \p from and \p to; that is, the node in \p to[i] has a dependency on the
    /// node in \p from[i]. \p from and \p to may both be NULL, in which
    /// case this function only returns the number of edges in \p numEdges. Otherwise,
    /// \p numEdges entries will be filled in. If \p numEdges is higher than the actual
    /// number of edges, the remaining entries in \p from and \p to will be set to NULL, and
    /// the number of edges actually returned will be written to \p numEdges.
    ///
    /// \param graph    - Graph to get the edges from
    /// \param from     - Location to return edge endpoints
    /// \param to       - Location to return edge endpoints
    /// \param numEdges - See description
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphGetNodes,
    /// ::cudaGraphGetRootNodes,
    /// ::cudaGraphAddDependencies,
    /// ::cudaGraphRemoveDependencies,
    /// ::cudaGraphNodeGetDependencies,
    /// ::cudaGraphNodeGetDependentNodes
    pub fn cudaGraphGetEdges(
        graph: cudaGraph_t,
        from: *mut cudaGraphNode_t,
        to: *mut cudaGraphNode_t,
        numEdges: *mut usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a node's dependencies
    ///
    /// Returns a list of \p node's dependencies. \p pDependencies may be NULL, in which case this
    /// function will return the number of dependencies in \p pNumDependencies. Otherwise,
    /// \p pNumDependencies entries will be filled in. If \p pNumDependencies is higher than the actual
    /// number of dependencies, the remaining entries in \p pDependencies will be set to NULL, and the
    /// number of nodes actually obtained will be returned in \p pNumDependencies.
    ///
    /// \param node           - Node to query
    /// \param pDependencies    - Pointer to return the dependencies
    /// \param pNumDependencies - See description
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphNodeGetDependentNodes,
    /// ::cudaGraphGetNodes,
    /// ::cudaGraphGetRootNodes,
    /// ::cudaGraphGetEdges,
    /// ::cudaGraphAddDependencies,
    /// ::cudaGraphRemoveDependencies
    pub fn cudaGraphNodeGetDependencies(
        node: cudaGraphNode_t,
        pDependencies: *mut cudaGraphNode_t,
        pNumDependencies: *mut usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Returns a node's dependent nodes
    ///
    /// Returns a list of \p node's dependent nodes. \p pDependentNodes may be NULL, in which
    /// case this function will return the number of dependent nodes in \p pNumDependentNodes.
    /// Otherwise, \p pNumDependentNodes entries will be filled in. If \p pNumDependentNodes is
    /// higher than the actual number of dependent nodes, the remaining entries in
    /// \p pDependentNodes will be set to NULL, and the number of nodes actually obtained will
    /// be returned in \p pNumDependentNodes.
    ///
    /// \param node             - Node to query
    /// \param pDependentNodes    - Pointer to return the dependent nodes
    /// \param pNumDependentNodes - See description
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphNodeGetDependencies,
    /// ::cudaGraphGetNodes,
    /// ::cudaGraphGetRootNodes,
    /// ::cudaGraphGetEdges,
    /// ::cudaGraphAddDependencies,
    /// ::cudaGraphRemoveDependencies
    pub fn cudaGraphNodeGetDependentNodes(
        node: cudaGraphNode_t,
        pDependentNodes: *mut cudaGraphNode_t,
        pNumDependentNodes: *mut usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Adds dependency edges to a graph.
    ///
    /// The number of dependencies to be added is defined by \p numDependencies
    /// Elements in \p pFrom and \p pTo at corresponding indices define a dependency.
    /// Each node in \p pFrom and \p pTo must belong to \p graph.
    ///
    /// If \p numDependencies is 0, elements in \p pFrom and \p pTo will be ignored.
    /// Specifying an existing dependency will return an error.
    ///
    /// \param graph - Graph to which dependencies are added
    /// \param from - Array of nodes that provide the dependencies
    /// \param to - Array of dependent nodes
    /// \param numDependencies - Number of dependencies to be added
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphRemoveDependencies,
    /// ::cudaGraphGetEdges,
    /// ::cudaGraphNodeGetDependencies,
    /// ::cudaGraphNodeGetDependentNodes
    pub fn cudaGraphAddDependencies(
        graph: cudaGraph_t,
        from: *mut cudaGraphNode_t,
        to: *mut cudaGraphNode_t,
        numDependencies: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Removes dependency edges from a graph.
    ///
    /// The number of \p pDependencies to be removed is defined by \p numDependencies.
    /// Elements in \p pFrom and \p pTo at corresponding indices define a dependency.
    /// Each node in \p pFrom and \p pTo must belong to \p graph.
    ///
    /// If \p numDependencies is 0, elements in \p pFrom and \p pTo will be ignored.
    /// Specifying a non-existing dependency will return an error.
    ///
    /// \param graph - Graph from which to remove dependencies
    /// \param from - Array of nodes that provide the dependencies
    /// \param to - Array of dependent nodes
    /// \param numDependencies - Number of dependencies to be removed
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphAddDependencies,
    /// ::cudaGraphGetEdges,
    /// ::cudaGraphNodeGetDependencies,
    /// ::cudaGraphNodeGetDependentNodes
    pub fn cudaGraphRemoveDependencies(
        graph: cudaGraph_t,
        from: *mut cudaGraphNode_t,
        to: *mut cudaGraphNode_t,
        numDependencies: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Remove a node from the graph
    ///
    /// Removes \p node from its graph. This operation also severs any dependencies of other nodes
    /// on \p node and vice versa.
    ///
    /// \param node  - Node to remove
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphAddChildGraphNode,
    /// ::cudaGraphAddEmptyNode,
    /// ::cudaGraphAddKernelNode,
    /// ::cudaGraphAddHostNode,
    /// ::cudaGraphAddMemcpyNode,
    /// ::cudaGraphAddMemsetNode
    pub fn cudaGraphDestroyNode(node: cudaGraphNode_t) -> cudaError_t;
}
extern "C" {
    /// \brief Creates an executable graph from a graph
    ///
    /// Instantiates \p graph as an executable graph. The graph is validated for any
    /// structural constraints or intra-node constraints which were not previously
    /// validated. If instantiation is successful, a handle to the instantiated graph
    /// is returned in \p pGraphExec.
    ///
    /// If there are any errors, diagnostic information may be returned in \p pErrorNode and
    /// \p pLogBuffer. This is the primary way to inspect instantiation errors. The output
    /// will be null terminated unless the diagnostics overflow
    /// the buffer. In this case, they will be truncated, and the last byte can be
    /// inspected to determine if truncation occurred.
    ///
    /// \param pGraphExec - Returns instantiated graph
    /// \param graph      - Graph to instantiate
    /// \param pErrorNode - In case of an instantiation error, this may be modified to
    ///                      indicate a node contributing to the error
    /// \param pLogBuffer   - A character buffer to store diagnostic messages
    /// \param bufferSize  - Size of the log buffer in bytes
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphCreate,
    /// ::cudaGraphLaunch,
    /// ::cudaGraphExecDestroy
    pub fn cudaGraphInstantiate(
        pGraphExec: *mut cudaGraphExec_t,
        graph: cudaGraph_t,
        pErrorNode: *mut cudaGraphNode_t,
        pLogBuffer: *mut ::std::os::raw::c_char,
        bufferSize: usize,
    ) -> cudaError_t;
}
extern "C" {
    /// \brief Launches an executable graph in a stream
    ///
    /// Executes \p graphExec in \p stream. Only one instance of \p graphExec may be executing
    /// at a time. Each launch is ordered behind both any previous work in \p stream
    /// and any previous launches of \p graphExec. To execute a graph concurrently, it must be
    /// instantiated multiple times into multiple executable graphs.
    ///
    /// \param graphExec - Executable graph to launch
    /// \param stream    - Stream in which to launch the graph
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphInstantiate,
    /// ::cudaGraphExecDestroy
    pub fn cudaGraphLaunch(graphExec: cudaGraphExec_t, stream: cudaStream_t) -> cudaError_t;
}
extern "C" {
    /// \brief Destroys an executable graph
    ///
    /// Destroys the executable graph specified by \p graphExec.
    ///
    /// \param graphExec - Executable graph to destroy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphInstantiate,
    /// ::cudaGraphLaunch
    pub fn cudaGraphExecDestroy(graphExec: cudaGraphExec_t) -> cudaError_t;
}
extern "C" {
    /// \brief Destroys a graph
    ///
    /// Destroys the graph specified by \p graph, as well as all of its nodes.
    ///
    /// \param graph - Graph to destroy
    ///
    /// \return
    /// ::cudaSuccess,
    /// ::cudaErrorInvalidValue
    /// \note_graph_thread_safety
    /// \notefnerr
    /// \note_init_rt
    /// \note_callback
    ///
    /// \sa
    /// ::cudaGraphCreate
    pub fn cudaGraphDestroy(graph: cudaGraph_t) -> cudaError_t;
}
extern "C" {
    /// \cond impl_private
    pub fn cudaGetExportTable(
        ppExportTable: *mut *const ::std::os::raw::c_void,
        pExportTableId: *const cudaUUID_t,
    ) -> cudaError_t;
}
